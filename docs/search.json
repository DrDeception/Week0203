[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Annotation",
    "section": "",
    "text": "Use Hypothes.is, an amazing tool for annotating the web. Go to Hypothes.is, and “get-started”\nInstall the the add-on for chrome, or other browser\nThat’s it, turn on Hypothes.is when you are reading this textbook, and you will see all public annotations made by anyone else.\nPlease see the VLE for the private Research Methods Group ID that will restrict sight of the comments to the year group! (Week 02 Lecture Comments)",
    "crumbs": [
      "General Info",
      "Quarto Example",
      "Index"
    ]
  },
  {
    "objectID": "index.html#hypothes.is",
    "href": "index.html#hypothes.is",
    "title": "Social Annotation",
    "section": "",
    "text": "Use Hypothes.is, an amazing tool for annotating the web. Go to Hypothes.is, and “get-started”\nInstall the the add-on for chrome, or other browser\nThat’s it, turn on Hypothes.is when you are reading this textbook, and you will see all public annotations made by anyone else.\nPlease see the VLE for the private Research Methods Group ID that will restrict sight of the comments to the year group! (Week 02 Lecture Comments)",
    "crumbs": [
      "General Info",
      "Quarto Example",
      "Index"
    ]
  },
  {
    "objectID": "materials/levelsofmeasurement.html",
    "href": "materials/levelsofmeasurement.html",
    "title": "Levels of Measurement",
    "section": "",
    "text": "Attribution\n\n\n\nCopied under CC BY-SA 4.0 license and shared alike at https://drdeception.github.io/Week0203/levelsofmeasurement.html\nOnline Textbook: https://crumplab.github.io/statistics/\nCitation: Crump, M. J. C., Navarro, D. J., & Suzuki, J. (2019, June 5). Answering Questions with Data (Textbook): Introductory Statistics for Psychology Students. https://doi.org/10.17605/OSF.IO/JZE52"
  },
  {
    "objectID": "materials/levelsofmeasurement.html#levels-of-measurement",
    "href": "materials/levelsofmeasurement.html#levels-of-measurement",
    "title": "Levels of Measurement",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nAs the previous section indicates, the outcome of a psychological measurement is called a variable. But not all variables are of the same qualitative type, and it’s very useful to understand what types there are. A very useful concept for distinguishing between different types of variables is what’s known as scales of measurement.\n\nNominal scale\nA nominal scale variable (also referred to as a categorical variable) is one in which there is no particular relationship between the different possibilities: for these kinds of variables it doesn’t make any sense to say that one of them is ``bigger’ or “better” than any other one, and it absolutely doesn’t make any sense to average them. The classic example for this is “eye color”. Eyes can be blue, green and brown, among other possibilities, but none of them is any “better” than any other one. As a result, it would feel really weird to talk about an “average eye color”. Similarly, gender is nominal too: male isn’t better or worse than female, neither does it make sense to try to talk about an “average gender”. In short, nominal scale variables are those for which the only thing you can say about the different possibilities is that they are different. That’s it.\nLet’s take a slightly closer look at this. Suppose I was doing research on how people commute to and from work. One variable I would have to measure would be what kind of transportation people use to get to work. This “transport type” variable could have quite a few possible values, including: “train”, “bus”, “car”, “bicycle”, etc. For now, let’s suppose that these four are the only possibilities, and suppose that when I ask 100 people how they got to work today, and I get this:\n\n\n\nTransportation\nNumber of people\n\n\n\n\n(1) Train\n12\n\n\n(2) Bus\n30\n\n\n(3) Car\n48\n\n\n(4) Bicycle\n10\n\n\n\nSo, what’s the average transportation type? Obviously, the answer here is that there isn’t one. It’s a silly question to ask. You can say that travel by car is the most popular method, and travel by train is the least popular method, but that’s about all. Similarly, notice that the order in which I list the options isn’t very interesting. I could have chosen to display the data like this and nothing really changes.\n\n\n\nTransportation\nNumber of people\n\n\n\n\n(3) Car\n48\n\n\n(1) Train\n12\n\n\n(4) Bicycle\n10\n\n\n(2) Bus\n30\n\n\n\n\n\nOrdinal scale\nOrdinal scale variables have a bit more structure than nominal scale variables, but not by a lot. An ordinal scale variable is one in which there is a natural, meaningful way to order the different possibilities, but you can’t do anything else. The usual example given of an ordinal variable is “finishing position in a race”. You can say that the person who finished first was faster than the person who finished second, but you don’t know how much faster. As a consequence we know that 1st \\(&gt;\\) 2nd, and we know that 2nd \\(&gt;\\) 3rd, but the difference between 1st and 2nd might be much larger than the difference between 2nd and 3rd.\nHere’s an more psychologically interesting example. Suppose I’m interested in people’s attitudes to climate change, and I ask them to pick one of these four statements that most closely matches their beliefs:\n\n\nTemperatures are rising, because of human activity\n\nTemperatures are rising, but we don’t know why\n\nTemperatures are rising, but not because of humans\n\nTemperatures are not rising\n\n\nNotice that these four statements actually do have a natural ordering, in terms of “the extent to which they agree with the current science”. Statement 1 is a close match, statement 2 is a reasonable match, statement 3 isn’t a very good match, and statement 4 is in strong opposition to the science. So, in terms of the thing I’m interested in (the extent to which people endorse the science), I can order the items as \\(1 &gt; 2 &gt; 3 &gt; 4\\). Since this ordering exists, it would be very weird to list the options like this…\n\n\nTemperatures are rising, but not because of humans\n\nTemperatures are rising, because of human activity\n\nTemperatures are not rising\n\nTemperatures are rising, but we don’t know why\n\n\n…because it seems to violate the natural “structure” to the question.\nSo, let’s suppose I asked 100 people these questions, and got the following answers:\n\n\n\n\nNumber\n\n\n\n\n(1) Temperatures are rising, because of human activity\n51\n\n\n(2) Temperatures are rising, but we don’t know why\n20\n\n\n(3) Temperatures are rising, but not because of humans\n10\n\n\n(4) Temperatures are not rising\n19\n\n\n\nWhen analyzing these data, it seems quite reasonable to try to group (1), (2) and (3) together, and say that 81 of 100 people were willing to at least partially endorse the science. And it’s also quite reasonable to group (2), (3) and (4) together and say that 49 of 100 people registered at least some disagreement with the dominant scientific view. However, it would be entirely bizarre to try to group (1), (2) and (4) together and say that 90 of 100 people said…what? There’s nothing sensible that allows you to group those responses together at all.\nThat said, notice that while we can use the natural ordering of these items to construct sensible groupings, what we can’t do is average them. For instance, in my simple example here, the “average” response to the question is 1.97. If you can tell me what that means, I’d love to know. Because that sounds like gibberish to me!\n\n\nInterval scale\nIn contrast to nominal and ordinal scale variables, interval scale and ratio scale variables are variables for which the numerical value is genuinely meaningful. In the case of interval scale variables, the differences between the numbers are interpretable, but the variable doesn’t have a “natural” zero value. A good example of an interval scale variable is measuring temperature in degrees Celsius. For instance, if it was 15\\(^\\circ\\) yesterday and 18\\(^\\circ\\) today, then the 3\\(^\\circ\\) difference between the two is genuinely meaningful. Moreover, that 3\\(^\\circ\\) difference is exactly the same as the 3\\(^\\circ\\) difference between \\(7^\\circ\\) and \\(10^\\circ\\). In short, addition and subtraction are meaningful for interval scale variables.\nHowever, notice that the \\(0^\\circ\\) does not mean “no temperature at all”: it actually means “the temperature at which water freezes”, which is pretty arbitrary. As a consequence, it becomes pointless to try to multiply and divide temperatures. It is wrong to say that \\(20^\\circ\\) is twice as hot as \\(10^\\circ\\), just as it is weird and meaningless to try to claim that \\(20^\\circ\\) is negative two times as hot as \\(-10^\\circ\\).\nAgain, lets look at a more psychological example. Suppose I’m interested in looking at how the attitudes of first-year university students have changed over time. Obviously, I’m going to want to record the year in which each student started. This is an interval scale variable. A student who started in 2003 did arrive 5 years before a student who started in 2008. However, it would be completely insane for me to divide 2008 by 2003 and say that the second student started “1.0024 times later” than the first one. That doesn’t make any sense at all.\n\n\nRatio scale\nThe fourth and final type of variable to consider is a ratio scale variable, in which zero really means zero, and it’s okay to multiply and divide. A good psychological example of a ratio scale variable is response time (RT). In a lot of tasks it’s very common to record the amount of time somebody takes to solve a problem or answer a question, because it’s an indicator of how difficult the task is. Suppose that Alan takes 2.3 seconds to respond to a question, whereas Ben takes 3.1 seconds. As with an interval scale variable, addition and subtraction are both meaningful here. Ben really did take \\(3.1 - 2.3 = 0.8\\) seconds longer than Alan did. However, notice that multiplication and division also make sense here too: Ben took \\(3.1 / 2.3 = 1.35\\) times as long as Alan did to answer the question. And the reason why you can do this is that, for a ratio scale variable such as RT, “zero seconds” really does mean “no time at all”.\n\n\nContinuous versus discrete variables\nThere’s a second kind of distinction that you need to be aware of, regarding what types of variables you can run into. This is the distinction between continuous variables and discrete variables. The difference between these is as follows:\n\nA continuous variable is one in which, for any two values that you can think of, it’s always logically possible to have another value in between.\nA discrete variable is, in effect, a variable that isn’t continuous. For a discrete variable, it’s sometimes the case that there’s nothing in the middle.\n\nThese definitions probably seem a bit abstract, but they’re pretty simple once you see some examples. For instance, response time is continuous. If Alan takes 3.1 seconds and Ben takes 2.3 seconds to respond to a question, then it’s possible for Cameron’s response time to lie in between, by taking 3.0 seconds. And of course it would also be possible for David to take 3.031 seconds to respond, meaning that his RT would lie in between Cameron’s and Alan’s. And while in practice it might be impossible to measure RT that precisely, it’s certainly possible in principle. Because we can always find a new value for RT in between any two other ones, we say that RT is continuous.\nDiscrete variables occur when this rule is violated. For example, nominal scale variables are always discrete: there isn’t a type of transportation that falls “in between” trains and bicycles, not in the strict mathematical way that 2.3 falls in between 2 and 3. So transportation type is discrete. Similarly, ordinal scale variables are always discrete: although “2nd place” does fall between “1st place” and “3rd place”, there’s nothing that can logically fall in between “1st place” and “2nd place”. Interval scale and ratio scale variables can go either way. As we saw above, response time (a ratio scale variable) is continuous. Temperature in degrees Celsius (an interval scale variable) is also continuous. However, the year you went to school (an interval scale variable) is discrete. There’s no year in between 2002 and 2003. The number of questions you get right on a true-or-false test (a ratio scale variable) is also discrete: since a true-or-false question doesn’t allow you to be “partially correct”, there’s nothing in between 5/10 and 6/10. The table summarizes the relationship between the scales of measurement and the discrete/continuity distinction. Cells with a tick mark correspond to things that are possible. I’m trying to hammer this point home, because (a) some textbooks get this wrong, and (b) people very often say things like “discrete variable” when they mean “nominal scale variable”. It’s very unfortunate.\n\nThe relationship between the scales of measurement and the discrete/continuity distinction. Cells with an x correspond to things that are possible.\n\n\n\ncontinuous\ndiscrete\n\n\n\n\nnominal\n\nx\n\n\nordinal\n\nx\n\n\ninterval\nx\nx\n\n\nratio\nx\nx\n\n\n\n\n\nSome complexities\nOkay, I know you’re going to be shocked to hear this, but …the real world is much messier than this little classification scheme suggests. Very few variables in real life actually fall into these nice neat categories, so you need to be kind of careful not to treat the scales of measurement as if they were hard and fast rules. It doesn’t work like that: they’re guidelines, intended to help you think about the situations in which you should treat different variables differently. Nothing more.\nSo let’s take a classic example, maybe the classic example, of a psychological measurement tool: the Likert scale. The humble Likert scale is the bread and butter tool of all survey design. You yourself have filled out hundreds, maybe thousands of them, and odds are you’ve even used one yourself. Suppose we have a survey question that looks like this:\n\nWhich of the following best describes your opinion of the statement that “all pirates are freaking awesome” …\n\nand then the options presented to the participant are these:\n\n(1) Strongly disagree\n(2) Disagree\n(3) Neither agree nor disagree\n(4) Agree\n(5) Strongly agree\n\nThis set of items is an example of a 5-point Likert scale: people are asked to choose among one of several (in this case 5) clearly ordered possibilities, generally with a verbal descriptor given in each case. However, it’s not necessary that all items be explicitly described. This is a perfectly good example of a 5-point Likert scale too:\n\n(1) Strongly disagree\n(2)\n(3)\n(4)\n(5) Strongly agree\n\nLikert scales are very handy, if somewhat limited, tools. The question is, what kind of variable are they? They’re obviously discrete, since you can’t give a response of 2.5. They’re obviously not nominal scale, since the items are ordered; and they’re not ratio scale either, since there’s no natural zero.\nBut are they ordinal scale or interval scale? One argument says that we can’t really prove that the difference between “strongly agree” and “agree” is of the same size as the difference between “agree” and “neither agree nor disagree”. In fact, in everyday life it’s pretty obvious that they’re not the same at all. So this suggests that we ought to treat Likert scales as ordinal variables. On the other hand, in practice most participants do seem to take the whole “on a scale from 1 to 5” part fairly seriously, and they tend to act as if the differences between the five response options were fairly similar to one another. As a consequence, a lot of researchers treat Likert scale data as if it were interval scale. It’s not interval scale, but in practice it’s close enough that we usually think of it as being quasi-interval scale.\n\n\nTextbook\n\nwebsite: https://crumplab.github.io/statistics/\nOSF: https://osf.io/jze52/\nGithub: https://github.com/CrumpLab/statistics\nDOI: 10.17605/OSF.IO/JZE52"
  },
  {
    "objectID": "materials/scalequestions.html",
    "href": "materials/scalequestions.html",
    "title": "Scaled Questions Clusters",
    "section": "",
    "text": "How happy do you normally feel on a scale of 1-5, 1 being extremely unhappy, and 5 being extremely happy?\nOn a scale of 1-5, how would you rate today’s lecture? (1 being the lowest and 5 the highest)\nRate how confident you’re feeling right now. 1 being very confident and 5 being very insecure.\n\n\n\n\n\nHow would you rate your experience in our institution so far on a scale from 1 to 5?\nOn a scale from 1 to 5, how satisfied are you with the social activities at the university?\nOn a scale from 1 to 5, how skilled do you consider yourself to be in Maths?\nOn the scale of 1-5, how happy was your childhood?\nIn a scale from 1 to 5 how hard will you say is psychology in university?\n\n\n\n\n\nOn a scale of 1-5, how good was the last movie you watched? (1= terrible, 5=great).\nHow much would you rate chocolate from a rating of 1 to 5? 1 being terrible and disgusting and 5 being amazing and delicious\nWould you consider yourself an extrovert? (1-5) 1-Absolutely not 5- Definitely\n\n\n\n\n\n\nOn a scale of 1 to 7, rate how hungry you are; 1 is not hungry at all and 7 is starving.\nOn a scale from 1 to 7 how severe is your phobia or fear.\nOn a scale from 1 to 7, where 1 = strongly disagree and 7 = strongly agree, rate the following statement “psychology is a science”\nShould a convicted sex offender be given an opportunity to be released? (1 disagree 7 agree)\nOn a scale of one to seven, how anxious have you felt this week?\n\n\n\n\n\n\n\nHow confident would you say you are on a scale from 1-10?\nOn a scale of 1 to 10 how confident do you feel about starting the Research and Methods module? (1 being not confident at all, 10 being extremely confident)\nIn a scale of 1 to 10, with 1 being the least confident and 10 being the most confident, how confident do you feel speaking up and asking questions during a seminar?\nOn a scale of 1 to 10, how confident do you feel speaking to new people you’ve never met? (1= least confident, 10= most confident)\nOn a scale from 1-10 how comfortable are you with yourself?\nOn a scale from 1-10 how confident were you when first starting University?\n\n\n\n\n\nHow happy are you on a scale from one to ten?\nHow would you rate your mood right now on a scale of 1 to 10 with 1 being the worst and 10 being the best.\nOn a scale of 1-10, how are you feeling today?\nWhat would you rate your current happiness level on a scale of 1 to 10 (1 being extremely unhappy to 10 being very happy)\nOn a scale from 1-10 how happy are you generally?\nOn a scale of 1-10 how happy do you feel, where 1 is feeling ‘extremely unhappy’ and 10 is feeling ‘extremely happy’?\n\n\n\n\n\nHow much do you like listening to music on a scale of 1-10?\nOn a scale from 1-10 how much do you enjoy psychology?\nOn a scale of 1 to 10, how would you rate your first day at University?\nHow likely, on a scale from 1 to 10, do you feel you will choose the path you want to follow on your career after completing your degree?\nOn a scale of 1 to 10 how would you rate your experience?\n\n\n\n\n\nOn a scale of 1-10, how difficult do you view psychology as?\nOn a scale from 1 to 10, with 10 being absolutely love them and 1 being dirt on my shoe, how do you feel about the Harry Potter movies?\nFrom a scale of 1 to 10, how much do you hate Monday mornings?\nOn a night out how much do you drink (1- one or two, 10- blackout)\nHow much pain do you feel on a scale of 1-10?\n\n\n\n\n\n\nOn a scale from one to five how likely you would ask question out loud in the public, where 1(I would not do it) and 5(i would definitely do it)?\nOn a scale from 1 - 3, How happy are you? 1 being not happy and 3 being happy\nHow often do you feel anxious?\nHow inclined are you to depression?\nHow apprehensive do you feel about starting this degree? 0 (not at all) - 10 (extremely)\nWith 1 being extremely important and 5 being not important at all, how important is future earning potential when choosing your degree?\nHow often do you feel happy or joyful in your day to day life?\nOn a scale of 1-5, where 1=very negative, and 5=very positive, how do you feel about the political state of the world?\nOn a scale from one to ten, how important do you believe rehabilitation is for convicted felons? One being unimportant and ten being crucial.\nAfter the first week at Goldsmiths, how confident are you that psychology is the right subject for you? 0 (not confident at all) - 5 (extremely confident)\nHow would you rate season 5 of Doctor Who on a scale of 1-10?\nOn a scale of 1-10, how significant do you believe the impact of mental health challenges is on men in society?\nHow interested are you in understanding how the human brain influences behaviour?\nOn a scale of 1-9, how vital would you say coffee is in your life, with 1 being unimportant and 9 being extremely vital?\nOn a scale of 1 to 5, how would you describe your stress levels during exam periods, from 5 being very stressed to 1 being not stressed at all?\nHow obedient are you to those with authority?\nI often seek support from others when I am facing challenges.",
    "crumbs": [
      "General Info",
      "Course Content",
      "Scale Questions"
    ]
  },
  {
    "objectID": "materials/scalequestions.html#scale-questions",
    "href": "materials/scalequestions.html#scale-questions",
    "title": "Scaled Questions Clusters",
    "section": "",
    "text": "How happy do you normally feel on a scale of 1-5, 1 being extremely unhappy, and 5 being extremely happy?\nOn a scale of 1-5, how would you rate today’s lecture? (1 being the lowest and 5 the highest)\nRate how confident you’re feeling right now. 1 being very confident and 5 being very insecure.\n\n\n\n\n\nHow would you rate your experience in our institution so far on a scale from 1 to 5?\nOn a scale from 1 to 5, how satisfied are you with the social activities at the university?\nOn a scale from 1 to 5, how skilled do you consider yourself to be in Maths?\nOn the scale of 1-5, how happy was your childhood?\nIn a scale from 1 to 5 how hard will you say is psychology in university?\n\n\n\n\n\nOn a scale of 1-5, how good was the last movie you watched? (1= terrible, 5=great).\nHow much would you rate chocolate from a rating of 1 to 5? 1 being terrible and disgusting and 5 being amazing and delicious\nWould you consider yourself an extrovert? (1-5) 1-Absolutely not 5- Definitely",
    "crumbs": [
      "General Info",
      "Course Content",
      "Scale Questions"
    ]
  },
  {
    "objectID": "materials/scalequestions.html#scale-questions-1",
    "href": "materials/scalequestions.html#scale-questions-1",
    "title": "Scaled Questions Clusters",
    "section": "",
    "text": "On a scale of 1 to 7, rate how hungry you are; 1 is not hungry at all and 7 is starving.\nOn a scale from 1 to 7 how severe is your phobia or fear.\nOn a scale from 1 to 7, where 1 = strongly disagree and 7 = strongly agree, rate the following statement “psychology is a science”\nShould a convicted sex offender be given an opportunity to be released? (1 disagree 7 agree)\nOn a scale of one to seven, how anxious have you felt this week?",
    "crumbs": [
      "General Info",
      "Course Content",
      "Scale Questions"
    ]
  },
  {
    "objectID": "materials/scalequestions.html#scale-questions-2",
    "href": "materials/scalequestions.html#scale-questions-2",
    "title": "Scaled Questions Clusters",
    "section": "",
    "text": "How confident would you say you are on a scale from 1-10?\nOn a scale of 1 to 10 how confident do you feel about starting the Research and Methods module? (1 being not confident at all, 10 being extremely confident)\nIn a scale of 1 to 10, with 1 being the least confident and 10 being the most confident, how confident do you feel speaking up and asking questions during a seminar?\nOn a scale of 1 to 10, how confident do you feel speaking to new people you’ve never met? (1= least confident, 10= most confident)\nOn a scale from 1-10 how comfortable are you with yourself?\nOn a scale from 1-10 how confident were you when first starting University?\n\n\n\n\n\nHow happy are you on a scale from one to ten?\nHow would you rate your mood right now on a scale of 1 to 10 with 1 being the worst and 10 being the best.\nOn a scale of 1-10, how are you feeling today?\nWhat would you rate your current happiness level on a scale of 1 to 10 (1 being extremely unhappy to 10 being very happy)\nOn a scale from 1-10 how happy are you generally?\nOn a scale of 1-10 how happy do you feel, where 1 is feeling ‘extremely unhappy’ and 10 is feeling ‘extremely happy’?\n\n\n\n\n\nHow much do you like listening to music on a scale of 1-10?\nOn a scale from 1-10 how much do you enjoy psychology?\nOn a scale of 1 to 10, how would you rate your first day at University?\nHow likely, on a scale from 1 to 10, do you feel you will choose the path you want to follow on your career after completing your degree?\nOn a scale of 1 to 10 how would you rate your experience?\n\n\n\n\n\nOn a scale of 1-10, how difficult do you view psychology as?\nOn a scale from 1 to 10, with 10 being absolutely love them and 1 being dirt on my shoe, how do you feel about the Harry Potter movies?\nFrom a scale of 1 to 10, how much do you hate Monday mornings?\nOn a night out how much do you drink (1- one or two, 10- blackout)\nHow much pain do you feel on a scale of 1-10?",
    "crumbs": [
      "General Info",
      "Course Content",
      "Scale Questions"
    ]
  },
  {
    "objectID": "materials/scalequestions.html#other-scales-and-questions",
    "href": "materials/scalequestions.html#other-scales-and-questions",
    "title": "Scaled Questions Clusters",
    "section": "",
    "text": "On a scale from one to five how likely you would ask question out loud in the public, where 1(I would not do it) and 5(i would definitely do it)?\nOn a scale from 1 - 3, How happy are you? 1 being not happy and 3 being happy\nHow often do you feel anxious?\nHow inclined are you to depression?\nHow apprehensive do you feel about starting this degree? 0 (not at all) - 10 (extremely)\nWith 1 being extremely important and 5 being not important at all, how important is future earning potential when choosing your degree?\nHow often do you feel happy or joyful in your day to day life?\nOn a scale of 1-5, where 1=very negative, and 5=very positive, how do you feel about the political state of the world?\nOn a scale from one to ten, how important do you believe rehabilitation is for convicted felons? One being unimportant and ten being crucial.\nAfter the first week at Goldsmiths, how confident are you that psychology is the right subject for you? 0 (not confident at all) - 5 (extremely confident)\nHow would you rate season 5 of Doctor Who on a scale of 1-10?\nOn a scale of 1-10, how significant do you believe the impact of mental health challenges is on men in society?\nHow interested are you in understanding how the human brain influences behaviour?\nOn a scale of 1-9, how vital would you say coffee is in your life, with 1 being unimportant and 9 being extremely vital?\nOn a scale of 1 to 5, how would you describe your stress levels during exam periods, from 5 being very stressed to 1 being not stressed at all?\nHow obedient are you to those with authority?\nI often seek support from others when I am facing challenges.",
    "crumbs": [
      "General Info",
      "Course Content",
      "Scale Questions"
    ]
  },
  {
    "objectID": "materials/slides_2b.html#lab-02-learning-objectives",
    "href": "materials/slides_2b.html#lab-02-learning-objectives",
    "title": "Lab 02 - Data Visualisation I",
    "section": "Lab 02 Learning Objectives",
    "text": "Lab 02 Learning Objectives\nBy the end of lab 02, you will be able to:\n\nAnalyze the value and effectiveness of visuals in a number of real-world contexts\nCritically evaluate the impact of audiovisual stimuli in experimental settings\nIdentify effective data visualisation techniques you have encountered\nVisualise how visualisation could be a tool worth mastering for YOU"
  },
  {
    "objectID": "materials/slides_2b.html#lab-02-schedule---90-mins",
    "href": "materials/slides_2b.html#lab-02-schedule---90-mins",
    "title": "Lab 02 - Data Visualisation I",
    "section": "Lab 02 Schedule - 90 mins",
    "text": "Lab 02 Schedule - 90 mins\n\nIntroduction and Warm-up (10 minutes)\nVisual Analysis of this week’s learning (20 minutes)\nWellbeing Experiment Stimuli Reflection (20 minutes)\nVisual Analysis of Scientific Communications (30 minutes)\nShowcase (10 minutes)"
  },
  {
    "objectID": "materials/slides_2b.html#dataskills-02---30-mins",
    "href": "materials/slides_2b.html#dataskills-02---30-mins",
    "title": "Lab 02 - Data Visualisation I",
    "section": "DataSkills 02 - 30 mins (+)",
    "text": "DataSkills 02 - 30 mins (+)\n\nSpeed Data Questionnaire (15 minutes)\nSubmit own ‘Top Viz’ with rationale (15 minutes)"
  },
  {
    "objectID": "materials/slides_2b.html#introduction-and-warm-up-10-mins",
    "href": "materials/slides_2b.html#introduction-and-warm-up-10-mins",
    "title": "Lab 02 - Data Visualisation I",
    "section": "1. Introduction and Warm-up 10 mins",
    "text": "1. Introduction and Warm-up 10 mins\n\nOverview of lab objectives\nIcebreaker: Share ways you ‘see data’ in your everyday lives\n\n\n\n\n\n\n\nIcebreaker Tip\n\n\nSleep trackers, free-coffee coupons, spotify, charts, sports league tables, fitbit etc\n\n\n\n\nThis activity sets the tone for the lab and gets students thinking about effective visual communication from the start."
  },
  {
    "objectID": "materials/slides_2b.html#visual-analysis-of-this-weeks-learning-20-mins",
    "href": "materials/slides_2b.html#visual-analysis-of-this-weeks-learning-20-mins",
    "title": "Lab 02 - Data Visualisation I",
    "section": "2. Visual Analysis of this week’s learning 20 mins",
    "text": "2. Visual Analysis of this week’s learning 20 mins\nWhat do we (as educators and Psychologists) need to do to help you understand and engage with complex topics like Wellbeing and Positive Psychology?\n- Discuss in pairs - What different techniques are used to achieve this important goal? - It’s not all words (e.g. spoken or text) - What else is in the mix?\n\n\n\n\n\n\nDiscussion Prompts\n\n\n\nVisuals? Do they help or not? Why?\nVideo or audio? Are they worthwhile?\nFonts (begin comic sans argument NOW!)\nEven just the lecturer’s voice, nonverbal behaviour or facial expressions. Are they important?"
  },
  {
    "objectID": "materials/slides_2b.html#wellbeing-experiment-stimuli-reflection-20-mins",
    "href": "materials/slides_2b.html#wellbeing-experiment-stimuli-reflection-20-mins",
    "title": "Lab 02 - Data Visualisation I",
    "section": "3. Wellbeing Experiment Stimuli Reflection 20 mins",
    "text": "3. Wellbeing Experiment Stimuli Reflection 20 mins\n\nReflect on the peace/tranquility vs. urban stress video experiment from the first seminar\nSmall group discussion on the effectiveness of these visual stimuli\nShare insights on how visuals impacted the experimental outcomes\nWhat alternatives exist? Would they be as effective? More effective?\n\n\n\n\n\n\n\nImportant\n\n\nThink about how the videos elicited specific emotional responses and why this is important in psychological research. Would immersive VR be STRONGER? Smells? Infra-sound?"
  },
  {
    "objectID": "materials/slides_2b.html#visual-analysis-of-scientific-communications-20-mins",
    "href": "materials/slides_2b.html#visual-analysis-of-scientific-communications-20-mins",
    "title": "Lab 02 - Data Visualisation I",
    "section": "4. Visual Analysis of Scientific Communications 20 mins",
    "text": "4. Visual Analysis of Scientific Communications 20 mins\n\nIntroduction to the BrowZine app via the library\nTask: Find a psychology journal article and evaluate its use of visuals\nConsider how visuals enhance or detract from the article’s content\nWhat other forms of ‘Scientific Communication’ exist?\n\n\n\n\n\n\n\nEvaluation Criteria\n\n\n\nClarity and relevance, Integration of visuals with text, Aesthetic appeal and professionalism, Contribution to overall understanding of the research\nPosters, Book Covers, Journal Covers, Academic Websites, Conference Posters, Business Cards?, Blogs, Lab Websites, Software interfaces and UX, Branding"
  },
  {
    "objectID": "materials/slides_2b.html#visual-analysis-of-communications-10-mins",
    "href": "materials/slides_2b.html#visual-analysis-of-communications-10-mins",
    "title": "Lab 02 - Data Visualisation I",
    "section": "5. Visual Analysis of Communications 10 mins",
    "text": "5. Visual Analysis of Communications 10 mins\n\nVisualisation and effective communication is bigger than just Psychology\nTask: Think about what you might want to present visually in the future\nConsider how you might benefit from this important skill\n\n\n\n\n\n\n\nNote\n\n\nBusiness Proposals, Marriage Proposals, Websites, Advertising, Blogs, Vlogs & Pods, Merchandise, Wedding invitations, CVs, Portfolios, Dating Profile, Political Campaign, LinkedIn profile etc"
  },
  {
    "objectID": "materials/slides_2b.html#showcase",
    "href": "materials/slides_2b.html#showcase",
    "title": "Lab 02 - Data Visualisation I",
    "section": "6. Showcase",
    "text": "6. Showcase\n\nAlthough not graphs. Are these things to consider?\n\n\n\n\n\n\n\nNote\n\n\nThese are just from Gordon, but please feel free to find any others."
  },
  {
    "objectID": "materials/slides_2b.html#event-shenanigans",
    "href": "materials/slides_2b.html#event-shenanigans",
    "title": "Lab 02 - Data Visualisation I",
    "section": "6.1 Event Shenanigans",
    "text": "6.1 Event Shenanigans"
  },
  {
    "objectID": "materials/slides_2b.html#dataskills-02",
    "href": "materials/slides_2b.html#dataskills-02",
    "title": "Lab 02 - Data Visualisation I",
    "section": "DataSkills 02",
    "text": "DataSkills 02\n\nSpeed Data Questionnaire - including some of your questions!!\nTo be used for next week!\nSubmit a response to the Data Visuals you’ve encountered this week via the link on the VLE\n\nTopViz - Only one good idea required.\n\nReview the results of the Questions we asked you to provide (roughly clustered so far). See VLE\n\n\n\n\n\n\n\nGive it some thought\n\n\n\nMastering Data Visualisation will pay off at Uni\nBut think bigger and consider how learning to communicate can be useful, and whether or not visuals (data-based or otherwise), or design, typography, digital or just artistic skills are worth pursuing"
  },
  {
    "objectID": "materials/slides_2b..htm",
    "href": "materials/slides_2b..htm",
    "title": "Lab 02 - Data Visualisation I",
    "section": "",
    "text": "By the end of lab 02, you will be able to:\n\nAnalyze the value and effectiveness of visuals in a number of real-world contexts\nCritically evaluate the impact of audiovisual stimuli in experimental settings\nIdentify effective data visualisation techniques you have encountered\nVisualise how visualisation could be a tool worth mastering for YOU"
  },
  {
    "objectID": "materials/slides_2b..htm#lab-02-learning-objectives",
    "href": "materials/slides_2b..htm#lab-02-learning-objectives",
    "title": "Lab 02 - Data Visualisation I",
    "section": "",
    "text": "By the end of lab 02, you will be able to:\n\nAnalyze the value and effectiveness of visuals in a number of real-world contexts\nCritically evaluate the impact of audiovisual stimuli in experimental settings\nIdentify effective data visualisation techniques you have encountered\nVisualise how visualisation could be a tool worth mastering for YOU"
  },
  {
    "objectID": "materials/slides_2b..htm#lab-02-schedule---90-mins",
    "href": "materials/slides_2b..htm#lab-02-schedule---90-mins",
    "title": "Lab 02 - Data Visualisation I",
    "section": "Lab 02 Schedule - 90 mins",
    "text": "Lab 02 Schedule - 90 mins\n\nIntroduction and Warm-up (10 minutes)\nVisual Analysis of this week’s learning (20 minutes)\nWellbeing Experiment Stimuli Reflection (20 minutes)\nVisual Analysis of Scientific Communications (30 minutes)\nShowcase (10 minutes)"
  },
  {
    "objectID": "materials/slides_2b..htm#dataskills-02---30-mins",
    "href": "materials/slides_2b..htm#dataskills-02---30-mins",
    "title": "Lab 02 - Data Visualisation I",
    "section": "DataSkills 02 - 30 mins (+)",
    "text": "DataSkills 02 - 30 mins (+)\n\nSpeed Data Questionnaire (15 minutes)\nSubmit own ‘Top Viz’ with rationale (15 minutes)"
  },
  {
    "objectID": "materials/slides_2b..htm#introduction-and-warm-up-10-mins",
    "href": "materials/slides_2b..htm#introduction-and-warm-up-10-mins",
    "title": "Lab 02 - Data Visualisation I",
    "section": "1. Introduction and Warm-up 10 mins",
    "text": "1. Introduction and Warm-up 10 mins\n\nOverview of lab objectives\nIcebreaker: Share ways you ‘see data’ in your everyday lives\n\n\n\n\n\n\n\nIcebreaker Tip\n\n\n\nSleep trackers, free-coffee coupons, spotify, charts, sports league tables, fitbit etc\n\n\n\nThis activity sets the tone for the lab and gets students thinking about effective visual communication from the start."
  },
  {
    "objectID": "materials/slides_2b..htm#visual-analysis-of-this-weeks-learning-20-mins",
    "href": "materials/slides_2b..htm#visual-analysis-of-this-weeks-learning-20-mins",
    "title": "Lab 02 - Data Visualisation I",
    "section": "2. Visual Analysis of this week’s learning 20 mins",
    "text": "2. Visual Analysis of this week’s learning 20 mins\nWhat do we (as educators and Psychologists) need to do to help you understand and engage with complex topics like Wellbeing and Positive Psychology?\n- Discuss in pairs - What different techniques are used to achieve this important goal? - It’s not all words (e.g. spoken or text) - What else is in the mix?\n\n\n\n\n\n\nDiscussion Prompts\n\n\n\n\nVisuals? Do they help or not? Why?\nVideo or audio? Are they worthwhile?\nFonts (begin comic sans argument NOW!)\nEven just the lecturer’s voice, nonverbal behaviour or facial expressions. Are they important?"
  },
  {
    "objectID": "materials/slides_2b..htm#wellbeing-experiment-stimuli-reflection-20-mins",
    "href": "materials/slides_2b..htm#wellbeing-experiment-stimuli-reflection-20-mins",
    "title": "Lab 02 - Data Visualisation I",
    "section": "3. Wellbeing Experiment Stimuli Reflection 20 mins",
    "text": "3. Wellbeing Experiment Stimuli Reflection 20 mins\n\nReflect on the peace/tranquility vs. urban stress video experiment from the first seminar\nSmall group discussion on the effectiveness of these visual stimuli\nShare insights on how visuals impacted the experimental outcomes\nWhat alternatives exist? Would they be as effective? More effective?\n\n\n\n\n\n\n\nImportant\n\n\n\nThink about how the videos elicited specific emotional responses and why this is important in psychological research. Would immersive VR be STRONGER? Smells? Infra-sound?"
  },
  {
    "objectID": "materials/slides_2b..htm#visual-analysis-of-scientific-communications-20-mins",
    "href": "materials/slides_2b..htm#visual-analysis-of-scientific-communications-20-mins",
    "title": "Lab 02 - Data Visualisation I",
    "section": "4. Visual Analysis of Scientific Communications 20 mins",
    "text": "4. Visual Analysis of Scientific Communications 20 mins\n\nIntroduction to the BrowZine app via the library\nTask: Find a psychology journal article and evaluate its use of visuals\nConsider how visuals enhance or detract from the article’s content\nWhat other forms of ‘Scientific Communication’ exist?\n\n\n\n\n\n\n\nEvaluation Criteria\n\n\n\n\nClarity and relevance, Integration of visuals with text, Aesthetic appeal and professionalism, Contribution to overall understanding of the research\nPosters, Book Covers, Journal Covers, Academic Websites, Conference Posters, Business Cards?, Blogs, Lab Websites, Software interfaces and UX, Branding"
  },
  {
    "objectID": "materials/slides_2b..htm#visual-analysis-of-communications-10-mins",
    "href": "materials/slides_2b..htm#visual-analysis-of-communications-10-mins",
    "title": "Lab 02 - Data Visualisation I",
    "section": "5. Visual Analysis of Communications 10 mins",
    "text": "5. Visual Analysis of Communications 10 mins\n\nVisualisation and effective communication is bigger than just Psychology\nTask: Think about what you might want to present visually in the future\nConsider how you might benefit from this important skill\n\n\n\n\n\n\n\nNote\n\n\n\nBusiness Proposals, Marriage Proposals, Websites, Advertising, Blogs, Vlogs & Pods, Merchandise, Wedding invitations, CVs, Portfolios, Dating Profile, Political Campaign, LinkedIn profile etc"
  },
  {
    "objectID": "materials/slides_2b..htm#showcase",
    "href": "materials/slides_2b..htm#showcase",
    "title": "Lab 02 - Data Visualisation I",
    "section": "6. Showcase",
    "text": "6. Showcase\n\nAlthough not graphs. Are these things to consider?\n\n\n\n\n\n\n\nNote\n\n\n\nThese are just from Gordon, but please feel free to find any others."
  },
  {
    "objectID": "materials/slides_2b..htm#event-shenanigans",
    "href": "materials/slides_2b..htm#event-shenanigans",
    "title": "Lab 02 - Data Visualisation I",
    "section": "6.1 Event Shenanigans",
    "text": "6.1 Event Shenanigans"
  },
  {
    "objectID": "materials/slides_2b..htm#dataskills-02",
    "href": "materials/slides_2b..htm#dataskills-02",
    "title": "Lab 02 - Data Visualisation I",
    "section": "DataSkills 02",
    "text": "DataSkills 02\n\nSpeed Data Questionnaire - including some of your questions!!\nTo be used for next week!\nSubmit a response to the Data Visuals you’ve encountered this week via the link on the VLE\n\nTopViz - Only one good idea required.\n\nReview the results of the Questions we asked you to provide (roughly clustered so far). See VLE\n\n\n\n\n\n\n\nGive it some thought\n\n\n\n\nMastering Data Visualisation will pay off at Uni\nBut think bigger and consider how learning to communicate can be useful, and whether or not visuals (data-based or otherwise), or design, typography, digital or just artistic skills are worth pursuing"
  },
  {
    "objectID": "materials/slides_3a.html#introduction-to-data-types",
    "href": "materials/slides_3a.html#introduction-to-data-types",
    "title": "Intro to Data Visualisation II",
    "section": "Introduction to Data Types",
    "text": "Introduction to Data Types\nIn statistics, we work with various types of data. Understanding these types is crucial for:\n\nChoosing appropriate statistical methods\nInterpreting results correctly\nMaking informed decisions based on data\n\nLet’s explore the four main levels of measurement…\n\nEmphasize the importance of understanding data types for statistical analysis."
  },
  {
    "objectID": "materials/slides_3a.html#nominal-data",
    "href": "materials/slides_3a.html#nominal-data",
    "title": "Intro to Data Visualisation II",
    "section": "Nominal Data",
    "text": "Nominal Data\n\nDefinition: Categories or groups without intrinsic order\nCharacteristics:\n\nCannot be ordered\nNo numerical value\nOnly shows distinct groups\n\nExamples from our dataset:\n\nMBTI (Myers-Briggs Type Indicator)\nCoin (Heads or Tails)\nDogCatBoth (Preference for pets)\n\n\n\nExplain that nominal data is qualitative and used for labeling variables without any quantitative value."
  },
  {
    "objectID": "materials/slides_3a.html#ordinal-data",
    "href": "materials/slides_3a.html#ordinal-data",
    "title": "Intro to Data Visualisation II",
    "section": "Ordinal Data",
    "text": "Ordinal Data\n\nDefinition: Categories with a meaningful order, but differences aren’t measurable\nCharacteristics:\n\nCan be ordered\nIntervals between ranks aren’t necessarily equal\n\nExamples from our dataset:\n\n1-7 rating scales (Likert scale responses)\nTIPI scores (Big Five - OCEAN)\n\n\n\nHighlight that ordinal data has an order, but the distance between categories is not uniform or meaningful."
  },
  {
    "objectID": "materials/slides_3a.html#interval-data",
    "href": "materials/slides_3a.html#interval-data",
    "title": "Intro to Data Visualisation II",
    "section": "Interval Data",
    "text": "Interval Data\n\nDefinition: Numerical data with consistent intervals, but no true zero point\nCharacteristics:\n\nOrdered with equal intervals between values\nCan be added or subtracted\nNo true zero point\n\nExamples:\n\nTemperature in Celsius or Fahrenheit\nCalendar years\n\n\n\nExplain that interval data allows for degree of difference, but not ratio comparisons."
  },
  {
    "objectID": "materials/slides_3a.html#ratio-data",
    "href": "materials/slides_3a.html#ratio-data",
    "title": "Intro to Data Visualisation II",
    "section": "Ratio Data",
    "text": "Ratio Data\n\nDefinition: Numerical data with equal intervals and a true zero point\nCharacteristics:\n\nOrdered with equal intervals\nHas a true zero point (absence of the variable is possible)\nCan be added, subtracted, multiplied, and divided\n\nExamples from our dataset:\n\nLoginCount\nCompTime (assuming it’s recorded in minutes)\nEyeContact (assuming it’s a continuous measure)\n\n\n\nEmphasize that ratio data allows for all arithmetic operations and comparisons."
  },
  {
    "objectID": "materials/slides_3a.html#summary-of-data-types",
    "href": "materials/slides_3a.html#summary-of-data-types",
    "title": "Intro to Data Visualisation II",
    "section": "Summary of Data Types",
    "text": "Summary of Data Types\n\n\n\n\n\n\n\n\n\n\nData Type\nCan Be Ordered?\nEqual Intervals?\nTrue Zero Point?\nExample\n\n\n\n\nNominal\nNo\nNo\nNo\nGender, MBTI\n\n\nOrdinal\nYes\nNo\nNo\nScale measures, TIPI scores\n\n\nInterval\nYes\nYes\nNo\nTemperature (°C)\n\n\nRatio\nYes\nYes\nYes\nLoginCount, CompTime\n\n\n\n\nReview the key differences between the four levels of measurement."
  },
  {
    "objectID": "materials/slides_3a.html#practical-application",
    "href": "materials/slides_3a.html#practical-application",
    "title": "Intro to Data Visualisation II",
    "section": "Practical Application",
    "text": "Practical Application\nLet’s look at some variables from our dataset:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nExplain how to identify the data type of each variable and why it matters for analysis."
  },
  {
    "objectID": "materials/slides_3a.html#conclusion-and-next-steps",
    "href": "materials/slides_3a.html#conclusion-and-next-steps",
    "title": "Intro to Data Visualisation II",
    "section": "Conclusion and Next Steps",
    "text": "Conclusion and Next Steps\n\nUnderstanding data types is crucial for proper statistical analysis\nIn our dataset, we have a mix of nominal, ordinal, and ratio data\nNext, we’ll explore how to summarize and visualize these different types of data\n\n\nSummarize the key points about data types and preview the next part of the lecture."
  },
  {
    "objectID": "materials/slides_3a.html#introduction-to-measures-of-central-tendency",
    "href": "materials/slides_3a.html#introduction-to-measures-of-central-tendency",
    "title": "Intro to Data Visualisation II",
    "section": "Introduction to Measures of Central Tendency",
    "text": "Introduction to Measures of Central Tendency\nMeasures of central tendency help us understand the typical or central value in a dataset. The three main measures are:\n\nMean\nMedian\nMode\n\nLet’s explore each of these using our dataset…\n\nExplain that these measures help summarize data and provide a “typical” value."
  },
  {
    "objectID": "materials/slides_3a.html#mean",
    "href": "materials/slides_3a.html#mean",
    "title": "Intro to Data Visualisation II",
    "section": "Mean",
    "text": "Mean\n\nDefinition: The average of all values in a dataset\nFormula: \\(\\bar{x} = \\frac{\\sum{X}}{N}\\)\nBest for: Interval and ratio data\nExample: Let’s calculate the mean LoginCount\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nExplain that the mean is sensitive to outliers and works best with symmetric distributions."
  },
  {
    "objectID": "materials/slides_3a.html#median",
    "href": "materials/slides_3a.html#median",
    "title": "Intro to Data Visualisation II",
    "section": "Median",
    "text": "Median\n\nDefinition: The middle value when data is ordered\nBest for: Ordinal, interval, and ratio data\nExample: Let’s find the median LoginCount\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nHighlight that the median is less affected by outliers than the mean."
  },
  {
    "objectID": "materials/slides_3a.html#mode",
    "href": "materials/slides_3a.html#mode",
    "title": "Intro to Data Visualisation II",
    "section": "Mode",
    "text": "Mode\n\nDefinition: The most frequently occurring value\nBest for: Any type of data, especially nominal\nExample: Let’s find the mode of DogCatBoth\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nExplain that mode is the only measure of central tendency for nominal data."
  },
  {
    "objectID": "materials/slides_3a.html#comparing-measures-of-central-tendency",
    "href": "materials/slides_3a.html#comparing-measures-of-central-tendency",
    "title": "Intro to Data Visualisation II",
    "section": "Comparing Measures of Central Tendency",
    "text": "Comparing Measures of Central Tendency\nLet’s compare these measures for LoginCount:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nDiscuss what these differences might tell us about the distribution of LoginCount."
  },
  {
    "objectID": "materials/slides_3a.html#introduction-to-measures-of-variance",
    "href": "materials/slides_3a.html#introduction-to-measures-of-variance",
    "title": "Intro to Data Visualisation II",
    "section": "Introduction to Measures of Variance",
    "text": "Introduction to Measures of Variance\nMeasures of variance help us understand the spread or dispersion of data. Key measures include:\n\nRange\nInterquartile Range (IQR)\nVariance\nStandard Deviation\n\n\nExplain that these measures complement central tendency by showing how spread out the data is."
  },
  {
    "objectID": "materials/slides_3a.html#range-and-interquartile-range-iqr",
    "href": "materials/slides_3a.html#range-and-interquartile-range-iqr",
    "title": "Intro to Data Visualisation II",
    "section": "Range and Interquartile Range (IQR)",
    "text": "Range and Interquartile Range (IQR)\n\nRange: Difference between the maximum and minimum values\nIQR: Range of the middle 50% of the data\n\nLet’s calculate these for LoginCount:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nExplain that range is sensitive to outliers, while IQR is more robust."
  },
  {
    "objectID": "materials/slides_3a.html#variance-and-standard-deviation",
    "href": "materials/slides_3a.html#variance-and-standard-deviation",
    "title": "Intro to Data Visualisation II",
    "section": "Variance and Standard Deviation",
    "text": "Variance and Standard Deviation\n\nVariance: Average squared deviation from the mean\nStandard Deviation: Square root of the variance\n\nLet’s calculate these for LoginCount:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nExplain that standard deviation is in the same units as the original data, making it easier to interpret."
  },
  {
    "objectID": "materials/slides_3a.html#interpreting-standard-deviation",
    "href": "materials/slides_3a.html#interpreting-standard-deviation",
    "title": "Intro to Data Visualisation II",
    "section": "Interpreting Standard Deviation",
    "text": "Interpreting Standard Deviation\n\nIn a normal distribution:\n\nAbout 68% of data falls within 1 SD of the mean\nAbout 95% falls within 2 SD\nAbout 99.7% falls within 3 SD\n\n\nLet’s visualize this for LoginCount:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nExplain how to interpret this visualization and what it tells us about the spread of LoginCount."
  },
  {
    "objectID": "materials/slides_3a.html#application-to-different-data-types",
    "href": "materials/slides_3a.html#application-to-different-data-types",
    "title": "Intro to Data Visualisation II",
    "section": "Application to Different Data Types",
    "text": "Application to Different Data Types\nLet’s apply these concepts to different types of data in our dataset:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nDiscuss how the choice of measure depends on the type of data, and how to interpret these results."
  },
  {
    "objectID": "materials/slides_3a.html#conclusion-and-next-steps-1",
    "href": "materials/slides_3a.html#conclusion-and-next-steps-1",
    "title": "Intro to Data Visualisation II",
    "section": "Conclusion and Next Steps",
    "text": "Conclusion and Next Steps\n\nMeasures of central tendency and variance provide crucial summaries of our data\nThe choice of measure depends on the type of data and the shape of its distribution\nThese measures form the basis for more advanced statistical analyses\nNext, we’ll explore how to visualize these concepts and dive deeper into our dataset\n\n\nSummarize the key points about central tendency and variance, and preview the next part on data visualization."
  },
  {
    "objectID": "materials/slides_3a.html#introduction-to-data-exploration",
    "href": "materials/slides_3a.html#introduction-to-data-exploration",
    "title": "Intro to Data Visualisation II",
    "section": "Introduction to Data Exploration",
    "text": "Introduction to Data Exploration\nData exploration involves:\n\nVisualizing data distributions\nIdentifying patterns and relationships\nDetecting outliers and anomalies\n\nWe’ll use various chart types to explore our dataset.\n\nExplain the importance of data exploration in understanding the dataset before applying more advanced statistical techniques."
  },
  {
    "objectID": "materials/slides_3a.html#histogram-login-count-distribution",
    "href": "materials/slides_3a.html#histogram-login-count-distribution",
    "title": "Intro to Data Visualisation II",
    "section": "Histogram: Login Count Distribution",
    "text": "Histogram: Login Count Distribution\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nDiscuss the shape of the distribution, any skewness, and what it tells us about student engagement."
  },
  {
    "objectID": "materials/slides_3a.html#bar-chart-pet-preferences",
    "href": "materials/slides_3a.html#bar-chart-pet-preferences",
    "title": "Intro to Data Visualisation II",
    "section": "Bar Chart: Pet Preferences",
    "text": "Bar Chart: Pet Preferences\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nInterpret the bar chart, discussing the most common preference and potential implications for student engagement strategies."
  },
  {
    "objectID": "materials/slides_3a.html#scatter-plot-eye-contact-vs-login-count",
    "href": "materials/slides_3a.html#scatter-plot-eye-contact-vs-login-count",
    "title": "Intro to Data Visualisation II",
    "section": "Scatter Plot: Eye Contact vs Login Count",
    "text": "Scatter Plot: Eye Contact vs Login Count\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nDiscuss any visible patterns or correlations, and what they might imply about the relationship between online engagement and social comfort."
  },
  {
    "objectID": "materials/slides_3a.html#box-plot-completion-time-by-conscientiousness",
    "href": "materials/slides_3a.html#box-plot-completion-time-by-conscientiousness",
    "title": "Intro to Data Visualisation II",
    "section": "Box Plot: Completion Time by Conscientiousness",
    "text": "Box Plot: Completion Time by Conscientiousness\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nInterpret the box plot, discussing differences in completion time between high and low conscientiousness groups."
  },
  {
    "objectID": "materials/slides_3a.html#radar-chart-tipi-personality-traits",
    "href": "materials/slides_3a.html#radar-chart-tipi-personality-traits",
    "title": "Intro to Data Visualisation II",
    "section": "Radar Chart: TIPI Personality Traits",
    "text": "Radar Chart: TIPI Personality Traits\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nExplain how to interpret the radar chart and discuss the overall personality profile of the class based on TIPI scores."
  },
  {
    "objectID": "materials/slides_3a.html#mbti-distribution",
    "href": "materials/slides_3a.html#mbti-distribution",
    "title": "Intro to Data Visualisation II",
    "section": "MBTI Distribution",
    "text": "MBTI Distribution\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nDiscuss the distribution of MBTI types in the class and potential implications for teaching strategies."
  },
  {
    "objectID": "materials/slides_3a.html#correlation-heatmap-tipi-traits",
    "href": "materials/slides_3a.html#correlation-heatmap-tipi-traits",
    "title": "Intro to Data Visualisation II",
    "section": "Correlation Heatmap: TIPI Traits",
    "text": "Correlation Heatmap: TIPI Traits\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nInterpret the correlation heatmap, discussing which personality traits tend to be related and which are more independent."
  },
  {
    "objectID": "materials/slides_3a.html#exploring-relationships-logincount-and-tipi-traits",
    "href": "materials/slides_3a.html#exploring-relationships-logincount-and-tipi-traits",
    "title": "Intro to Data Visualisation II",
    "section": "Exploring Relationships: LoginCount and TIPI Traits",
    "text": "Exploring Relationships: LoginCount and TIPI Traits\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nDiscuss any notable correlations between login count and personality traits, and what this might imply about online engagement."
  },
  {
    "objectID": "materials/slides_3a.html#violin-plot-eyecontact-by-mbti-extraversionintroversion",
    "href": "materials/slides_3a.html#violin-plot-eyecontact-by-mbti-extraversionintroversion",
    "title": "Intro to Data Visualisation II",
    "section": "Violin Plot: EyeContact by MBTI Extraversion/Introversion",
    "text": "Violin Plot: EyeContact by MBTI Extraversion/Introversion\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nInterpret the violin plot, discussing differences in eye contact scores between extraverts and introverts."
  },
  {
    "objectID": "materials/slides_3a.html#stacked-bar-chart-mbti-types-by-pet-preference",
    "href": "materials/slides_3a.html#stacked-bar-chart-mbti-types-by-pet-preference",
    "title": "Intro to Data Visualisation II",
    "section": "Stacked Bar Chart: MBTI Types by Pet Preference",
    "text": "Stacked Bar Chart: MBTI Types by Pet Preference\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nDiscuss any interesting patterns in pet preferences across MBTI types and potential implications for understanding student personalities."
  },
  {
    "objectID": "materials/slides_3a.html#conclusion-and-key-insights",
    "href": "materials/slides_3a.html#conclusion-and-key-insights",
    "title": "Intro to Data Visualisation II",
    "section": "Conclusion and Key Insights",
    "text": "Conclusion and Key Insights\n\nRecap the main findings from our data exploration\nDiscuss how these insights can inform teaching strategies and student engagement\nHighlight the importance of data-driven decision making in education\n\n\nSummarize the key takeaways from the data exploration and how they can be applied in the educational context."
  },
  {
    "objectID": "materials/slides_3a.html#next-steps",
    "href": "materials/slides_3a.html#next-steps",
    "title": "Intro to Data Visualisation II",
    "section": "Next Steps",
    "text": "Next Steps\n\nEncourage further exploration of the dataset\nDiscuss potential research questions that could be investigated\nIntroduce more advanced statistical techniques for future analysis\n\n\nProvide direction for students to continue their statistical journey and apply these concepts to their own research interests."
  },
  {
    "objectID": "materials/slides_3a.html#platos-triad-the-true-the-good-and-the-beautiful",
    "href": "materials/slides_3a.html#platos-triad-the-true-the-good-and-the-beautiful",
    "title": "Intro to Data Visualisation II",
    "section": "Plato’s Triad: The True, The Good, and The Beautiful",
    "text": "Plato’s Triad: The True, The Good, and The Beautiful\nPlato believed in the intrinsic connection between truth, goodness, and beauty — a concept that has influenced Western thought for centuries.\nFor Plato, these three qualities are inseparable in the realm of Forms.\nThe truth is inherently good, and what is good is also inherently beautiful.\nThus, if something is false, it cannot be truly beautiful or good."
  },
  {
    "objectID": "materials/slides_3a.html#financial-times",
    "href": "materials/slides_3a.html#financial-times",
    "title": "Intro to Data Visualisation II",
    "section": "Financial Times",
    "text": "Financial Times\nVisual Vocabulary is a site for stuff ## Introduction\n\n“Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” —John W. Tukey\n\n\nDescriptive statistics are tools for describing data\nThere are many ways to describe data\nChoose the most useful way for your data\nDescribing data is necessary because there’s usually too much of it"
  },
  {
    "objectID": "materials/slides_3a.html#too-many-numbers",
    "href": "materials/slides_3a.html#too-many-numbers",
    "title": "Intro to Data Visualisation II",
    "section": "Too Many Numbers",
    "text": "Too Many Numbers\n\nExample: Asking thousands of people about their happiness\nRaw data is overwhelming and difficult to interpret\n\n\n\n\n\n\n694\n766\n781\n279\n145\n386\n-155\n59\n347\n-72\n\n\n743\n695\n-907\n121\n180\n730\n744\n-1057\n-8\n745\n\n\n124\n236\n147\n26\n523\n27\n436\n-86\n-483\n293\n\n\n-148\n-94\n-47\n737\n-639\n37\n-200\n313\n679\n-261\n\n\n1315\n-200\n266\n-929\n145\n-413\n349\n-159\n-657\n-287"
  },
  {
    "objectID": "materials/slides_3a.html#what-do-you-currently-use",
    "href": "materials/slides_3a.html#what-do-you-currently-use",
    "title": "Intro to Data Visualisation II",
    "section": "What do you currently use?",
    "text": "What do you currently use?\nHow do you write your essays or lab reports?\n\nMicrosoft Word?\nGoogle Docs?\nMarkdown?\n\n\nHow do you currently play with numbers?\n\nExcel?\nSPSS?\nR?\nPython?"
  },
  {
    "objectID": "materials/slides_3a.html#what-is-quarto",
    "href": "materials/slides_3a.html#what-is-quarto",
    "title": "Intro to Data Visualisation II",
    "section": "What is Quarto?",
    "text": "What is Quarto?\nQuarto is an open-source scientific and technical publishing system that allows you to combine text, images, code, plots, and tables in a fully-reproducible document.\n\nQuarto has support for multiple languages including R, Python, Julia, and Observable.\n\nIt also works for a range of output formats such as PDFs, HTML documents, websites, presentations,…"
  },
  {
    "objectID": "materials/slides_3a.html#why-use-quarto-why-use-r",
    "href": "materials/slides_3a.html#why-use-quarto-why-use-r",
    "title": "Intro to Data Visualisation II",
    "section": "Why use Quarto? Why use R?",
    "text": "Why use Quarto? Why use R?\n\nMore journals require code to be submitted (for transparency and reproducibility). Keeping the code with the paper makes this easier.\nCopying and pasting is tedious (and a great source of accidental errors).\nIf you fix an error in code or data, the results and figures in the paper update automatically.\nEasy to share publicly.\nOpen source so anyone can use it."
  },
  {
    "objectID": "materials/slides_3a.html#what-about-r-markdown",
    "href": "materials/slides_3a.html#what-about-r-markdown",
    "title": "Intro to Data Visualisation II",
    "section": "What about R Markdown?",
    "text": "What about R Markdown?\nR Markdown isn’t going anywhere but…\n\nQuarto has better multi-language support\nMore user-friendly\nBetter control of the output layouts"
  },
  {
    "objectID": "materials/slides_3a.html#creating-a-document",
    "href": "materials/slides_3a.html#creating-a-document",
    "title": "Intro to Data Visualisation II",
    "section": "Creating a document",
    "text": "Creating a document"
  },
  {
    "objectID": "materials/slides_3a.html#quarto-in-rstudio",
    "href": "materials/slides_3a.html#quarto-in-rstudio",
    "title": "Intro to Data Visualisation II",
    "section": "Quarto in RStudio",
    "text": "Quarto in RStudio\n\n\nSource editor\n\n\n\n\n\n\nVisual editor"
  },
  {
    "objectID": "materials/slides_3a.html#rendering-a-document",
    "href": "materials/slides_3a.html#rendering-a-document",
    "title": "Intro to Data Visualisation II",
    "section": "Rendering a document",
    "text": "Rendering a document\nWithin RStudio IDE: click Render (or Ctrl+Shift+K)\n\n\nUsing {quarto}\n\nlibrary(quarto)\nquarto_render(\"document.qmd\")\n\n\n\n\nUsing the command line\nquarto render document.qmd"
  },
  {
    "objectID": "materials/slides_3a.html#what-makes-a-quarto-document",
    "href": "materials/slides_3a.html#what-makes-a-quarto-document",
    "title": "Intro to Data Visualisation II",
    "section": "What makes a Quarto document?",
    "text": "What makes a Quarto document?\nYAML header\n---\ntitle: \"A very cool title\"\nformat: html\n---\n\nContent\n\nText, links, images\nCode, tables, plots\nEquations, references"
  },
  {
    "objectID": "materials/slides_3a.html#output-types",
    "href": "materials/slides_3a.html#output-types",
    "title": "Intro to Data Visualisation II",
    "section": "Output types",
    "text": "Output types\n\n\nDocuments: HTML, PDF, MS Word, Markdown\nPresentations: Revealjs, PowerPoint, Beamer\nWebsites\nBooks\n…"
  },
  {
    "objectID": "materials/slides_3a.html#looking-at-data-graphs",
    "href": "materials/slides_3a.html#looking-at-data-graphs",
    "title": "Intro to Data Visualisation II",
    "section": "Looking at Data: Graphs",
    "text": "Looking at Data: Graphs\nScatter Plot"
  },
  {
    "objectID": "materials/slides_3a.html#looking-at-data-histograms",
    "href": "materials/slides_3a.html#looking-at-data-histograms",
    "title": "Intro to Data Visualisation II",
    "section": "Looking at Data: Histograms",
    "text": "Looking at Data: Histograms\n\n\nShows the distribution of data\nReveals shape, center, and spread"
  },
  {
    "objectID": "materials/slides_3a.html#important-ideas",
    "href": "materials/slides_3a.html#important-ideas",
    "title": "Intro to Data Visualisation II",
    "section": "Important Ideas",
    "text": "Important Ideas\n\nDistribution\nCentral Tendency\nVariance"
  },
  {
    "objectID": "materials/slides_3a.html#measures-of-central-tendency",
    "href": "materials/slides_3a.html#measures-of-central-tendency",
    "title": "Intro to Data Visualisation II",
    "section": "Measures of Central Tendency",
    "text": "Measures of Central Tendency\n\nMode\nMedian\nMean"
  },
  {
    "objectID": "materials/slides_3a.html#mode-1",
    "href": "materials/slides_3a.html#mode-1",
    "title": "Intro to Data Visualisation II",
    "section": "Mode",
    "text": "Mode\n\nMost frequently occurring number\nExample: 1, 1, 1, 2, 3, 4, 5, 6\nMode = 1"
  },
  {
    "objectID": "materials/slides_3a.html#median-1",
    "href": "materials/slides_3a.html#median-1",
    "title": "Intro to Data Visualisation II",
    "section": "Median",
    "text": "Median\n\nMiddle value when data is ordered\nExample: 1, 3, 4, 5, 6, 7, 9\nMedian = 5"
  },
  {
    "objectID": "materials/slides_3a.html#mean-1",
    "href": "materials/slides_3a.html#mean-1",
    "title": "Intro to Data Visualisation II",
    "section": "Mean",
    "text": "Mean\n\\(Mean = \\bar{X} = \\frac{\\sum_{i=1}^{n} x_{i}}{N}\\)\n\nSum of all values divided by the number of values\nRepresents the “average”"
  },
  {
    "objectID": "materials/slides_3a.html#measures-of-variation",
    "href": "materials/slides_3a.html#measures-of-variation",
    "title": "Intro to Data Visualisation II",
    "section": "Measures of Variation",
    "text": "Measures of Variation\n\nRange\nVariance\nStandard Deviation"
  },
  {
    "objectID": "materials/slides_3a.html#range",
    "href": "materials/slides_3a.html#range",
    "title": "Intro to Data Visualisation II",
    "section": "Range",
    "text": "Range\n\nMinimum and maximum values\nExample: 1, 3, 4, 5, 5, 6, 7, 8, 9, 24\nRange: 1 to 24"
  },
  {
    "objectID": "materials/slides_3a.html#variance",
    "href": "materials/slides_3a.html#variance",
    "title": "Intro to Data Visualisation II",
    "section": "Variance",
    "text": "Variance\n\\(variance = \\frac{\\text{Sum of squared difference scores}}{\\text{Number of Scores}}\\)\n\nMeasures spread of data around the mean"
  },
  {
    "objectID": "materials/slides_3a.html#standard-deviation",
    "href": "materials/slides_3a.html#standard-deviation",
    "title": "Intro to Data Visualisation II",
    "section": "Standard Deviation",
    "text": "Standard Deviation\n\\(\\text{standard deviation} = \\sqrt{\\frac{\\sum_{i}^{n}({x_{i}-\\bar{x})^2}}{N}}\\)\n\nSquare root of variance\nIn same units as original data"
  },
  {
    "objectID": "materials/slides_3a.html#using-descriptive-statistics",
    "href": "materials/slides_3a.html#using-descriptive-statistics",
    "title": "Intro to Data Visualisation II",
    "section": "Using Descriptive Statistics",
    "text": "Using Descriptive Statistics\n\nReduce large datasets to summary statistics\nCombine with graphical representations\nBe aware of limitations (e.g., Anscombe’s Quartet)"
  },
  {
    "objectID": "materials/slides_3a.html#anscombes-quartet",
    "href": "materials/slides_3a.html#anscombes-quartet",
    "title": "Intro to Data Visualisation II",
    "section": "Anscombe’s Quartet",
    "text": "Anscombe’s Quartet\n\n\nSame descriptive statistics, different patterns"
  },
  {
    "objectID": "materials/slides_3a.html#remember",
    "href": "materials/slides_3a.html#remember",
    "title": "Intro to Data Visualisation II",
    "section": "Remember",
    "text": "Remember\n\nAlways look at your data\nCombine descriptive statistics with visualizations\nBe aware of potential hidden patterns\n\n\n\n\nData Visualisation II"
  },
  {
    "objectID": "materials/GraphTypes.html#what-for",
    "href": "materials/GraphTypes.html#what-for",
    "title": "GraphTypes",
    "section": "What for",
    "text": "What for\nHistograms are used to study the distribution of one or a few variables. Checking the distribution of your variables one by one is probably the first task you should do when you get a new dataset. It delivers a good quantity of information. Several distribution shapes exist, here is an illustration of the 6 most common ones:"
  },
  {
    "objectID": "materials/GraphTypes.html#examples",
    "href": "materials/GraphTypes.html#examples",
    "title": "GraphTypes",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "materials/GraphTypes.html#useful-for",
    "href": "materials/GraphTypes.html#useful-for",
    "title": "GraphTypes",
    "section": "Useful for:",
    "text": "Useful for:\nChecking this distribution also helps you discovering mistakes in the data. For example, the comb distribution can often denote a rounding that has been applied to the variable or another mistake.\nAs a second step, histogram allow to compare the distribution of a few variables. Don’t compare more than 3 or 4, it would make the figure cluttered and unreadable. This comparison can be done showing the 2 variables on the same graphic and using transparency."
  },
  {
    "objectID": "materials/GraphTypes.html#example",
    "href": "materials/GraphTypes.html#example",
    "title": "GraphTypes",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "materials/GraphTypes.html#variation",
    "href": "materials/GraphTypes.html#variation",
    "title": "GraphTypes",
    "section": "Variation",
    "text": "Variation"
  },
  {
    "objectID": "materials/GraphTypes.html#common-mistakes",
    "href": "materials/GraphTypes.html#common-mistakes",
    "title": "GraphTypes",
    "section": "Common mistakes",
    "text": "Common mistakes"
  },
  {
    "objectID": "materials/GraphTypes.html#what-for-1",
    "href": "materials/GraphTypes.html#what-for-1",
    "title": "GraphTypes",
    "section": "What for",
    "text": "What for\nDensity plots are used to study the distribution of one or a few variables. Checking the distribution of your variables one by one is probably the first task you should do when you get a new dataset. It delivers a good quantity of information. Several distribution shapes exist, here is an illustration of the 6 most common ones:"
  },
  {
    "objectID": "materials/GraphTypes.html#examples-1",
    "href": "materials/GraphTypes.html#examples-1",
    "title": "GraphTypes",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "materials/GraphTypes.html#useful-for-1",
    "href": "materials/GraphTypes.html#useful-for-1",
    "title": "GraphTypes",
    "section": "Useful for:",
    "text": "Useful for:\nChecking this distribution also helps you discovering mistakes in the data. For example, the comb distribution can often denote a rounding that has been applied to the variable or another mistake.\nAs a second step, density plots allow to compare the distribution of a few variables. Don’t compare more than 3 or 4, it would make the figure cluttered and unreadable. This comparison can be done showing the 2 variables on the same graphic and using transparency."
  },
  {
    "objectID": "materials/GraphTypes.html#example-1",
    "href": "materials/GraphTypes.html#example-1",
    "title": "GraphTypes",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "materials/GraphTypes.html#variation-1",
    "href": "materials/GraphTypes.html#variation-1",
    "title": "GraphTypes",
    "section": "Variation",
    "text": "Variation"
  },
  {
    "objectID": "materials/GraphTypes.html#common-mistakes-1",
    "href": "materials/GraphTypes.html#common-mistakes-1",
    "title": "GraphTypes",
    "section": "Common mistakes",
    "text": "Common mistakes"
  },
  {
    "objectID": "materials/GraphTypes.html#what-for-2",
    "href": "materials/GraphTypes.html#what-for-2",
    "title": "GraphTypes",
    "section": "What for",
    "text": "What for\nA scatterplot is made to study the relationship between 2 variables. Thus it is often accompanied by a correlation coefficient calculation, that usually tries to measure the linear relationship.\nHowever other types of relationship can be detected using scatterplots, and a common task consists to fit a model explaining Y in function of X. Here are a few patterns you can detect doing a scatterplot."
  },
  {
    "objectID": "materials/GraphTypes.html#examples-2",
    "href": "materials/GraphTypes.html#examples-2",
    "title": "GraphTypes",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "materials/GraphTypes.html#useful-for-2",
    "href": "materials/GraphTypes.html#useful-for-2",
    "title": "GraphTypes",
    "section": "Useful for:",
    "text": "Useful for:\n\nDetecting relationships between two variables\nIdentifying patterns or trends in data\nSpotting outliers or unusual data points\nComparing different groups or categories within the data"
  },
  {
    "objectID": "materials/GraphTypes.html#variation-2",
    "href": "materials/GraphTypes.html#variation-2",
    "title": "GraphTypes",
    "section": "Variation",
    "text": "Variation"
  },
  {
    "objectID": "materials/GraphTypes.html#common-mistakes-2",
    "href": "materials/GraphTypes.html#common-mistakes-2",
    "title": "GraphTypes",
    "section": "Common mistakes",
    "text": "Common mistakes"
  },
  {
    "objectID": "materials/GraphTypes.html#what-for-3",
    "href": "materials/GraphTypes.html#what-for-3",
    "title": "GraphTypes",
    "section": "What for",
    "text": "What for\nA barplot shows the relationship between a numeric and a categoric variable. In the previous graphic, each country is a level of the categoric variable, and the quantity of weapon sold is the numeric variable. An ordered barplot is a very good choice here since it displays both the ranking of countries and their specific value.\nA barplot can also display values for several levels of grouping. Here’s an example of a grouped barplot:"
  },
  {
    "objectID": "materials/GraphTypes.html#examples-3",
    "href": "materials/GraphTypes.html#examples-3",
    "title": "GraphTypes",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "materials/GraphTypes.html#useful-for-3",
    "href": "materials/GraphTypes.html#useful-for-3",
    "title": "GraphTypes",
    "section": "Useful for:",
    "text": "Useful for:\n\nComparing values across categories\nShowing the distribution of a numeric variable for different groups\nDisplaying rankings or ordered data\nVisualizing part-to-whole relationships (in stacked bar charts)"
  },
  {
    "objectID": "materials/GraphTypes.html#variation-3",
    "href": "materials/GraphTypes.html#variation-3",
    "title": "GraphTypes",
    "section": "Variation",
    "text": "Variation"
  },
  {
    "objectID": "materials/GraphTypes.html#common-mistakes-3",
    "href": "materials/GraphTypes.html#common-mistakes-3",
    "title": "GraphTypes",
    "section": "Common mistakes",
    "text": "Common mistakes"
  },
  {
    "objectID": "materials/GraphTypes.html#what-for-4",
    "href": "materials/GraphTypes.html#what-for-4",
    "title": "GraphTypes",
    "section": "What for",
    "text": "What for\nLine charts can be used to show the evolution of one or several variables. Here is an example showing the evolution of three baby name frequencies in the US between 1880 and 2015:"
  },
  {
    "objectID": "materials/GraphTypes.html#useful-for-4",
    "href": "materials/GraphTypes.html#useful-for-4",
    "title": "GraphTypes",
    "section": "Useful for:",
    "text": "Useful for:\n\nVisualizing trends over time\nComparing multiple variables or categories over a continuous axis\nShowing the rate of change between data points\nIdentifying patterns, cycles, or anomalies in data"
  },
  {
    "objectID": "materials/GraphTypes.html#variation-4",
    "href": "materials/GraphTypes.html#variation-4",
    "title": "GraphTypes",
    "section": "Variation",
    "text": "Variation"
  },
  {
    "objectID": "materials/GraphTypes.html#common-caveats",
    "href": "materials/GraphTypes.html#common-caveats",
    "title": "GraphTypes",
    "section": "Common caveats",
    "text": "Common caveats"
  },
  {
    "objectID": "materials/GraphTypes.html#common-caveats-1",
    "href": "materials/GraphTypes.html#common-caveats-1",
    "title": "GraphTypes",
    "section": "Common caveats",
    "text": "Common caveats"
  },
  {
    "objectID": "materials/GraphTypes.html#what-for-5",
    "href": "materials/GraphTypes.html#what-for-5",
    "title": "GraphTypes",
    "section": "What for",
    "text": "What for\nWord clouds are useful for:\n\nQuickly perceiving the most prominent terms\nLocating a term alphabetically to determine its relative prominence\nCreating visually appealing representations of text data\n\nThey are widely used in media and well understood by the public."
  },
  {
    "objectID": "materials/GraphTypes.html#useful-for-5",
    "href": "materials/GraphTypes.html#useful-for-5",
    "title": "GraphTypes",
    "section": "Useful for:",
    "text": "Useful for:\n\nSummarizing large amounts of text data\nHighlighting key themes or topics in a dataset\nCreating engaging visuals for presentations or reports\nComparing word frequencies across different texts or sources"
  },
  {
    "objectID": "materials/GraphTypes.html#example-2",
    "href": "materials/GraphTypes.html#example-2",
    "title": "GraphTypes",
    "section": "Example",
    "text": "Example\nHere’s an alternative representation using a lollipop plot, which addresses some of the limitations of word clouds:\n\n\n'data.frame':   11529 obs. of  3 variables:\n $ artist: chr  \"booba\" \"booba\" \"booba\" \"booba\" ...\n $ song  : chr  \"113\" \"113\" \"113\" \"113\" ...\n $ word  : chr  \"paroles.net\" \"above\" \"lyrics\" \"function\" ..."
  },
  {
    "objectID": "materials/GraphTypes.html#variation-5",
    "href": "materials/GraphTypes.html#variation-5",
    "title": "GraphTypes",
    "section": "Variation",
    "text": "Variation\nMany variations exist for word clouds:\n\nDifferent shapes, sometimes using the shape of an object related to the topic\nVarying text orientation, font, size, and colors\nInteractive word clouds that change or provide additional information on hover"
  },
  {
    "objectID": "materials/GraphTypes.html#common-mistakes-4",
    "href": "materials/GraphTypes.html#common-mistakes-4",
    "title": "GraphTypes",
    "section": "Common mistakes",
    "text": "Common mistakes\n\nRelying too heavily on word clouds for accurate data representation\nIgnoring the limitations of area as a metaphor for numeric values\nNot accounting for the bias created by longer words appearing larger\nUsing word clouds when more precise visualizations (like bar charts or lollipop plots) would be more appropriate\nCreating overly complex or cluttered word clouds that are difficult to read or interpret\n\nWord clouds, while visually appealing, are often criticized for their lack of accuracy in conveying information. Consider using them primarily for aesthetic purposes or in conjunction with more precise data visualization methods."
  },
  {
    "objectID": "materials/home_week03.html#overview",
    "href": "materials/home_week03.html#overview",
    "title": "Week 3 Materials",
    "section": "Overview",
    "text": "Overview\nThis week we’ll be covering mostly big picture background content. We’ll begin with background on data science generally then narrow to talking about why this course is taught in R (rather than some other coding language). I’ll then give you some hard-won tips on problem solving so that you can begin to develop strategies for handling the errors and warnings that you’ll inevitably encounter as you grow your competency in this arena. In lab, we’ll discuss some crucial components of navigating and using RStudio as well as begin our conversation about R scripts and “base R”"
  },
  {
    "objectID": "materials/home_week03.html#lecture-3---data-visualisation",
    "href": "materials/home_week03.html#lecture-3---data-visualisation",
    "title": "Week 3 Materials",
    "section": "Lecture 3 - Data Visualisation",
    "text": "Lecture 3 - Data Visualisation\n\nLecture Slides – Full Screen"
  },
  {
    "objectID": "materials/home_week03.html#lab-2---rstudio-base-r",
    "href": "materials/home_week03.html#lab-2---rstudio-base-r",
    "title": "Week 3 Materials",
    "section": "Lab 2 - RStudio & Base R",
    "text": "Lab 2 - RStudio & Base R\n\nLab Slides – Full Screen"
  },
  {
    "objectID": "materials/home_week03.html#dataskills-3",
    "href": "materials/home_week03.html#dataskills-3",
    "title": "Week 3 Materials",
    "section": "DataSkills 3",
    "text": "DataSkills 3\n\nLearning Objective(s)\nUpon completion of these assignments, students will be able to:\n\nDescribe a real-world file path\nDemonstrate comfortability with using comments in an R script\nExplain fundamental principles of R and RStudio\nPerform arithmetic using objects in R\nManipulate and interpret vectors and objects of various classes and contents\n\n\n\nAssignment Due Date(s)\nEach homework is due at midnight the day before each lecture (i.e., Monday night) Late work will be accepted but will be subject to the late assignments policy outlined in this course’s syllabus.\n\n\nAssignment Description\nThis homework should be submitted as an R script with your last name and week 1 as the file name (e.g., “Lyon_week1.R”). Remember to include comments explaining what line(s) correspond to each of the following prompts.\n\nAs a comment, write the full file path on your computer to where you’ve saved this script. Folders should be separated by slashes. Include the name of the R script in the file path\nAs a comment, explain the difference between the RStudio “Console” and “Source” panes\nAs a comment, describe–in your own words–what is meant by the term “base R”\nCreate an object named “weight_kg” and assign the value 62 as a number\nCreate a new object by multiplying the ‘weight_kg’ object by 2.2 and assign the result to an object called “weight_lb”\nAs a comment, define both (A) the value of ‘weight_lb’ and (B) the class of ‘weight_lb’\nWrite the code necessary to access the help file for the function floor"
  },
  {
    "objectID": "materials/home_week02.html",
    "href": "materials/home_week02.html",
    "title": "Week 2 Materials",
    "section": "",
    "text": "This is a website built using Quarto. Don’t be afraid. If Gordon can do it, you can do it.\nThis week we’re going to start by trying to demonstrate that using design principles and using a creative mindset to approach the challenges of communicating science can lead to some really exciting outcomes.\nWe are going to talk about some of the tools that you might end up using during the course of your degree, the point being that they will be useful hopefully for the rest of your lives.\nOne of the joys of reading Psychology at Goldsmiths is the artistic environment and spirit of creativity. By investing just a little thought, time and energy, you could bring the arts and sciences closer together!",
    "crumbs": [
      "General Info",
      "Course Content",
      "Week 2 Content"
    ]
  },
  {
    "objectID": "materials/home_week02.html#overview",
    "href": "materials/home_week02.html#overview",
    "title": "Week 2 Materials",
    "section": "",
    "text": "This is a website built using Quarto. Don’t be afraid. If Gordon can do it, you can do it.\nThis week we’re going to start by trying to demonstrate that using design principles and using a creative mindset to approach the challenges of communicating science can lead to some really exciting outcomes.\nWe are going to talk about some of the tools that you might end up using during the course of your degree, the point being that they will be useful hopefully for the rest of your lives.\nOne of the joys of reading Psychology at Goldsmiths is the artistic environment and spirit of creativity. By investing just a little thought, time and energy, you could bring the arts and sciences closer together!",
    "crumbs": [
      "General Info",
      "Course Content",
      "Week 2 Content"
    ]
  },
  {
    "objectID": "materials/home_week02.html#lecture-2---data-visualisation",
    "href": "materials/home_week02.html#lecture-2---data-visualisation",
    "title": "Week 2 Materials",
    "section": "Lecture 2 - Data Visualisation",
    "text": "Lecture 2 - Data Visualisation\n\nLecture Slides – Full Screen",
    "crumbs": [
      "General Info",
      "Course Content",
      "Week 2 Content"
    ]
  },
  {
    "objectID": "materials/home_week02.html#lab-2---data-visualisation-treasure-hunt",
    "href": "materials/home_week02.html#lab-2---data-visualisation-treasure-hunt",
    "title": "Week 2 Materials",
    "section": "Lab 2 - Data Visualisation Treasure Hunt",
    "text": "Lab 2 - Data Visualisation Treasure Hunt\n\nLab Slides – Full Screen\nSubmit your favourite visualisation - TopViz here",
    "crumbs": [
      "General Info",
      "Course Content",
      "Week 2 Content"
    ]
  },
  {
    "objectID": "materials/home_week02.html#showcase-browzine",
    "href": "materials/home_week02.html#showcase-browzine",
    "title": "Week 2 Materials",
    "section": "Showcase BrowZine",
    "text": "Showcase BrowZine\nhttps://browzine.com/libraries/1374/subjects/67/bookcases/169?sort=title",
    "crumbs": [
      "General Info",
      "Course Content",
      "Week 2 Content"
    ]
  },
  {
    "objectID": "materials/home_week02.html#speed-data-survey",
    "href": "materials/home_week02.html#speed-data-survey",
    "title": "Week 2 Materials",
    "section": "Speed Data Survey",
    "text": "Speed Data Survey\nThis survey is to generate data for lab sessions in the future. Please fill it in! It features a number of your questions!\nLINK TO SPEED DATA SURVEY WEEK 02",
    "crumbs": [
      "General Info",
      "Course Content",
      "Week 2 Content"
    ]
  },
  {
    "objectID": "materials/home_week02.html#your-questions",
    "href": "materials/home_week02.html#your-questions",
    "title": "Week 2 Materials",
    "section": "Your Questions",
    "text": "Your Questions\nIf you are interested in viewing the questions you submitted last week, anonymously of course, please use the links below\nQuestions on a scale\nQuestions with choices\nFreeText Questions",
    "crumbs": [
      "General Info",
      "Course Content",
      "Week 2 Content"
    ]
  },
  {
    "objectID": "materials/freetextquestions.html",
    "href": "materials/freetextquestions.html",
    "title": "Freetext Questions Clusters",
    "section": "",
    "text": "How do you feel about statistics?\nWhat improvements could be made to the course you’re studying?\nHow would you describe your experience at University?\nDescribe a challenge you faced as new student at University.\nHow do you feel about being a mature student in a university?\nWhat would help improve your experience at Goldsmiths?\nWhat was your opinion on today’s lecture?\nHow do you feel about the University campus?\nHow would you describe your experience of the London Underground?\nWhat do you want to achieve with your Psychology degree?\n\n\n\n\n\nWhat got you interested in psychology?\nWhat motivated you to study Psychology, and how do you hope to apply it in the future?\nDo you have a favourite topic within psychology? If so, what is it and why?\nHow do you use psychology in your day to day life?\nCan you define psychology?\nHow do you believe the new era of AI will impact the study of psychology in the next 10 years?\nThink of the first job you wanted to do as a child, does this link to your decision to study psychology?\nIn what ways do you think psychology can help improve societal issues like mental health awareness or education?\nWhy did you want to do clinical psychology?\nWhich career path do you feel you want to follow in psychology and why?\n\n\n\n\n\nHow does the sound of rain make you feel?\nWhat makes you feel anxious?\nWhat role do you think self-reflection plays in your social interactions?\nHow would you describe your feelings when someone asks you questions in public?\nWhat motivates you to get out of bed in the morning?\nHow do you feel about attending social activities and events at the university?\nHow do feelings of depression or happiness impact your daily activities and overall well-being?\nWhat makes you happy?\nWhat is a phobia?\nWhat do you find is the best method to soothe anxiety?\nHow do you feel today?\nHow would you describe your overall happiness/wellbeing?\nHow do you think your childhood experiences affect your relations with others?\nWhat does the pain feel like?\nWhat motivates you in life?\nWhat factors contribute to your stress levels during exam periods? Include any experiences or challenges you find relevant.\nIn what ways do you think your overall happiness has been affected this week?\nWhat impact does anxiety have on your university assignments?\nWhat’s the biggest factor for your anger?\nWhat challenges have you faced in seeking support from others?\n\n\n\n\n\nWhat are your opinions on the statement “Listening to music during studying is helpful for recall”? Please explain your answer.\nHow would your friends describe your personality?\nWhat is the meaning of life?\nWhat is your opinion on modern technology and how it impacts people’s lives?\nWhat’s something you’d like to change about yourself (not physically) by the end of 1st year?\nWhat is your favourite childhood memory?\nWhat is your favourite topic to discuss? Why?\nWhat is your opinion about the use of medication to cure illnesses?\nWhat’s your opinion on movies?\nIn what ways do you think childhood trauma might contribute to an individual’s likelihood of engaging in criminal behaviour later in life?\nWhat would your childhood dream treehouse look like?\nWhat are the main aspirations and goals you hope to achieve in the next 5 years?\nWhat is your opinion on the fast food industry?\nHow would you feel if you were unable to drink coffee for a day?\nHow do you see yourself in 10 years?\nDo you believe as a society, we should always obey those with social power/authority?\n\n\n\n\n\nWhat is your favourite season of Doctor Who?\nWhat is the best Star Wars film?\nWhat is your favourite TV show?\nWhat is your favourite memory associated with eating ice cream?\n\n```",
    "crumbs": [
      "General Info",
      "Course Content",
      "Free Text"
    ]
  },
  {
    "objectID": "materials/freetextquestions.html#academic-and-university-experience",
    "href": "materials/freetextquestions.html#academic-and-university-experience",
    "title": "Freetext Questions Clusters",
    "section": "",
    "text": "How do you feel about statistics?\nWhat improvements could be made to the course you’re studying?\nHow would you describe your experience at University?\nDescribe a challenge you faced as new student at University.\nHow do you feel about being a mature student in a university?\nWhat would help improve your experience at Goldsmiths?\nWhat was your opinion on today’s lecture?\nHow do you feel about the University campus?\nHow would you describe your experience of the London Underground?\nWhat do you want to achieve with your Psychology degree?",
    "crumbs": [
      "General Info",
      "Course Content",
      "Free Text"
    ]
  },
  {
    "objectID": "materials/freetextquestions.html#psychology-related-questions",
    "href": "materials/freetextquestions.html#psychology-related-questions",
    "title": "Freetext Questions Clusters",
    "section": "",
    "text": "What got you interested in psychology?\nWhat motivated you to study Psychology, and how do you hope to apply it in the future?\nDo you have a favourite topic within psychology? If so, what is it and why?\nHow do you use psychology in your day to day life?\nCan you define psychology?\nHow do you believe the new era of AI will impact the study of psychology in the next 10 years?\nThink of the first job you wanted to do as a child, does this link to your decision to study psychology?\nIn what ways do you think psychology can help improve societal issues like mental health awareness or education?\nWhy did you want to do clinical psychology?\nWhich career path do you feel you want to follow in psychology and why?",
    "crumbs": [
      "General Info",
      "Course Content",
      "Free Text"
    ]
  },
  {
    "objectID": "materials/freetextquestions.html#mental-health-and-well-being",
    "href": "materials/freetextquestions.html#mental-health-and-well-being",
    "title": "Freetext Questions Clusters",
    "section": "",
    "text": "How does the sound of rain make you feel?\nWhat makes you feel anxious?\nWhat role do you think self-reflection plays in your social interactions?\nHow would you describe your feelings when someone asks you questions in public?\nWhat motivates you to get out of bed in the morning?\nHow do you feel about attending social activities and events at the university?\nHow do feelings of depression or happiness impact your daily activities and overall well-being?\nWhat makes you happy?\nWhat is a phobia?\nWhat do you find is the best method to soothe anxiety?\nHow do you feel today?\nHow would you describe your overall happiness/wellbeing?\nHow do you think your childhood experiences affect your relations with others?\nWhat does the pain feel like?\nWhat motivates you in life?\nWhat factors contribute to your stress levels during exam periods? Include any experiences or challenges you find relevant.\nIn what ways do you think your overall happiness has been affected this week?\nWhat impact does anxiety have on your university assignments?\nWhat’s the biggest factor for your anger?\nWhat challenges have you faced in seeking support from others?",
    "crumbs": [
      "General Info",
      "Course Content",
      "Free Text"
    ]
  },
  {
    "objectID": "materials/freetextquestions.html#personal-reflections-and-opinions",
    "href": "materials/freetextquestions.html#personal-reflections-and-opinions",
    "title": "Freetext Questions Clusters",
    "section": "",
    "text": "What are your opinions on the statement “Listening to music during studying is helpful for recall”? Please explain your answer.\nHow would your friends describe your personality?\nWhat is the meaning of life?\nWhat is your opinion on modern technology and how it impacts people’s lives?\nWhat’s something you’d like to change about yourself (not physically) by the end of 1st year?\nWhat is your favourite childhood memory?\nWhat is your favourite topic to discuss? Why?\nWhat is your opinion about the use of medication to cure illnesses?\nWhat’s your opinion on movies?\nIn what ways do you think childhood trauma might contribute to an individual’s likelihood of engaging in criminal behaviour later in life?\nWhat would your childhood dream treehouse look like?\nWhat are the main aspirations and goals you hope to achieve in the next 5 years?\nWhat is your opinion on the fast food industry?\nHow would you feel if you were unable to drink coffee for a day?\nHow do you see yourself in 10 years?\nDo you believe as a society, we should always obey those with social power/authority?",
    "crumbs": [
      "General Info",
      "Course Content",
      "Free Text"
    ]
  },
  {
    "objectID": "materials/freetextquestions.html#pop-culture-and-entertainment",
    "href": "materials/freetextquestions.html#pop-culture-and-entertainment",
    "title": "Freetext Questions Clusters",
    "section": "",
    "text": "What is your favourite season of Doctor Who?\nWhat is the best Star Wars film?\nWhat is your favourite TV show?\nWhat is your favourite memory associated with eating ice cream?\n\n```",
    "crumbs": [
      "General Info",
      "Course Content",
      "Free Text"
    ]
  },
  {
    "objectID": "materials/pokemon.html",
    "href": "materials/pokemon.html",
    "title": "Pokemon",
    "section": "",
    "text": "Radar Plots of Personality\n```{=html cat(’\n\n  Download Y1W3_data.csv \n\n’)}\n```"
  },
  {
    "objectID": "materials/choicequestions.html",
    "href": "materials/choicequestions.html",
    "title": "Choice Questions Clusters",
    "section": "",
    "text": "Which lab group were you in?\nWhich topic are you most looking forward to studying on your course?\nWhat psychology program, if any, did you do prior to attending Goldsmiths?\nHow long is your commute? Please select the option which applies best.\n\nUnder 30 minutes\n30 minutes to an hour\nOver an hour\n\nWhat are some fundamental features you look for in a university?\nWhen were you last in a school environment?\n\nI just finished secondary school\nI finished a different degree recently\nIt’s been 1-3 years\nIt’s been 4+ years\n\nWhat module are you most looking forward to study?\n\nMental Health and Wellbeing\nProfessional and Personal Development\nResearch Methods and Statistics\nLife and Society\nCognition and Culture\nIdentity, Environment and Agency\n\nWhich bachelor’s degree in psychology do you think has the highest number of students enrolled at Goldsmiths University? Please choose one of the following options:\n\nBSc (Hons) Psychology\nBSc (Hons) Psychology with Forensic Psychology\nBSc (Hons) Psychology with Clinical Psychology\n\nHow difficult is it to navigate around the uni?\n\nVery difficult\nSomewhat difficult\nNeutral\nOK\nVery simple\n\nHow often do you typically ask for help during a class?\n\nOften\nSometimes\nRarely\nNever\n\nAnswering either Yes or No, did you feel like today’s lesson was beneficial?\n\n\n\n\n\nWhich area of psychology do you find more interesting?\n\nCognitive Psychology\nSocial Psychology\nClinical Psychology\nOther (please specify)\n\nWhat type of psychology do you find more interesting?\n\nForensic psych\nClinical psych\nCognitive neuroscience\n\nDo you have a phobia or fear? Yes, maybe, no.\nHow beneficial do believe exposure therapy is for people suffering from phobias? Very helpful, helpful, neutral, unhelpful, irrelevant.\nWhich of the following do you believe has the most significant long-term impact on individuals who experience childhood trauma?\nWhich area of psychology interests you the most?\nHow often have you felt down, depressed, or hopeless in the past two weeks?\nHow often do you feel anxious?\nHow many days a week does anxiety affect your daily routine?\nHow regularly do you feel angry?\nTo what extent do you feel you have a support system in your life?\n\n\n\n\n\nDo you prefer playing volleyball or basketball?\nFrom these options, what would you say you enjoy the most: football, basketball, tennis and badminton.\nWhich of these appeals to you as a weekend activity?\n\nWatching television at home\nMeeting friends at a friend’s house\nGoing out to a nightclub\n\nWhat is your favourite Season? Spring, Summer, autumn or spring?\nWhat do you like to do with your free time? Option 1 - Watching TV or a show Option 2 - Playing a sport Option 3 - Hanging out with friends\nWhat is your favourite colour? (Options: Red, Yellow, Orange, Green, Blue, Purple, Pink, Brown, Black, Grey, White)\nWhat type of genre for movies do you prefer?\n\nRomance\nAction\nComedy\nFantasy\nThriller\nHorror\n\nWhat activities make you the most happy from sports, games, talking with friends and family or spending alone time?\nWhat sensory experience is most important to you when it comes to eating food?\n\nTexture\nSmell\nTaste\nTemperature\nAtmosphere of the room you are eating in\n\nWhat kind of pizza toppings do you like?\nWhich of the following options best suits your opinion on candy floss?\nWhat is your favourite drink?\n\nCoffee\nTea\nApple juice\n\nAre you working a part/full time job at the moment?\nHow often do you drink alcohol? (Never, not often, somewhat often, fairly regularly, everyday)\nOut of these options, which animal is your favorite: hyena, dolphin, elephant, penguin and Lion?\nWhat do you think is the best sweet out of, Haribo’s, Skittles, Twix and Maltesers?\nWhat is your most preferred type of caffeine?\nHow much time do you spend reading throughout the week?\n\n10+ hours\nLess than 10\nLess than 2 hours\nNever\n\nWhich ice cream flavour do you prefer the most?\n\nChocolate\nVanilla\nCookies and cream\nSalted caramel\nNone\n\n\n\n\n\n\nYou find out that a colleague has been stealing from work, which action will you take?\n\nSay nothing\nSpeak to them privately and tell them to stop\nTell multiple other colleagues about it under the guise of “asking for advice”\nReport them to your manager\n\nWhat should be the main goal for our prison systems? (Option for answer e.g. reform, punish etc.)\nDo you think they should raise the legal age to get married to 21? Yes, no or not sure.\nIn a scenario where an authority figure’s orders may be destructive, how likely are you to blindly obey?\n\n\n\n\n\nWhat environments make you feel the most anxious?\n\nNightclub\nBusy street\nLibrary\nClass room\nAt home\n\nIf you had to pick between being happy and having never ending money which would u pick?\nHow often do you find yourself reflecting on past experiences to reflect on your current behaviour?\nWould you like to be asked questions in public?\nDo you feel safe in your home?\nWhat type of activities do you do in your free time to relax?\nWhat are your hobbies?\nWhat motivates you to participate in social activities at the university? A. Meeting new people B. Taking a break from studies E. I don’t feel motivated to attend\nHow likely are you to enjoy the subject you’re studying?\nWhat experiments would be carried out during the psychology degree?\nWhich is your favourite between season 5, season 6, and season 7 of Doctor Who?\nIf there were suddenly an increase in a bug species, would you rather have it be cockroaches, mosquitoes, wasps, or spiders?\nHow often did you feel safe in your childhood home?\nIn your day-to-day existence, how do you usually manage difficult situations?\nWhere do you feel the pain the most?\nDo you think it will rain tomorrow? Pick: Yes, No or Maybe\nWhat has been your prevailing mood this week?",
    "crumbs": [
      "General Info",
      "Course Content",
      "Choice Questions"
    ]
  },
  {
    "objectID": "materials/choicequestions.html#academic-and-university-life",
    "href": "materials/choicequestions.html#academic-and-university-life",
    "title": "Choice Questions Clusters",
    "section": "",
    "text": "Which lab group were you in?\nWhich topic are you most looking forward to studying on your course?\nWhat psychology program, if any, did you do prior to attending Goldsmiths?\nHow long is your commute? Please select the option which applies best.\n\nUnder 30 minutes\n30 minutes to an hour\nOver an hour\n\nWhat are some fundamental features you look for in a university?\nWhen were you last in a school environment?\n\nI just finished secondary school\nI finished a different degree recently\nIt’s been 1-3 years\nIt’s been 4+ years\n\nWhat module are you most looking forward to study?\n\nMental Health and Wellbeing\nProfessional and Personal Development\nResearch Methods and Statistics\nLife and Society\nCognition and Culture\nIdentity, Environment and Agency\n\nWhich bachelor’s degree in psychology do you think has the highest number of students enrolled at Goldsmiths University? Please choose one of the following options:\n\nBSc (Hons) Psychology\nBSc (Hons) Psychology with Forensic Psychology\nBSc (Hons) Psychology with Clinical Psychology\n\nHow difficult is it to navigate around the uni?\n\nVery difficult\nSomewhat difficult\nNeutral\nOK\nVery simple\n\nHow often do you typically ask for help during a class?\n\nOften\nSometimes\nRarely\nNever\n\nAnswering either Yes or No, did you feel like today’s lesson was beneficial?",
    "crumbs": [
      "General Info",
      "Course Content",
      "Choice Questions"
    ]
  },
  {
    "objectID": "materials/choicequestions.html#psychology-and-mental-health",
    "href": "materials/choicequestions.html#psychology-and-mental-health",
    "title": "Choice Questions Clusters",
    "section": "",
    "text": "Which area of psychology do you find more interesting?\n\nCognitive Psychology\nSocial Psychology\nClinical Psychology\nOther (please specify)\n\nWhat type of psychology do you find more interesting?\n\nForensic psych\nClinical psych\nCognitive neuroscience\n\nDo you have a phobia or fear? Yes, maybe, no.\nHow beneficial do believe exposure therapy is for people suffering from phobias? Very helpful, helpful, neutral, unhelpful, irrelevant.\nWhich of the following do you believe has the most significant long-term impact on individuals who experience childhood trauma?\nWhich area of psychology interests you the most?\nHow often have you felt down, depressed, or hopeless in the past two weeks?\nHow often do you feel anxious?\nHow many days a week does anxiety affect your daily routine?\nHow regularly do you feel angry?\nTo what extent do you feel you have a support system in your life?",
    "crumbs": [
      "General Info",
      "Course Content",
      "Choice Questions"
    ]
  },
  {
    "objectID": "materials/choicequestions.html#lifestyle-and-preferences",
    "href": "materials/choicequestions.html#lifestyle-and-preferences",
    "title": "Choice Questions Clusters",
    "section": "",
    "text": "Do you prefer playing volleyball or basketball?\nFrom these options, what would you say you enjoy the most: football, basketball, tennis and badminton.\nWhich of these appeals to you as a weekend activity?\n\nWatching television at home\nMeeting friends at a friend’s house\nGoing out to a nightclub\n\nWhat is your favourite Season? Spring, Summer, autumn or spring?\nWhat do you like to do with your free time? Option 1 - Watching TV or a show Option 2 - Playing a sport Option 3 - Hanging out with friends\nWhat is your favourite colour? (Options: Red, Yellow, Orange, Green, Blue, Purple, Pink, Brown, Black, Grey, White)\nWhat type of genre for movies do you prefer?\n\nRomance\nAction\nComedy\nFantasy\nThriller\nHorror\n\nWhat activities make you the most happy from sports, games, talking with friends and family or spending alone time?\nWhat sensory experience is most important to you when it comes to eating food?\n\nTexture\nSmell\nTaste\nTemperature\nAtmosphere of the room you are eating in\n\nWhat kind of pizza toppings do you like?\nWhich of the following options best suits your opinion on candy floss?\nWhat is your favourite drink?\n\nCoffee\nTea\nApple juice\n\nAre you working a part/full time job at the moment?\nHow often do you drink alcohol? (Never, not often, somewhat often, fairly regularly, everyday)\nOut of these options, which animal is your favorite: hyena, dolphin, elephant, penguin and Lion?\nWhat do you think is the best sweet out of, Haribo’s, Skittles, Twix and Maltesers?\nWhat is your most preferred type of caffeine?\nHow much time do you spend reading throughout the week?\n\n10+ hours\nLess than 10\nLess than 2 hours\nNever\n\nWhich ice cream flavour do you prefer the most?\n\nChocolate\nVanilla\nCookies and cream\nSalted caramel\nNone",
    "crumbs": [
      "General Info",
      "Course Content",
      "Choice Questions"
    ]
  },
  {
    "objectID": "materials/choicequestions.html#social-and-ethical-questions",
    "href": "materials/choicequestions.html#social-and-ethical-questions",
    "title": "Choice Questions Clusters",
    "section": "",
    "text": "You find out that a colleague has been stealing from work, which action will you take?\n\nSay nothing\nSpeak to them privately and tell them to stop\nTell multiple other colleagues about it under the guise of “asking for advice”\nReport them to your manager\n\nWhat should be the main goal for our prison systems? (Option for answer e.g. reform, punish etc.)\nDo you think they should raise the legal age to get married to 21? Yes, no or not sure.\nIn a scenario where an authority figure’s orders may be destructive, how likely are you to blindly obey?",
    "crumbs": [
      "General Info",
      "Course Content",
      "Choice Questions"
    ]
  },
  {
    "objectID": "materials/choicequestions.html#miscellaneous",
    "href": "materials/choicequestions.html#miscellaneous",
    "title": "Choice Questions Clusters",
    "section": "",
    "text": "What environments make you feel the most anxious?\n\nNightclub\nBusy street\nLibrary\nClass room\nAt home\n\nIf you had to pick between being happy and having never ending money which would u pick?\nHow often do you find yourself reflecting on past experiences to reflect on your current behaviour?\nWould you like to be asked questions in public?\nDo you feel safe in your home?\nWhat type of activities do you do in your free time to relax?\nWhat are your hobbies?\nWhat motivates you to participate in social activities at the university? A. Meeting new people B. Taking a break from studies E. I don’t feel motivated to attend\nHow likely are you to enjoy the subject you’re studying?\nWhat experiments would be carried out during the psychology degree?\nWhich is your favourite between season 5, season 6, and season 7 of Doctor Who?\nIf there were suddenly an increase in a bug species, would you rather have it be cockroaches, mosquitoes, wasps, or spiders?\nHow often did you feel safe in your childhood home?\nIn your day-to-day existence, how do you usually manage difficult situations?\nWhere do you feel the pain the most?\nDo you think it will rain tomorrow? Pick: Yes, No or Maybe\nWhat has been your prevailing mood this week?",
    "crumbs": [
      "General Info",
      "Course Content",
      "Choice Questions"
    ]
  },
  {
    "objectID": "materials/visuals.html",
    "href": "materials/visuals.html",
    "title": "Drawing graphs",
    "section": "",
    "text": "Above all else show the data.\n–Edward Tufte1\n\nVisualising data is one of the most important tasks facing the data analyst. It’s important for two distinct but closely related reasons. Firstly, there’s the matter of drawing “presentation graphics”: displaying your data in a clean, visually appealing fashion makes it easier for your reader to understand what you’re trying to tell them. Equally important, perhaps even more important, is the fact that drawing graphs helps you to understand the data. To that end, it’s important to draw “exploratory graphics” that help you learn about the data as you go about analysing it. These points might seem pretty obvious, but I cannot count the number of times I’ve seen people forget them.\n\n\n\n\n\nA stylised redrawing of John Snow’s original cholera map. Each small dot represents the location of a cholera case, and each large circle shows the location of a well. As the plot makes clear, the cholera outbreak is centred very closely on the Broad St pump. This image uses the data from the HistData package, and was drawn using minor alterations to the commands provided in the help files. Note that Snow’s original hand drawn map used different symbols and labels, but you get the idea.\n\n\n\n\nTo give a sense of the importance of this chapter, I want to start with a classic illustration of just how powerful a good graph can be. To that end, Figure @ref(fig:snowmap1) shows a redrawing of one of the most famous data visualisations of all time: John Snow’s 1854 map of cholera deaths. The map is elegant in its simplicity. In the background we have a street map, which helps orient the viewer. Over the top, we see a large number of small dots, each one representing the location of a cholera case. The larger symbols show the location of water pumps, labelled by name. Even the most casual inspection of the graph makes it very clear that the source of the outbreak is almost certainly the Broad Street pump. Upon viewing this graph, Dr Snow arranged to have the handle removed from the pump, ending the outbreak that had killed over 500 people. Such is the power of a good data visualisation.\nThe goals in this chapter are twofold: firstly, to discuss several fairly standard graphs that we use a lot when analysing and presenting data, and secondly, to show you how to create these graphs in R. The graphs themselves tend to be pretty straightforward, so in that respect this chapter is pretty simple. Where people usually struggle is learning how to produce graphs, and especially, learning how to produce good graphs.2 Fortunately, learning how to draw graphs in R is reasonably simple, as long as you’re not too picky about what your graph looks like. What I mean when I say this is that R has a lot of very good graphing functions, and most of the time you can produce a clean, high-quality graphic without having to learn very much about the low-level details of how R handles graphics. Unfortunately, on those occasions when you do want to do something non-standard, or if you need to make highly specific changes to the figure, you actually do need to learn a fair bit about the these details; and those details are both complicated and boring. With that in mind, the structure of this chapter is as follows: I’ll start out by giving you a very quick overview of how graphics work in R. I’ll then discuss several different kinds of graph and how to draw them, as well as showing the basics of how to customise these plots. I’ll then talk in more detail about R graphics, discussing some of those complicated and boring issues. In a future version of this book, I intend to finish this chapter off by talking about what makes a good or a bad graph, but I haven’t yet had the time to write that section.\n\n\nReduced to its simplest form, you can think of an R graphic as being much like a painting. You start out with an empty canvas. Every time you use a graphics function, it paints some new things onto your canvas. Later on, you can paint more things over the top if you want; but just like painting, you can’t “undo” your strokes. If you make a mistake, you have to throw away your painting and start over. Fortunately, this is way more easy to do when using R than it is when painting a picture in real life: you delete the plot and then type a new set of commands.3 This way of thinking about drawing graphs is referred to as the painter’s model. So far, this probably doesn’t sound particularly complicated, and for the vast majority of graphs you’ll want to draw it’s exactly as simple as it sounds. Much like painting in real life, the headaches usually start when we dig into details. To see why, I’ll expand this “painting metaphor” a bit further just to show you the basics of what’s going on under the hood, but before I do I want to stress that you really don’t need to understand all these complexities in order to draw graphs. I’d been using R for years before I even realised that most of these issues existed! However, I don’t want you to go through the same pain I went through every time I inadvertently discovered one of these things, so here’s a quick overview.\nFirstly, if you want to paint a picture, you need to paint it on something. In real life, you can paint on lots of different things. Painting onto canvas isn’t the same as painting onto paper, and neither one is the same as painting on a wall. In R, the thing that you paint your graphic onto is called a device. For most applications that we’ll look at in this book, this “device” will be a window on your computer. If you’re using Windows as your operating system, then the name for this device is windows; on a Mac it’s called quartz because that’s the name of the software that the Mac OS uses to draw pretty pictures; and on Linux/Unix, you’re probably using X11. On the other hand, if you’re using Rstudio (regardless of which operating system you’re on), there’s a separate device called RStudioGD that forces R to paint inside the “plots” panel in Rstudio. However, from the computers perspective there’s nothing terribly special about drawing pictures on screen: and so R is quite happy to paint pictures directly into a file. R can paint several different types of image files: jpeg, png, pdf, postscript, tiff and bmp files are all among the options that you have available to you. For the most part, these different devices all behave the same way, so you don’t really need to know much about the differences between them when learning how to draw pictures. But, just like real life painting, sometimes the specifics do matter. Unless stated otherwise, you can assume that I’m drawing a picture on screen, using the appropriate device (i.e., windows, quartz, X11 or RStudioGD). One the rare occasions where these behave differently from one another, I’ll try to point it out in the text.\nSecondly, when you paint a picture you need to paint it with something. Maybe you want to do an oil painting, but maybe you want to use watercolour. And, generally speaking, you pretty much have to pick one or the other. The analog to this in R is a “graphics system”. A graphics system defines a collection of very low-level graphics commands about what to draw and where to draw it. Something that surprises most new R users is the discovery that R actually has two completely independent graphics systems, known as traditional graphics (in the graphics package) and grid graphics (in the grid package).4 Not surprisingly, the traditional graphics system is the older of the two: in fact, it’s actually older than R since it has it’s origins in S, the system from which R is descended. Grid graphics are newer, and in some respects more powerful, so many of the more recent, fancier graphical tools in R make use of grid graphics. However, grid graphics are somewhat more complicated beasts, so most people start out by learning the traditional graphics system. Nevertheless, as long as you don’t want to use any low-level commands yourself, then you don’t really need to care about whether you’re using traditional graphics or grid graphics. However, the moment you do want to tweak your figure by using some low-level commands you do need to care. Because these two different systems are pretty much incompatible with each other, there’s a pretty big divide in R graphics universe. Unless stated otherwise, you can assume that everything I’m saying pertains to traditional graphics.\nThirdly, a painting is usually done in a particular style. Maybe it’s a still life, maybe it’s an impressionist piece, or maybe you’re trying to annoy me by pretending that cubism is a legitimate artistic style. Regardless, each artistic style imposes some overarching aesthetic and perhaps even constraints on what can (or should) be painted using that style. In the same vein, R has quite a number of different packages, each of which provide a collection of high-level graphics commands. A single high-level command is capable of drawing an entire graph, complete with a range of customisation options. Most but not all of the high-level commands that I’ll talk about in this book come from the graphics package itself, and so belong to the world of traditional graphics. These commands all tend to share a common visual style, although there are a few graphics that I’ll use that come from other packages that differ in style somewhat. On the other side of the great divide, the grid universe relies heavily on two different packages – lattice and ggplots2 – each of which provides a quite different visual style. As you’ve probably guessed, there’s a whole separate bunch of functions that you’d need to learn if you want to use lattice graphics or make use of the ggplots2. However, for the purposes of this book I’ll restrict myself to talking about the basic graphics tools.\nAt this point, I think we’ve covered more than enough background material. The point that I’m trying to make by providing this discussion isn’t to scare you with all these horrible details, but rather to try to convey to you the fact that R doesn’t really provide a single coherent graphics system. Instead, R itself provides a platform, and different people have built different graphical tools using that platform. As a consequence of this fact, there’s two different universes of graphics, and a great multitude of packages that live in them. At this stage you don’t need to understand these complexities, but it’s useful to know that they’re there. But for now, I think we can be happy with a simpler view of things: we’ll draw pictures on screen using the traditional graphics system, and as much as possible we’ll stick to high level commands only.\nSo let’s start painting.\n\n\n\nBefore I discuss any specialised graphics, let’s start by drawing a few very simple graphs just to get a feel for what it’s like to draw pictures using R. To that end, let’s create a small vector Fibonacci that contains a few numbers we’d like R to draw for us. Then, we’ll ask R to plot() those numbers. The result is Figure @ref(fig:firstplot).\n\nFibonacci &lt;- c( 1,1,2,3,5,8,13 )\nplot( Fibonacci )\n\n\n\n\nOur first plot\n\n\n\n\nAs you can see, what R has done is plot the values stored in the Fibonacci variable on the vertical axis (y-axis) and the corresponding index on the horizontal axis (x-axis). In other words, since the 4th element of the vector has a value of 3, we get a dot plotted at the location (4,3). That’s pretty straightforward, and the image in Figure @ref(fig:firstplot) is probably pretty close to what you would have had in mind when I suggested that we plot the Fibonacci data. However, there’s quite a lot of customisation options available to you, so we should probably spend a bit of time looking at some of those options. So, be warned: this ends up being a fairly long section, because there’s so many possibilities open to you. Don’t let it overwhelm you though… while all of the options discussed here are handy to know about, you can get by just fine only knowing a few of them. The only reason I’ve included all this stuff right at the beginning is that it ends up making the rest of the chapter a lot more readable!\n\n\nBefore we go into any discussion of customising plots, we need a little more background. The important thing to note when using the plot() function, is that it’s another example of a generic function (Section @ref(generics), much like print() and summary(), and so its behaviour changes depending on what kind of input you give it. However, the plot() function is somewhat fancier than the other two, and its behaviour depends on two arguments, x (the first input, which is required) and y (which is optional). This makes it (a) extremely powerful once you get the hang of it, and (b) hilariously unpredictable, when you’re not sure what you’re doing. As much as possible, I’ll try to make clear what type of inputs produce what kinds of outputs. For now, however, it’s enough to note that I’m only doing very basic plotting, and as a consequence all of the work is being done by the plot.default() function.\nWhat kinds of customisations might we be interested in? If you look at the help documentation for the default plotting method (i.e., type ?plot.default or help(\"plot.default\")) you’ll see a very long list of arguments that you can specify to customise your plot. I’ll talk about several of them in a moment, but first I want to point out something that might seem quite wacky. When you look at all the different options that the help file talks about, you’ll notice that some of the options that it refers to are “proper” arguments to the plot.default() function, but it also goes on to mention a bunch of things that look like they’re supposed to be arguments, but they’re not listed in the “Usage” section of the file, and the documentation calls them graphical parameters instead. Even so, it’s usually possible to treat them as if they were arguments of the plotting function. Very odd. In order to stop my readers trying to find a brick and look up my home address, I’d better explain what’s going on; or at least give the basic gist behind it.\nWhat exactly is a graphical parameter? Basically, the idea is that there are some characteristics of a plot which are pretty universal: for instance, regardless of what kind of graph you’re drawing, you probably need to specify what colour to use for the plot, right? So you’d expect there to be something like a col argument to every single graphics function in R? Well, sort of. In order to avoid having hundreds of arguments for every single function, what R does is refer to a bunch of these “graphical parameters” which are pretty general purpose. Graphical parameters can be changed directly by using the low-level par() function, which I discuss briefly in Section @ref(par) though not in a lot of detail. If you look at the help files for graphical parameters (i.e., type ?par) you’ll see that there’s lots of them. Fortunately, (a) the default settings are generally pretty good so you can ignore the majority of the parameters, and (b) as you’ll see as we go through this chapter, you very rarely need to use par() directly, because you can “pretend” that graphical parameters are just additional arguments to your high-level function (e.g. plot.default()). In short… yes, R does have these wacky “graphical parameters” which can be quite confusing. But in most basic uses of the plotting functions, you can act as if they were just undocumented additional arguments to your function.\n\n\n\nOne of the first things that you’ll find yourself wanting to do when customising your plot is to label it better. You might want to specify more appropriate axis labels, add a title or add a subtitle. The arguments that you need to specify to make this happen are:\n\nmain. A character string containing the title.\nsub. A character string containing the subtitle.\nxlab. A character string containing the x-axis label.\nylab. A character string containing the y-axis label.\n\nThese aren’t graphical parameters, they’re arguments to the high-level function. However, because the high-level functions all rely on the same low-level function to do the drawing5 the names of these arguments are identical for pretty much every high-level function I’ve come across. Let’s have a look at what happens when we make use of all these arguments. Here’s the command. The picture that this draws is shown in Figure @ref(fig:secondplot).\n\nplot( x = Fibonacci,\n               main = \"You specify title using the 'main' argument\",\n               sub = \"The subtitle appears here! (Use the 'sub' argument for this)\",\n               xlab = \"The x-axis label is 'xlab'\",\n                ylab = \"The y-axis label is 'ylab'\" \n            )\n\n\n\n\nHow to add your own title, subtitle, x-axis label and y-axis label to the plot.\n\n\n\n\nIt’s more or less as you’d expect. The plot itself is identical to the one we drew in Figure @ref(fig:firstplot), except for the fact that we’ve changed the axis labels, and added a title and a subtitle. Even so, there’s a couple of interesting features worth calling your attention to. Firstly, notice that the subtitle is drawn below the plot, which I personally find annoying; as a consequence I almost never use subtitles. You may have a different opinion, of course, but the important thing is that you remember where the subtitle actually goes. Secondly, notice that R has decided to use boldface text and a larger font size for the title. This is one of my most hated default settings in R graphics, since I feel that it draws too much attention to the title. Generally, while I do want my reader to look at the title, I find that the R defaults are a bit overpowering, so I often like to change the settings. To that end, there are a bunch of graphical parameters that you can use to customise the font style:\n\nFont styles: font.main, font.sub, font.lab, font.axis. These four parameters control the font style used for the plot title (font.main), the subtitle (font.sub), the axis labels (font.lab: note that you can’t specify separate styles for the x-axis and y-axis without using low level commands), and the numbers next to the tick marks on the axis (font.axis). Somewhat irritatingly, these arguments are numbers instead of meaningful names: a value of 1 corresponds to plain text, 2 means boldface, 3 means italic and 4 means bold italic.\nFont colours: col.main, col.sub, col.lab, col.axis. These parameters do pretty much what the name says: each one specifies a colour in which to type each of the different bits of text. Conveniently, R has a very large number of named colours (type colours() to see a list of over 650 colour names that R knows), so you can use the English language name of the colour to select it.6 Thus, the parameter value here string like \"red\", \"gray25\" or \"springgreen4\" (yes, R really does recognise four different shades of “spring green”).\nFont size: cex.main, cex.sub, cex.lab, cex.axis. Font size is handled in a slightly curious way in R. The “cex” part here is short for “character expansion”, and it’s essentially a magnification value. By default, all of these are set to a value of 1, except for the font title: cex.main has a default magnification of 1.2, which is why the title font is 20% bigger than the others.\nFont family: family. This argument specifies a font family to use: the simplest way to use it is to set it to \"sans\", \"serif\", or \"mono\", corresponding to a san serif font, a serif font, or a monospaced font. If you want to, you can give the name of a specific font, but keep in mind that different operating systems use different fonts, so it’s probably safest to keep it simple. Better yet, unless you have some deep objections to the R defaults, just ignore this parameter entirely. That’s what I usually do.\n\nTo give you a sense of how you can use these parameters to customise your titles, the following command can be used to draw Figure @ref(fig:thirdplot):\n\nplot( x = Fibonacci,                           # the data to plot\n          main = \"The first 7 Fibonacci numbers\",  # the title\n          xlab = \"Position in the sequence\",       # x-axis label\n          ylab = \"The Fibonacci number\",           # y-axis \n          font.main = 1,\n          cex.main = 1,\n          font.axis = 2,\n          col.lab = \"gray50\" )\n\n\n\n\nHow to customise the appearance of the titles and labels.\n\n\n\n\nAlthough this command is quite long, it’s not complicated: all it does is override a bunch of the default parameter values. The only difficult aspect to this is that you have to remember what each of these parameters is called, and what all the different values are. And in practice I never remember: I have to look up the help documentation every time, or else look it up in this book.\n\n\n\nAdding and customising the titles associated with the plot is one way in which you can play around with what your picture looks like. Another thing that you’ll want to do is customise the appearance of the actual plot! To start with, let’s look at the single most important options that the plot() function (or, recalling that we’re dealing with a generic function, in this case the plot.default() function, since that’s the one doing all the work) provides for you to use, which is the type argument. The type argument specifies the visual style of the plot. The possible values for this are:\n\ntype = \"p\". Draw the points only.\ntype = \"l\". Draw a line through the points.\ntype = \"o\". Draw the line over the top of the points.\ntype = \"b\". Draw both points and lines, but don’t overplot.\ntype = \"h\". Draw “histogram-like” vertical bars.\ntype = \"s\". Draw a staircase, going horizontally then vertically.\ntype = \"S\". Draw a Staircase, going vertically then horizontally.\ntype = \"c\". Draw only the connecting lines from the “b” version.\ntype = \"n\". Draw nothing. (Apparently this is useful sometimes?)\n\nThe simplest way to illustrate what each of these really looks like is just to draw them. To that end, Figure @ref(fig:simpleplots) shows the same Fibonacci data, drawn using six different types of plot. As you can see, by altering the type argument you can get a qualitatively different appearance to your plot. In other words, as far as R is concerned, the only difference between a scatterplot (like the ones we drew in Section @ref(correl) and a line plot is that you draw a scatterplot by setting type = \"p\" and you draw a line plot by setting type = \"l\". However, that doesn’t imply that you should think of them as begin equivalent to each other. As you can see by looking at Figure @ref(fig:simpleplots), a line plot implies that there is some notion of continuity from one point to the next, whereas a scatterplot does not.\n\n\n\n\n\nChanging the type of the plot.\n\n\n\n\n\n\n\nIn Section @ref(figtitles) we talked about a group of graphical parameters that are related to the formatting of titles, axis labels etc. The second group of parameters I want to discuss are those related to the formatting of the plot itself:\n\nColour of the plot: col. As we saw with the previous colour-related parameters, the simplest way to specify this parameter is using a character string: e.g., col = \"blue\". It’s a pretty straightforward parameter to specify: the only real subtlety is that every high-level function tends to draw a different “thing” as it’s output, and so this parameter gets interpreted a little differently by different functions. However, for the plot.default() function it’s pretty simple: the col argument refers to the colour of the points and/or lines that get drawn!\nCharacter used to plot points: pch. The plot character parameter is a number, usually between 1 and 25. What it does is tell R what symbol to use to draw the points that it plots. The simplest way to illustrate what the different values do is with a picture. Figure @ref(fig:pch) shows the first 25 plotting characters. The default plotting character is a hollow circle (i.e., pch = 1).\n\n\n\n\n\n\nChanging the plotted characters\n\n\n\n\n\nPlot size: cex. This parameter describes a character expansion factor (i.e., magnification) for the plotted characters. By default cex=1, but if you want bigger symbols in your graph you should specify a larger value.\nLine type: lty. The line type parameter describes the kind of line that R draws. It has seven values which you can specify using a number between 0 and 7, or using a meaningful character string: \"blank\", \"solid\", \"dashed\", \"dotted\", \"dotdash\", \"longdash\", or \"twodash\". Note that the “blank” version (value 0) just means that R doesn’t draw the lines at all. The other six versions are shown in Figure @ref(fig:lty).\n\n\n\n\n\n\nLine types\n\n\n\n\n\nLine width: lwd. The last graphical parameter in this category that I want to mention is the line width parameter, which is just a number specifying the width of the line. The default value is 1. Not surprisingly, larger values produce thicker lines and smaller values produce thinner lines. Try playing around with different values of lwd to see what happens.\n\nTo illustrate what you can do by altering these parameters, let’s try the following command, the output is shown in Figure @ref(fig:fifthplot).\n\nplot( x = Fibonacci,\n         type = \"b\",\n         col = \"blue\",\n         pch = 19,\n         cex=5,\n         lty=2,\n         lwd=4)\n\n\n\n\nCustomising various aspects to the plot itself.\n\n\n\n\n\n\n\nThere are several other possibilities worth discussing. Ignoring graphical parameters for the moment, there’s a few other arguments to the plot.default() function that you might want to use. As before, many of these are standard arguments that are used by a lot of high level graphics functions:\n\nChanging the axis scales: xlim, ylim. Generally R does a pretty good job of figuring out where to set the edges of the plot. However, you can override its choices by setting the xlim and ylim arguments. For instance, if I decide I want the vertical scale of the plot to run from 0 to 100, then I’d set ylim = c(0, 100).\nSuppress labelling: ann. This is a logical-valued argument that you can use if you don’t want R to include any text for a title, subtitle or axis label. To do so, set ann = FALSE. This will stop R from including any text that would normally appear in those places. Note that this will override any of your manual titles. For example, if you try to add a title using the main argument, but you also specify ann = FALSE, no title will appear.\nSuppress axis drawing: axes. Again, this is a logical valued argument. Suppose you don’t want R to draw any axes at all. To suppress the axes, all you have to do is add axes = FALSE. This will remove the axes and the numbering, but not the axis labels (i.e. the xlab and ylab text). Note that you can get finer grain control over this by specifying the xaxt and yaxt graphical parameters instead (see below).\nInclude a framing box: frame.plot. Suppose you’ve removed the axes by setting axes = FALSE, but you still want to have a simple box drawn around the plot; that is, you only wanted to get rid of the numbering and the tick marks, but you want to keep the box. To do that, you set frame.plot = TRUE.\n\nNote that this list isn’t exhaustive. There are a few other arguments to the plot.default function that you can play with if you want to, but those are the ones you are probably most likely to want to use. As always, however, if these aren’t enough options for you, there’s also a number of other graphical parameters that you might want to play with as well. That’s the focus of the next section. In the meantime, here’s a command that makes use of all these different options. The output is shown in Figure @ref(fig:fourthplot), and it’s pretty much exactly as you’d expect. The axis scales on both the horizontal and vertical dimensions have been expanded, the axes have been suppressed as have the annotations, but I’ve kept a box around the plot.\n\nplot( x = Fibonacci,       # the data\n       xlim = c(0, 15),     # expand the x-scale\n       ylim = c(0, 15),     # expand the y-scale\n       ann = FALSE,         # delete all annotations\n       axes = FALSE,        # delete the axes\n       frame.plot = TRUE    # but include a framing box\n )\n\n\n\n\nAltering the scale and appearance of the plot axes.\n\n\n\n\nBefore moving on, I should point out that there are several graphical parameters relating to the axes, the box, and the general appearance of the plot which allow finer grain control over the appearance of the axes and the annotations.\n\nSuppressing the axes individually: xaxt, yaxt. These graphical parameters are basically just fancier versions of the axes argument we discussed earlier. If you want to stop R from drawing the vertical axis but you’d like it to keep the horizontal axis, set yaxt = \"n\". I trust that you can figure out how to keep the vertical axis and suppress the horizontal one!\nBox type: bty. In the same way that xaxt, yaxt are just fancy versions of axes, the box type parameter is really just a fancier version of the frame.plot argument, allowing you to specify exactly which out of the four borders you want to keep. The way we specify this parameter is a bit stupid, in my opinion: the possible values are \"o\" (the default), \"l\", \"7\", \"c\", \"u\", or \"]\", each of which will draw only those edges that the corresponding character suggests. That is, the letter \"c\" has a top, a bottom and a left, but is blank on the right hand side, whereas \"7\" has a top and a right, but is blank on the left and the bottom. Alternatively a value of \"n\" means that no box will be drawn.\nOrientation of the axis labels las. I presume that the name of this parameter is an acronym of label style or something along those lines; but what it actually does is govern the orientation of the text used to label the individual tick marks (i.e., the numbering, not the xlab and ylab axis labels). There are four possible values for las: A value of 0 means that the labels of both axes are printed parallel to the axis itself (the default). A value of 1 means that the text is always horizontal. A value of 2 means that the labelling text is printed at right angles to the axis. Finally, a value of 3 means that the text is always vertical.\n\nAgain, these aren’t the only possibilities. There are a few other graphical parameters that I haven’t mentioned that you could use to customise the appearance of the axes,7 but that’s probably enough (or more than enough) for now. To give a sense of how you could use these parameters, let’s try the following command. The output is shown in Figure @ref(fig:sixthplot). As you can see, this isn’t a very useful plot at all. However, it does illustrate the graphical parameters we’re talking about, so I suppose it serves its purpose.\n\n    plot( x = Fibonacci, # the data\n         xaxt = \"n\",       # don't draw the x-axis  \n         bty = \"]\",        # keep bottom, right and top of box only\n         las = 1 )         # rotate the text\n\n\n\n\nOther ways to customise the axes\n\n\n\n\n\n\n\nAt this point, a lot of readers will be probably be thinking something along the lines of, “if there’s this much detail just for drawing a simple plot, how horrible is it going to get when we start looking at more complicated things?” Perhaps, contrary to my earlier pleas for mercy, you’ve found a brick to hurl and are right now leafing through an Adelaide phone book trying to find my address. Well, fear not! And please, put the brick down. In a lot of ways, we’ve gone through the hardest part: we’ve already covered vast majority of the plot customisations that you might want to do. As you’ll see, each of the other high level plotting commands we’ll talk about will only have a smallish number of additional options. Better yet, even though I’ve told you about a billion different ways of tweaking your plot, you don’t usually need them. So in practice, now that you’ve read over it once to get the gist, the majority of the content of this section is stuff you can safely forget: just remember to come back to this section later on when you want to tweak your plot.\n\n\n\n\nNow that we’ve tamed (or possibly fled from) the beast that is R graphical parameters, let’s talk more seriously about some real life graphics that you’ll want to draw. We begin with the humble histogram. Histograms are one of the simplest and most useful ways of visualising data. They make most sense when you have an interval or ratio scale (e.g., the afl.margins data from Chapter @ref(descriptives) and what you want to do is get an overall impression of the data. Most of you probably know how histograms work, since they’re so widely used, but for the sake of completeness I’ll describe them. All you do is divide up the possible values into bins, and then count the number of observations that fall within each bin. This count is referred to as the frequency of the bin, and is displayed as a bar: in the AFL winning margins data, there are 33 games in which the winning margin was less than 10 points, and it is this fact that is represented by the height of the leftmost bar in Figure @ref(fig:hist1a). Drawing this histogram in R is pretty straightforward. The function you need to use is called hist(), and it has pretty reasonable default settings. In fact, Figure @ref(fig:hist1a) is exactly what you get if you just type this:\n\nhist( afl.margins )\n\n\n\n\n\n\nThe default histogram that R produces\n\n\n\n\nAlthough this image would need a lot of cleaning up in order to make a good presentation graphic (i.e., one you’d include in a report), it nevertheless does a pretty good job of describing the data. In fact, the big strength of a histogram is that (properly used) it does show the entire spread of the data, so you can get a pretty good sense about what it looks like. The downside to histograms is that they aren’t very compact: unlike some of the other plots I’ll talk about it’s hard to cram 20-30 histograms into a single image without overwhelming the viewer. And of course, if your data are nominal scale (e.g., the afl.finalists data) then histograms are useless.\nThe main subtlety that you need to be aware of when drawing histograms is determining where the breaks that separate bins should be located, and (relatedly) how many breaks there should be. In Figure @ref(fig:hist1a), you can see that R has made pretty sensible choices all by itself: the breaks are located at 0, 10, 20, … 120, which is exactly what I would have done had I been forced to make a choice myself. On the other hand, consider the two histograms in Figure @ref(fig:hist1b) and @ref(fig:hist1c), which I produced using the following two commands:\n\nhist( x = afl.margins, breaks = 3 )\n\n\n\n\nA histogram with too few bins\n\n\n\n\n\nhist( x = afl.margins, breaks = 0:116 )\n\n\n\n\nA histogram with too many bins\n\n\n\n\nIn Figure @ref(fig:hist1c), the bins are only 1 point wide. As a result, although the plot is very informative (it displays the entire data set with no loss of information at all!) the plot is very hard to interpret, and feels quite cluttered. On the other hand, the plot in Figure @ref(fig:hist1b) has a bin width of 50 points, and has the opposite problem: it’s very easy to “read” this plot, but it doesn’t convey a lot of information. One gets the sense that this histogram is hiding too much. In short, the way in which you specify the breaks has a big effect on what the histogram looks like, so it’s important to make sure you choose the breaks sensibly. In general R does a pretty good job of selecting the breaks on its own, since it makes use of some quite clever tricks that statisticians have devised for automatically selecting the right bins for a histogram, but nevertheless it’s usually a good idea to play around with the breaks a bit to see what happens.\nThere is one fairly important thing to add regarding how the breaks argument works. There are two different ways you can specify the breaks. You can either specify how many breaks you want (which is what I did for panel b when I typed breaks = 3) and let R figure out where they should go, or you can provide a vector that tells R exactly where the breaks should be placed (which is what I did for panel c when I typed breaks = 0:116). The behaviour of the hist() function is slightly different depending on which version you use. If all you do is tell it how many breaks you want, R treats it as a “suggestion” not as a demand. It assumes you want “approximately 3” breaks, but if it doesn’t think that this would look very pretty on screen, it picks a different (but similar) number. It does this for a sensible reason – it tries to make sure that the breaks are located at sensible values (like 10) rather than stupid ones (like 7.224414). And most of the time R is right: usually, when a human researcher says “give me 3 breaks”, he or she really does mean “give me approximately 3 breaks, and don’t put them in stupid places”. However, sometimes R is dead wrong. Sometimes you really do mean “exactly 3 breaks”, and you know precisely where you want them to go. So you need to invoke “real person privilege”, and order R to do what it’s bloody well told. In order to do that, you have to input the full vector that tells R exactly where you want the breaks. If you do that, R will go back to behaving like the nice little obedient calculator that it’s supposed to be.\n\n\nOkay, so at this point we can draw a basic histogram, and we can alter the number and even the location of the breaks. However, the visual style of the histograms shown in Figures @ref(fig:hist1a), @ref(fig:hist1b), and @ref(fig:hist1c) could stand to be improved. We can fix this by making use of some of the other arguments to the hist() function. Most of the things you might want to try doing have already been covered in Section @ref(introplotting), but there’s a few new things:\n\nShading lines: density, angle. You can add diagonal lines to shade the bars: the density value is a number indicating how many lines per inch R should draw (the default value of NULL means no lines), and the angle is a number indicating how many degrees from horizontal the lines should be drawn at (default is angle = 45 degrees).\nSpecifics regarding colours: col, border. You can also change the colours: in this instance the col parameter sets the colour of the shading (either the shading lines if there are any, or else the colour of the interior of the bars if there are not), and the border argument sets the colour of the edges of the bars.\nLabelling the bars: labels. You can also attach labels to each of the bars using the labels argument. The simplest way to do this is to set labels = TRUE, in which case R will add a number just above each bar, that number being the exact number of observations in the bin. Alternatively, you can choose the labels yourself, by inputting a vector of strings, e.g., labels = c(\"label 1\",\"label 2\",\"etc\")\n\nNot surprisingly, this doesn’t exhaust the possibilities. If you type help(\"hist\") or ?hist and have a look at the help documentation for histograms, you’ll see a few more options. A histogram that makes use of the histogram-specific customisations as well as several of the options we discussed in Section @ref(introplotting) is shown in Figure @ref(fig:hist1d). The R command that I used to draw it is this:\n\nhist( x = afl.margins, \n      main = \"2010 AFL margins\", # title of the plot\n      xlab = \"Margin\",           # set the x-axis label\n      density = 10,              # draw shading lines: 10 per inch\n      angle = 40,                # set the angle of the shading lines is 40 degrees\n      border = \"gray20\",         # set the colour of the borders of the bars\n      col = \"gray80\",            # set the colour of the shading lines\n      labels = TRUE,             # add frequency labels to each bar\n      ylim = c(0,40)             # change the scale of the y-axis\n)\n\n\n\n\nA histogram with histogram specific customisations\n\n\n\n\nOverall, this is a much nicer histogram than the default ones.\n\n\n\n\nHistograms are one of the most widely used methods for displaying the observed values for a variable. They’re simple, pretty, and very informative. However, they do take a little bit of effort to draw. Sometimes it can be quite useful to make use of simpler, if less visually appealing, options. One such alternative is the stem and leaf plot. To a first approximation you can think of a stem and leaf plot as a kind of text-based histogram. Stem and leaf plots aren’t used as widely these days as they were 30 years ago, since it’s now just as easy to draw a histogram as it is to draw a stem and leaf plot. Not only that, they don’t work very well for larger data sets. As a consequence you probably won’t have as much of a need to use them yourself, though you may run into them in older publications. These days, the only real world situation where I use them is if I have a small data set with 20-30 data points and I don’t have a computer handy, because it’s pretty easy to quickly sketch a stem and leaf plot by hand.\nWith all that as background, lets have a look at stem and leaf plots. The AFL margins data contains 176 observations, which is at the upper end for what you can realistically plot this way. The function in R for drawing stem and leaf plots is called stem() and if we ask for a stem and leaf plot of the afl.margins data, here’s what we get:\n\nstem( afl.margins )\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n   0 | 001111223333333344567788888999999\n   1 | 0000011122234456666899999\n   2 | 00011222333445566667788999999\n   3 | 01223555566666678888899\n   4 | 012334444477788899\n   5 | 00002233445556667\n   6 | 0113455678\n   7 | 01123556\n   8 | 122349\n   9 | 458\n  10 | 148\n  11 | 6\n\n\nThe values to the left of the | are called stems and the values to the right are called leaves. If you just look at the shape that the leaves make, you can see something that looks a lot like a histogram made out of numbers, just rotated by 90 degrees. But if you know how to read the plot, there’s quite a lot of additional information here. In fact, it’s also giving you the actual values of all of the observations in the data set. To illustrate, let’s have a look at the last line in the stem and leaf plot, namely 11 | 6. Specifically, let’s compare this to the largest values of the afl.margins data set:\n\nmax( afl.margins )\n\n[1] 116\n\n\nHm… 11 | 6 versus 116. Obviously the stem and leaf plot is trying to tell us that the largest value in the data set is 116. Similarly, when we look at the line that reads 10 | 148, the way we interpret it to note that the stem and leaf plot is telling us that the data set contains observations with values 101, 104 and 108. Finally, when we see something like 5 | 00002233445556667 the four 0s in the the stem and leaf plot are telling us that there are four observations with value 50.\nI won’t talk about them in a lot of detail, but I should point out that some customisation options are available for stem and leaf plots in R. The two arguments that you can use to do this are:\n\nscale. Changing the scale of the plot (default value is 1), which is analogous to changing the number of breaks in a histogram. Reducing the scale causes R to reduce the number of stem values (i.e., the number of breaks, if this were a histogram) that the plot uses.\nwidth. The second way that to can customise a stem and leaf plot is to alter the width (default value is 80). Changing the width alters the maximum number of leaf values that can be displayed for any given stem.\n\nHowever, since stem and leaf plots aren’t as important as they used to be, I’ll leave it to the interested reader to investigate these options. Try the following two commands to see what happens:\n\nstem( x = afl.margins, scale = .25 )\nstem( x = afl.margins, width = 20 )\n\nThe only other thing to note about stem and leaf plots is the line in which R tells you where the decimal point is. If our data set had included only the numbers .11, .15, .23, .35 and .59 and we’d drawn a stem and leaf plot of these data, then R would move the decimal point: the stem values would be 1,2,3,4 and 5, but R would tell you that the decimal point has moved to the left of the | symbol. If you want to see this in action, try the following command:\n\nstem( x = afl.margins / 1000 )\n\nThe stem and leaf plot itself will look identical to the original one we drew, except for the fact that R will tell you that the decimal point has moved.\n\n\n\nAnother alternative to histograms is a boxplot, sometimes called a “box and whiskers” plot. Like histograms, they’re most suited to interval or ratio scale data. The idea behind a boxplot is to provide a simple visual depiction of the median, the interquartile range, and the range of the data. And because they do so in a fairly compact way, boxplots have become a very popular statistical graphic, especially during the exploratory stage of data analysis when you’re trying to understand the data yourself. Let’s have a look at how they work, again using the afl.margins data as our example. Firstly, let’s actually calculate these numbers ourselves using the summary() function:8\n\nsummary( afl.margins )\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00   12.75   30.50   35.30   50.50  116.00 \n\n\nSo how does a boxplot capture these numbers? The easiest way to describe what a boxplot looks like is just to draw one. The function for doing this in R is (surprise, surprise) boxplot(). As always there’s a lot of optional arguments that you can specify if you want, but for the most part you can just let R choose the defaults for you. That said, I’m going to override one of the defaults to start with by specifying the range option, but for the most part you won’t want to do this (I’ll explain why in a minute). With that as preamble, let’s try the following command:\n\nboxplot( x = afl.margins, range = 100 )\n\n\n\n\nA basic boxplot\n\n\n\n\nWhat R draws is shown in Figure @ref(fig:boxplot1a), the most basic boxplot possible. When you look at this plot, this is how you should interpret it: the thick line in the middle of the box is the median; the box itself spans the range from the 25th percentile to the 75th percentile; and the “whiskers” cover the full range from the minimum value to the maximum value. This is summarised in the annotated plot in Figure @ref(fig:boxplot1b).\n\n\n\n\n\nAn annotated boxplot\n\n\n\n\nIn practice, this isn’t quite how boxplots usually work. In most applications, the “whiskers” don’t cover the full range from minimum to maximum. Instead, they actually go out to the most extreme data point that doesn’t exceed a certain bound. By default, this value is 1.5 times the interquartile range, corresponding to a range value of 1.5. Any observation whose value falls outside this range is plotted as a circle instead of being covered by the whiskers, and is commonly referred to as an outlier. For our AFL margins data, there is one observation (a game with a margin of 116 points) that falls outside this range. As a consequence, the upper whisker is pulled back to the next largest observation (a value of 108), and the observation at 116 is plotted as a circle. This is illustrated in Figure @ref(fig:boxplot2a). Since the default value is range = 1.5 we can draw this plot using the simple command\n\nboxplot( afl.margins )\n\n\n\n\nBy default, R will only extent the whiskers a distance of 1.5 times the interquartile range, and will plot any points that fall outside that range separately\n\n\n\n\n\n\nI’ll talk a little more about the relationship between boxplots and outliers in the Section @ref(boxplotoutliers), but before I do let’s take the time to clean this figure up. Boxplots in R are extremely customisable. In addition to the usual range of graphical parameters that you can tweak to make the plot look nice, you can also exercise nearly complete control over every element to the plot. Consider the boxplot in Figure @ref(fig:boxplot2b): in this version of the plot, not only have I added labels (xlab, ylab) and removed the stupid border (frame.plot), I’ve also dimmed all of the graphical elements of the boxplot except the central bar that plots the median (border) so as to draw more attention to the median rather than the rest of the boxplot.\n\n\n\n\n\nA boxplot with boxplot specific customisations\n\n\n\n\nYou’ve seen all these options in previous sections in this chapter, so hopefully those customisations won’t need any further explanation. However, I’ve done two new things as well: I’ve deleted the cross-bars at the top and bottom of the whiskers (known as the “staples” of the plot), and converted the whiskers themselves to solid lines. The arguments that I used to do this are called by the ridiculous names of staplewex and whisklty,9 and I’ll explain these in a moment.\nBut first, here’s the actual command I used to draw this figure:\n\nboxplot( x = afl.margins,           # the data\n          xlab = \"AFL games, 2010\",  # x-axis label\n          ylab = \"Winning Margin\",   # y-axis label\n          border = \"grey50\",         # dim the border of the box\n          frame.plot = FALSE,        # don't draw a frame\n          staplewex = 0,             # don't draw staples\n          whisklty = 1               # solid line for whisker \n )\n\nOverall, I think the resulting boxplot is a huge improvement in visual design over the default version. In my opinion at least, there’s a fairly minimalist aesthetic that governs good statistical graphics. Ideally, every visual element that you add to a plot should convey part of the message. If your plot includes things that don’t actually help the reader learn anything new, you should consider removing them. Personally, I can’t see the point of the cross-bars on a standard boxplot, so I’ve deleted them.\nOkay, what commands can we use to customise the boxplot? If you type ?boxplot and flick through the help documentation, you’ll notice that it does mention staplewex as an argument, but there’s no mention of whisklty. The reason for this is that the function that handles the drawing is called bxp(), so if you type ?bxp all the gory details appear. Here’s the short summary. In order to understand why these arguments have such stupid names, you need to recognise that they’re put together from two components. The first part of the argument name specifies one part of the box plot: staple refers to the staples of the plot (i.e., the cross-bars), and whisk refers to the whiskers. The second part of the name specifies a graphical parameter: wex is a width parameter, and lty is a line type parameter. The parts of the plot you can customise are:\n\nbox. The box that covers the interquartile range.\nmed. The line used to show the median.\nwhisk. The vertical lines used to draw the whiskers.\nstaple. The cross bars at the ends of the whiskers.\nout. The points used to show the outliers.\n\nThe actual graphical parameters that you might want to specify are slightly different for each visual element, just because they’re different shapes from each other. As a consequence, the following options are available:\n\nWidth expansion: boxwex, staplewex, outwex. These are scaling factors that govern the width of various parts of the plot. The default scaling factor is (usually) 0.8 for the box, and 0.5 for the other two. Note that in the case of the outliers this parameter is meaningless unless you decide to draw lines plotting the outliers rather than use points.\nLine type: boxlty, medlty, whisklty, staplelty, outlty. These govern the line type for the relevant elements. The values for this are exactly the same as those used for the regular lty parameter, with two exceptions. There’s an additional option where you can set medlty = \"blank\" to suppress the median line completely (useful if you want to draw a point for the median rather than plot a line). Similarly, by default the outlier line type is set to outlty = \"blank\", because the default behaviour is to draw outliers as points instead of lines.\nLine width: boxlwd, medlwd, whisklwd, staplelwd, outlwd. These govern the line widths for the relevant elements, and behave the same way as the regular lwd parameter. The only thing to note is that the default value for medlwd value is three times the value of the others.\nLine colour: boxcol, medcol, whiskcol, staplecol, outcol. These govern the colour of the lines used to draw the relevant elements. Specify a colour in the same way that you usually do.\nFill colour: boxfill. What colour should we use to fill the box?\nPoint character: medpch, outpch. These behave like the regular pch parameter used to select the plot character. Note that you can set outpch = NA to stop R from plotting the outliers at all, and you can also set medpch = NA to stop it from drawing a character for the median (this is the default!)\nPoint expansion: medcex, outcex. Size parameters for the points used to plot medians and outliers. These are only meaningful if the corresponding points are actually plotted. So for the default boxplot, which includes outlier points but uses a line rather than a point to draw the median, only the outcex parameter is meaningful.\nBackground colours: medbg, outbg. Again, the background colours are only meaningful if the points are actually plotted.\n\nTaken as a group, these parameters allow you almost complete freedom to select the graphical style for your boxplot that you feel is most appropriate to the data set you’re trying to describe. That said, when you’re first starting out there’s no shame in using the default settings! But if you want to master the art of designing beautiful figures, it helps to try playing around with these parameters to see what works and what doesn’t. Finally, I should mention a few other arguments that you might want to make use of:\n\nhorizontal. Set this to TRUE to display the plot horizontally rather than vertically.\nvarwidth. Set this to TRUE to get R to scale the width of each box so that the areas are proportional to the number of observations that contribute to the boxplot. This is only useful if you’re drawing multiple boxplots at once (see Section @ref(multipleboxplots).\nshow.names. Set this to TRUE to get R to attach labels to the boxplots.\nnotch. If you set notch = TRUE, R will draw little notches in the sides of each box. If the notches of two boxplots don’t overlap, then there is a “statistically significant” difference between the corresponding medians. If you haven’t read Chapter @ref(hypothesistesting), ignore this argument – we haven’t discussed statistical significance, so this doesn’t mean much to you. I’m mentioning it only because you might want to come back to the topic later on. (see also the notch.frac option when you type ?bxp).\n\n\n\n\nBecause the boxplot automatically (unless you change the range argument) separates out those observations that lie within a certain range, people often use them as an informal method for detecting outliers: observations that are “suspiciously” distant from the rest of the data. Here’s an example. Suppose that I’d drawn the boxplot for the AFL margins data, and it came up looking like Figure @ref(fig:boxplotoutlier).\n\n\n\n\n\nA boxplot showing one very suspicious outlier! I’ve drawn this plot in a similar, minimalist style to the one in Figure @ref(fig:boxplot2b), but I’ve used the horizontal argument to draw it sideways in order to save space.\n\n\n\n\nIt’s pretty clear that something funny is going on with one of the observations. Apparently, there was one game in which the margin was over 300 points! That doesn’t sound right to me. Now that I’ve become suspicious, it’s time to look a bit more closely at the data. One function that can be handy for this is the which() function; it takes as input a vector of logicals, and outputs the indices of the TRUE cases. This is particularly useful in the current context because it lets me do this:\n\nsuspicious.cases &lt;- afl.margins &gt; 300\nwhich( suspicious.cases )\n\n[1] 137\n\n\nalthough in real life I probably wouldn’t bother creating the suspicious.cases variable: I’d just cut out the middle man and use a command like which( afl.margins &gt; 300 ). In any case, what this has done is shown me that the outlier corresponds to game 137. Then, I find the recorded margin for that game:\n\nafl.margins[137]\n\n[1] 333\n\n\nHm. That definitely doesn’t sound right. So then I go back to the original data source (the internet!) and I discover that the actual margin of that game was 33 points. Now it’s pretty clear what happened. Someone must have typed in the wrong number. Easily fixed, just by typing afl.margins[137] &lt;- 33. While this might seem like a silly example, I should stress that this kind of thing actually happens a lot. Real world data sets are often riddled with stupid errors, especially when someone had to type something into a computer at some point. In fact, there’s actually a name for this phase of data analysis, since in practice it can waste a huge chunk of our time: data cleaning. It involves searching for typos, missing data and all sorts of other obnoxious errors in raw data files.10\nWhat about the real data? Does the value of 116 constitute a funny observation not? Possibly. As it turns out the game in question was Fremantle v Hawthorn, and was played in round 21 (the second last home and away round of the season). Fremantle had already qualified for the final series and for them the outcome of the game was irrelevant; and the team decided to rest several of their star players. As a consequence, Fremantle went into the game severely underpowered. In contrast, Hawthorn had started the season very poorly but had ended on a massive winning streak, and for them a win could secure a place in the finals. With the game played on Hawthorn’s home turf11 and with so many unusual factors at play, it is perhaps no surprise that Hawthorn annihilated Fremantle by 116 points. Two weeks later, however, the two teams met again in an elimination final on Fremantle’s home ground, and Fremantle won comfortably by 30 points.12\nSo, should we exclude the game from subsequent analyses? If this were a psychology experiment rather than an AFL season, I’d be quite tempted to exclude it because there’s pretty strong evidence that Fremantle weren’t really trying very hard: and to the extent that my research question is based on an assumption that participants are genuinely trying to do the task. On the other hand, in a lot of studies we’re actually interested in seeing the full range of possible behaviour, and that includes situations where people decide not to try very hard: so excluding that observation would be a bad idea. In the context of the AFL data, a similar distinction applies. If I’d been trying to make tips about who would perform well in the finals, I would have (and in fact did) disregard the Round 21 massacre, because it’s way too misleading. On the other hand, if my interest is solely in the home and away season itself, I think it would be a shame to throw away information pertaining to one of the most distinctive (if boring) games of the year. In other words, the decision about whether to include outliers or exclude them depends heavily on why you think the data look they way they do, and what you want to use the data for. Statistical tools can provide an automatic method for suggesting candidates for deletion, but you really need to exercise good judgment here. As I’ve said before, R is a mindless automaton. It doesn’t watch the footy, so it lacks the broader context to make an informed decision. You are not a mindless automaton, so you should exercise judgment: if the outlier looks legitimate to you, then keep it. In any case, I’ll return to the topic again in Section @ref(regressiondiagnostics), so let’s return to our discussion of how to draw boxplots.\n\n\n\nOne last thing. What if you want to draw multiple boxplots at once? Suppose, for instance, I wanted separate boxplots showing the AFL margins not just for 2010, but for every year between 1987 and 2010. To do that, the first thing we’ll have to do is find the data. These are stored in the aflsmall2.Rdata file. So let’s load it and take a quick peek at what’s inside:\n\nload( \"\"materials/data/aflsmall2.Rdata\"\" )\nwho( TRUE )\n#   -- Name --   -- Class --   -- Size --\n#   afl2         data.frame    4296 x 2  \n#    $margin     numeric       4296      \n#    $year       numeric       4296     \n\nNotice that afl2 data frame is pretty big. It contains 4296 games, which is far more than I want to see printed out on my computer screen. To that end, R provides you with a few useful functions to print out only a few of the row in the data frame. The first of these is head() which prints out the first 6 rows, of the data frame, like this:\n\nhead( afl2 )\n\n  margin year\n1     33 1987\n2     59 1987\n3     45 1987\n4     91 1987\n5     39 1987\n6      1 1987\n\n\nYou can also use the tail() function to print out the last 6 rows. The car package also provides a handy little function called some() which prints out a random subset of the rows.\nIn any case, the important thing is that we have the afl2 data frame which contains the variables that we’re interested in. What we want to do is have R draw boxplots for the margin variable, plotted separately for each separate year. The way to do this using the boxplot() function is to input a formula rather than a variable as the input. In this case, the formula we want is margin ~ year. So our boxplot command now looks like this. The result is shown in Figure @ref(fig:multipleboxplots).13\n\nboxplot( formula = margin ~ year,\n         data = afl2\n)\n\n\n\n\nBoxplots showing the AFL winning margins for the 24 years from 1987 to 2010 inclusive. This is the default plot created by R, with no annotations added and no changes to the visual design. It’s pretty readable, though at a minimum you’d want to include some basic annotations labelling the axes. Compare and contrast with Figure @ref(fig:multipleboxplots2)\n\n\n\n\nEven this, the default version of the plot, gives a sense of why it’s sometimes useful to choose boxplots instead of histograms. Even before taking the time to turn this basic output into something more readable, it’s possible to get a good sense of what the data look like from year to year without getting overwhelmed with too much detail. Now imagine what would have happened if I’d tried to cram 24 histograms into this space: no chance at all that the reader is going to learn anything useful.\nThat being said, the default boxplot leaves a great deal to be desired in terms of visual clarity. The outliers are too visually prominent, the dotted lines look messy, and the interesting content (i.e., the behaviour of the median and the interquartile range across years) gets a little obscured. Fortunately, this is easy to fix, since we’ve already covered a lot of tools you can use to customise your output. After playing around with several different versions of the plot, the one I settled on is shown in Figure @ref(fig:multipleboxplots2). The command I used to produce it is long, but not complicated:\n\nboxplot( formula =  margin ~ year,   # the formula\n           data = afl2,                # the data set\n           xlab = \"AFL season\",        # x axis label\n           ylab = \"Winning Margin\",    # y axis label\n           frame.plot = FALSE,         # don't draw a frame\n           staplewex = 0,              # don't draw staples\n           staplecol = \"white\",        # (fixes a tiny display issue)\n           boxwex = .75,               # narrow the boxes slightly\n           boxfill = \"grey80\",         # lightly shade the boxes\n           whisklty = 1,               # solid line for whiskers \n           whiskcol = \"grey70\",        # dim the whiskers\n           boxcol = \"grey70\",          # dim the box borders\n           outcol = \"grey70\",          # dim the outliers\n           outpch = 20,                # outliers as solid dots\n           outcex = .5,                # shrink the outliers\n           medlty = \"blank\",           # no line for the medians\n           medpch = 20,                # instead, draw solid dots\n           medlwd = 1.5                # make them larger\n )\n\n\n\n\nA cleaned up version of Figure @ref(fig:multipleboxplots). Notice that I’ve used a very minimalist design for the boxplots, so as to focus the eye on the medians. I’ve also converted the medians to solid dots, to convey a sense that year to year variation in the median should be thought of as a single coherent plot (similar to what we did when plotting the Fibonacci variable earlier). The size of outliers has been shrunk, because they aren’t actually very interesting. In contrast, I’ve added a fill colour to the boxes, to make it easier to look at the trend in the interquartile range across years.\n\n\n\n\nOf course, given that the command is that long, you might have guessed that I didn’t spend ages typing all that rubbish in over and over again. Instead, I wrote a script, which I kept tweaking until it produced the figure that I wanted. We’ll talk about scripts later in Section @ref(scripts), but given the length of the command I thought I’d remind you that there’s an easier way of trying out different commands than typing them all in over and over.\n\n\n\n\nScatterplots are a simple but effective tool for visualising data. We’ve already seen scatterplots in this chapter, when using the plot() function to draw the Fibonacci variable as a collection of dots (Section @ref(introplotting). However, for the purposes of this section I have a slightly different notion in mind. Instead of just plotting one variable, what I want to do with my scatterplot is display the relationship between two variables, like we saw with the figures in the section on correlation (Section @ref(correl). It’s this latter application that we usually have in mind when we use the term “scatterplot”. In this kind of plot, each observation corresponds to one dot: the horizontal location of the dot plots the value of the observation on one variable, and the vertical location displays its value on the other variable. In many situations you don’t really have a clear opinions about what the causal relationship is (e.g., does A cause B, or does B cause A, or does some other variable C control both A and B). If that’s the case, it doesn’t really matter which variable you plot on the x-axis and which one you plot on the y-axis. However, in many situations you do have a pretty strong idea which variable you think is most likely to be causal, or at least you have some suspicions in that direction. If so, then it’s conventional to plot the cause variable on the x-axis, and the effect variable on the y-axis. With that in mind, let’s look at how to draw scatterplots in R, using the same parenthood data set (i.e. parenthood.Rdata) that I used when introducing the idea of correlations.\nSuppose my goal is to draw a scatterplot displaying the relationship between the amount of sleep that I get (dan.sleep) and how grumpy I am the next day (dan.grump). As you might expect given our earlier use of plot() to display the Fibonacci data, the function that we use is the plot() function, but because it’s a generic function all the hard work is still being done by the plot.default() function. In any case, there are two different ways in which we can get the plot that we’re after. The first way is to specify the name of the variable to be plotted on the x axis and the variable to be plotted on the y axis. When we do it this way, the command looks like this:\n\nplot( x = parenthood$dan.sleep,   # data on the x-axis\n      y = parenthood$dan.grump    # data on the y-axis\n )  \n\n\n\n\nthe default scatterplot that R produces\n\n\n\n\nThe second way do to it is to use a “formula and data frame” format, but I’m going to avoid using it.14 For now, let’s just stick with the x and y version. If we do this, the result is the very basic scatterplot shown in Figure @ref(fig:scattera). This serves fairly well, but there’s a few customisations that we probably want to make in order to have this work properly. As usual, we want to add some labels, but there’s a few other things we might want to do as well. Firstly, it’s sometimes useful to rescale the plots. In Figure @ref(fig:scattera) R has selected the scales so that the data fall neatly in the middle. But, in this case, we happen to know that the grumpiness measure falls on a scale from 0 to 100, and the hours slept falls on a natural scale between 0 hours and about 12 or so hours (the longest I can sleep in real life). So the command I might use to draw this is:\n\nplot( x = parenthood$dan.sleep,          # data on the x-axis\n       y = parenthood$dan.grump,         # data on the y-axis\n       xlab = \"My sleep (hours)\",        # x-axis label\n       ylab = \"My grumpiness (0-100)\",   # y-axis label\n       xlim = c(0,12),                   # scale the x-axis\n       ylim = c(0,100),                  # scale the y-axis\n       pch = 20,                         # change the plot type\n       col = \"gray50\",                   # dim the dots slightly\n       frame.plot = FALSE                # don't draw a box\n )\n\nThis command produces the scatterplot in Figure @ref(fig:scatterb), or at least very nearly. What it doesn’t do is draw the line through the middle of the points. Sometimes it can be very useful to do this, and I can do so using lines(), which is a low level plotting function. Better yet, the arguments that I need to specify are pretty much the exact same ones that I use when calling the plot() function. That is, suppose that I want to draw a line that goes from the point (4,93) to the point (9.5,37). Then the x locations can be specified by the vector c(4,9.5) and the y locations correspond to the vector c(93,37). In other words, I use this command:\n\nplot( x = parenthood$dan.sleep,          # data on the x-axis\n       y = parenthood$dan.grump,         # data on the y-axis\n       xlab = \"My sleep (hours)\",        # x-axis label\n       ylab = \"My grumpiness (0-100)\",   # y-axis label\n       xlim = c(0,12),                   # scale the x-axis\n       ylim = c(0,100),                  # scale the y-axis\n       pch = 20,                         # change the plot type\n       col = \"gray50\",                   # dim the dots slightly\n       frame.plot = FALSE                # don't draw a box\n)\n lines( x = c(4,9.5),   # the horizontal locations\n        y = c(93,37),   # the vertical locations\n        lwd = 2         # line width\n)\n\n\n\n\nA scatterplot with scatter plot specific customisations\n\n\n\n\nAnd when I do so, R plots the line over the top of the plot that I drew using the previous command. In most realistic data analysis situations you absolutely don’t want to just guess where the line through the points goes, since there’s about a billion different ways in which you can get R to do a better job. However, it does at least illustrate the basic idea.\nOne possibility, if you do want to get R to draw nice clean lines through the data for you, is to use the scatterplot() function in the car package. Before we can use scatterplot() we need to load the package:\n\nlibrary( car )\n\nLoading required package: carData\n\n\nHaving done so, we can now use the function. The command we need is this one:\n\nscatterplot( dan.grump ~ dan.sleep,\n              data = parenthood, \n              smooth = FALSE\n)\n\n\n\n\n\n\nA fancy scatterplot drawn using the scatterplot() function in the car package.\n\n\n\n\nThe first two arguments should be familiar: the first input is a formula dan.grump ~ dan.sleep telling R what variables to plot,15 and the second specifies a data frame. The third argument smooth I’ve set to FALSE to stop the scatterplot() function from drawing a fancy “smoothed” trendline (since it’s a bit confusing to beginners). The scatterplot itself is shown in Figure @ref(fig:fancyscatter). As you can see, it’s not only drawn the scatterplot, but its also drawn boxplots for each of the two variables, as well as a simple line of best fit showing the relationship between the two variables.\n\n\nOften you find yourself wanting to look at the relationships between several variables at once. One useful tool for doing so is to produce a scatterplot matrix, analogous to the correlation matrix.\n\ncor( x = parenthood ) # calculate correlation matrix\n\n             dan.sleep  baby.sleep   dan.grump         day\ndan.sleep   1.00000000  0.62794934 -0.90338404 -0.09840768\nbaby.sleep  0.62794934  1.00000000 -0.56596373 -0.01043394\ndan.grump  -0.90338404 -0.56596373  1.00000000  0.07647926\nday        -0.09840768 -0.01043394  0.07647926  1.00000000\n\n\nWe can get a the corresponding scatterplot matrix by using the pairs() function:16\n\npairs( x = parenthood ) # draw corresponding scatterplot matrix  \n\n\n\n\nA scatterplot matrix from the `pairs() function\n\n\n\n\nThe output of the pairs() command is shown in Figure @ref(fig:pairs). An alternative way of calling the pairs() function, which can be useful in some situations, is to specify the variables to include using a one-sided formula. For instance, this\n\npairs( formula = ~ dan.sleep + baby.sleep + dan.grump,\n        data = parenthood\n)\n\nwould produce a \\(3 \\times 3\\) scatterplot matrix that only compare dan.sleep, dan.grump and baby.sleep. Obviously, the first version is much easier, but there are cases where you really only want to look at a few of the variables, so it’s nice to use the formula interface.\n\n\n\n\nAnother form of graph that you often want to plot is the bar graph. The main function that you can use in R to draw them is the barplot() function.17 And to illustrate the use of the function, I’ll use the finalists variable that I introduced in Section @ref(mode). What I want to do is draw a bar graph that displays the number of finals that each team has played in over the time spanned by the afl data set. So, let’s start by creating a vector that contains this information. I’ll use the tabulate() function to do this (which will be discussed properly in Section @ref(freqtables), since it creates a simple numeric vector:\n\nfreq &lt;- tabulate( afl.finalists )\nprint( freq )\n\n [1] 26 25 26 28 32  0  6 39 27 28 28 17  6 24 26 38 24\n\n\nThis isn’t exactly the prettiest of frequency tables, of course. I’m only doing it this way so that you can see the barplot() function in it’s “purest” form: when the input is just an ordinary numeric vector. That being said, I’m obviously going to need the team names to create some labels, so let’s create a variable with those. I’ll do this using the levels() function, which outputs the names of all the levels of a factor (see Section @ref(factors):\n\nteams &lt;- levels( afl.finalists )\nprint( teams )\n\n [1] \"Adelaide\"         \"Brisbane\"         \"Carlton\"          \"Collingwood\"     \n [5] \"Essendon\"         \"Fitzroy\"          \"Fremantle\"        \"Geelong\"         \n [9] \"Hawthorn\"         \"Melbourne\"        \"North Melbourne\"  \"Port Adelaide\"   \n[13] \"Richmond\"         \"St Kilda\"         \"Sydney\"           \"West Coast\"      \n[17] \"Western Bulldogs\"\n\n\nOkay, so now that we have the information we need, let’s draw our bar graph. The main argument that you need to specify for a bar graph is the height of the bars, which in our case correspond to the values stored in the freq variable:\n\nbarplot( height = freq )  # specifying the argument name\nbarplot( freq )           # the lazier version\n\nEither of these two commands will produce the simple bar graph shown in Figure @ref(fig:bar1a).\n\n\n\n\n\nthe simplest version of a bargraph, containing the data but no labels\n\n\n\n\nAs you can see, R has drawn a pretty minimal plot. It doesn’t have any labels, obviously, because we didn’t actually tell the barplot() function what the labels are! To do this, we need to specify the names.arg argument. The names.arg argument needs to be a vector of character strings containing the text that needs to be used as the label for each of the items. In this case, the teams vector is exactly what we need, so the command we’re looking for is:\n\n    barplot( height = freq, names.arg = teams ) \n\n\n\n\nwe’ve added the labels, but because the text runs horizontally R only includes a few of them\n\n\n\n\nThis is an improvement, but not much of an improvement. R has only included a few of the labels, because it can’t fit them in the plot. This is the same behaviour we saw earlier with the multiple-boxplot graph in Figure @ref(fig:multipleboxplots). However, in Figure @ref(fig:multipleboxplots) it wasn’t an issue: it’s pretty obvious from inspection that the two unlabelled plots in between 1987 and 1990 must correspond to the data from 1988 and 1989. However, the fact that barplot() has omitted the names of every team in between Adelaide and Fitzroy is a lot more problematic.\nThe simplest way to fix this is to rotate the labels, so that the text runs vertically not horizontally. To do this, we need to alter set the las parameter, which I discussed briefly in Section @ref(introplotting). What I’ll do is tell R to rotate the text so that it’s always perpendicular to the axes (i.e., I’ll set las = 2). When I do that, as per the following command…\n\n    barplot(height = freq,  # the frequencies\n            names.arg = teams,  # the label\n            las = 2)            # rotate the labels\n\n\n\n\nwe’ve rotated the labels, but now the text is too long to fit\n\n\n\n\n… the result is the bar graph shown in Figure @ref(fig:bar1c). We’ve fixed the problem, but we’ve created a new one: the axis labels don’t quite fit anymore. To fix this, we have to be a bit cleverer again. A simple fix would be to use shorter names rather than the full name of all teams, and in many situations that’s probably the right thing to do. However, at other times you really do need to create a bit more space to add your labels, so I’ll show you how to do that.\n\n\nAltering the margins to the plot is actually a somewhat more complicated exercise than you might think. In principle it’s a very simple thing to do: the size of the margins is governed by a graphical parameter called mar, so all we need to do is alter this parameter. First, let’s look at what the mar argument specifies. The mar argument is a vector containing four numbers: specifying the amount of space at the bottom, the left, the top and then the right. The units are “number of ‘lines’”. The default value for mar is c(5.1, 4.1, 4.1, 2.1), meaning that R leaves 5.1 “lines” empty at the bottom, 4.1 lines on the left and the bottom, and only 2.1 lines on the right. In order to make more room at the bottom, what I need to do is change the first of these numbers. A value of 10.1 should do the trick.\nSo far this doesn’t seem any different to the other graphical parameters that we’ve talked about. However, because of the way that the traditional graphics system in R works, you need to specify what the margins will be before calling your high-level plotting function. Unlike the other cases we’ve see, you can’t treat mar as if it were just another argument in your plotting function. Instead, you have to use the par() function to change the graphical parameters beforehand, and only then try to draw your figure. In other words, the first thing I would do is this:\n\npar( mar = c( 10.1, 4.1, 4.1, 2.1) )\n\nThere’s no visible output here, but behind the scenes R has changed the graphical parameters associated with the current device (remember, in R terminology all graphics are drawn onto a “device”). Now that this is done, we could use the exact same command as before, but this time you’d see that the labels all fit, because R now leaves twice as much room for the labels at the bottom. However, since I’ve now figured out how to get the labels to display properly, I might as well play around with some of the other options, all of which are things you’ve seen before:\n\nbarplot( height = freq,\n        names.arg = teams,\n        las=2,\n        ylab = \"Number of Finals\",\n        main = \"Finals Played, 1987-2010\",  \n        density = 10,\n        angle = 20)\n\n\n\n\nwe fix this by expanding the margin at the bottom, and add several other customisations to make the chart a bit nicer\n\n\n\n\nHowever, one thing to remember about the par() function is that it doesn’t just change the graphical parameters for the current plot. Rather, the changes pertain to any subsequent plot that you draw onto the same device. This might be exactly what you want, in which case there’s no problem. But if not, you need to reset the graphical parameters to their original settings. To do this, you can either close the device (e.g., close the window, or click the “Clear All” button in the Plots panel in Rstudio) or you can reset the graphical parameters to their original values, using a command like this:\n\npar( mar = c(5.1, 4.1, 4.1, 2.1) )\n\n\n\n\n\nHold on, you might be thinking. What’s the good of being able to draw pretty pictures in R if I can’t save them and send them to friends to brag about how awesome my data is? How do I save the picture? This is another one of those situations where the easiest thing to do is to use the RStudio tools.\nIf you’re running R through Rstudio, then the easiest way to save your image is to click on the “Export” button in the Plot panel (i.e., the area in Rstudio where all the plots have been appearing). When you do that you’ll see a menu that contains the options “Save Plot as PDF” and “Save Plot as Image”. Either version works. Both will bring up dialog boxes that give you a few options that you can play with, but besides that it’s pretty simple.\nThis works pretty nicely for most situations. So, unless you’re filled with a burning desire to learn the low level details, feel free to skip the rest of this section.\n\n\nAs I say, the menu-based options should be good enough for most people most of the time. However, one day you might want to be a bit more sophisticated, and make use of R’s image writing capabilities at a lower level. In this section I’ll give you a very basic introduction to this. In all honesty, this barely scratches the surface, but it will help a little bit in getting you started if you want to learn the details.\nOkay, as I hinted earlier, whenever you’re drawing pictures in R you’re deemed to be drawing to a device of some kind. There are devices that correspond to a figure drawn on screen, and there are devices that correspond to graphics files that R will produce for you. For the purposes of this section I’ll assume that you’re using the default application in either Windows or Mac OS, not Rstudio. The reason for this is that my experience with the graphical device provided by Rstudio has led me to suspect that it still has a bunch on non-standard (or possibly just undocumented) features, and so I don’t quite trust that it always does what I expect. I’ve no doubt they’ll smooth it out later, but I can honestly say that I don’t quite get what’s going on with the RStudioGD device. In any case, we can ask R to list all of the graphics devices that currently exist, simply by using the command dev.list(). If there are no figure windows open, then you’ll see this:\n\ndev.list()\n# NULL\n\nwhich just means that R doesn’t have any graphics devices open. However, suppose if you’ve just drawn a histogram and you type the same command, R will now give you a different answer. For instance, if you’re using Windows:\n\nhist( afl.margins )\ndev.list()\n# windows \n#      2\n\nWhat this means is that there is one graphics device (device 2) that is currently open, and it’s a figure window. If you did the same thing on a Mac, you get basically the same answer, except that the name of the device would be quartz rather than windows. If you had several graphics windows open (which, incidentally, you can do by using the dev.new() command) then you’d see something like this:\n\ndev.list()\n# windows windows windows  \n#       2       3       4 \n\nOkay, so that’s the basic idea behind graphics devices. The key idea here is that graphics files (like JPEG images etc) are also graphics devices as far as R is concerned. So what you want to do is to copy the contents of one graphics device to another one. There’s a command called dev.copy() that does this, but what I’ll explain to you is a simpler one called dev.print(). It’s pretty simple:\n\ndev.print( device = jpeg,              # what are we printing to?\n            filename = \"thisfile.jpg\",  # name of the image file\n            width = 480,                # how many pixels wide should it be\n            height = 300                # how many pixels high should it be\n)\n\nThis takes the “active” figure window, copies it to a jpeg file (which R treats as a device) and then closes that device. The filename = \"thisfile.jpg\" part tells R what to name the graphics file, and the width = 480 and height = 300 arguments tell R to draw an image that is 300 pixels high and 480 pixels wide. If you want a different kind of file, just change the device argument from jpeg to something else. R has devices for png, tiff and bmp that all work in exactly the same way as the jpeg command, but produce different kinds of files. Actually, for simple cartoonish graphics like this histogram, you’d be better advised to use PNG or TIFF over JPEG. The JPEG format is very good for natural images, but is wasteful for simple line drawings. The information above probably covers most things you might want to. However, if you want more information about what kinds of options you can specify using R, have a look at the help documentation by typing ?jpeg or ?tiff or whatever.\n\n\n\n\nPerhaps I’m a simple minded person, but I love pictures. Every time I write a new scientific paper, one of the first things I do is sit down and think about what the pictures will be. In my head, an article is really just a sequence of pictures, linked together by a story. All the rest of it is just window dressing. What I’m really trying to say here is that the human visual system is a very powerful data analysis tool. Give it the right kind of information and it will supply a human reader with a massive amount of knowledge very quickly. Not for nothing do we have the saying “a picture is worth a thousand words”. With that in mind, I think that this is one of the most important chapters in the book. The topics covered were:\n\nBasic overview to R graphics. In Section @ref(rgraphics) we talked about how graphics in R are organised, and then moved on to the basics of how they’re drawn in Section @ref(introplotting).\nCommon plots. Much of the chapter was focused on standard graphs that statisticians like to produce: histograms (Section @ref(hist)), stem and leaf plots (Section @ref(stem)), boxplots (Section @ref(boxplots)), scatterplots (Section @ref(scatterplots)) and bar graphs (Section @ref(bargraph)).\nSaving image files. The last part of the chapter talked about how to export your pictures (Section @ref(saveimage))\n\nOne final thing to point out. At the start of the chapter I mentioned that R has several completely distinct systems for drawing figures. In this chapter I’ve focused on the traditional graphics system. It’s the easiest one to get started with: you can draw a histogram with a command as simple as hist(x). However, it’s not the most powerful tool for the job, and after a while most R users start looking to shift to fancier systems. One of the most popular graphics systems is provided by the ggplot2 package (see ), which is loosely based on “The grammar of graphics” [@Wilkinson2006]. It’s not for novices: you need to have a pretty good grasp of R before you can start using it, and even then it takes a while to really get the hang of it. But when you’re finally at that stage, it’s worth taking the time to teach yourself, because it’s a much cleaner system."
  },
  {
    "objectID": "materials/visuals.html#rgraphics",
    "href": "materials/visuals.html#rgraphics",
    "title": "Drawing graphs",
    "section": "",
    "text": "Reduced to its simplest form, you can think of an R graphic as being much like a painting. You start out with an empty canvas. Every time you use a graphics function, it paints some new things onto your canvas. Later on, you can paint more things over the top if you want; but just like painting, you can’t “undo” your strokes. If you make a mistake, you have to throw away your painting and start over. Fortunately, this is way more easy to do when using R than it is when painting a picture in real life: you delete the plot and then type a new set of commands.3 This way of thinking about drawing graphs is referred to as the painter’s model. So far, this probably doesn’t sound particularly complicated, and for the vast majority of graphs you’ll want to draw it’s exactly as simple as it sounds. Much like painting in real life, the headaches usually start when we dig into details. To see why, I’ll expand this “painting metaphor” a bit further just to show you the basics of what’s going on under the hood, but before I do I want to stress that you really don’t need to understand all these complexities in order to draw graphs. I’d been using R for years before I even realised that most of these issues existed! However, I don’t want you to go through the same pain I went through every time I inadvertently discovered one of these things, so here’s a quick overview.\nFirstly, if you want to paint a picture, you need to paint it on something. In real life, you can paint on lots of different things. Painting onto canvas isn’t the same as painting onto paper, and neither one is the same as painting on a wall. In R, the thing that you paint your graphic onto is called a device. For most applications that we’ll look at in this book, this “device” will be a window on your computer. If you’re using Windows as your operating system, then the name for this device is windows; on a Mac it’s called quartz because that’s the name of the software that the Mac OS uses to draw pretty pictures; and on Linux/Unix, you’re probably using X11. On the other hand, if you’re using Rstudio (regardless of which operating system you’re on), there’s a separate device called RStudioGD that forces R to paint inside the “plots” panel in Rstudio. However, from the computers perspective there’s nothing terribly special about drawing pictures on screen: and so R is quite happy to paint pictures directly into a file. R can paint several different types of image files: jpeg, png, pdf, postscript, tiff and bmp files are all among the options that you have available to you. For the most part, these different devices all behave the same way, so you don’t really need to know much about the differences between them when learning how to draw pictures. But, just like real life painting, sometimes the specifics do matter. Unless stated otherwise, you can assume that I’m drawing a picture on screen, using the appropriate device (i.e., windows, quartz, X11 or RStudioGD). One the rare occasions where these behave differently from one another, I’ll try to point it out in the text.\nSecondly, when you paint a picture you need to paint it with something. Maybe you want to do an oil painting, but maybe you want to use watercolour. And, generally speaking, you pretty much have to pick one or the other. The analog to this in R is a “graphics system”. A graphics system defines a collection of very low-level graphics commands about what to draw and where to draw it. Something that surprises most new R users is the discovery that R actually has two completely independent graphics systems, known as traditional graphics (in the graphics package) and grid graphics (in the grid package).4 Not surprisingly, the traditional graphics system is the older of the two: in fact, it’s actually older than R since it has it’s origins in S, the system from which R is descended. Grid graphics are newer, and in some respects more powerful, so many of the more recent, fancier graphical tools in R make use of grid graphics. However, grid graphics are somewhat more complicated beasts, so most people start out by learning the traditional graphics system. Nevertheless, as long as you don’t want to use any low-level commands yourself, then you don’t really need to care about whether you’re using traditional graphics or grid graphics. However, the moment you do want to tweak your figure by using some low-level commands you do need to care. Because these two different systems are pretty much incompatible with each other, there’s a pretty big divide in R graphics universe. Unless stated otherwise, you can assume that everything I’m saying pertains to traditional graphics.\nThirdly, a painting is usually done in a particular style. Maybe it’s a still life, maybe it’s an impressionist piece, or maybe you’re trying to annoy me by pretending that cubism is a legitimate artistic style. Regardless, each artistic style imposes some overarching aesthetic and perhaps even constraints on what can (or should) be painted using that style. In the same vein, R has quite a number of different packages, each of which provide a collection of high-level graphics commands. A single high-level command is capable of drawing an entire graph, complete with a range of customisation options. Most but not all of the high-level commands that I’ll talk about in this book come from the graphics package itself, and so belong to the world of traditional graphics. These commands all tend to share a common visual style, although there are a few graphics that I’ll use that come from other packages that differ in style somewhat. On the other side of the great divide, the grid universe relies heavily on two different packages – lattice and ggplots2 – each of which provides a quite different visual style. As you’ve probably guessed, there’s a whole separate bunch of functions that you’d need to learn if you want to use lattice graphics or make use of the ggplots2. However, for the purposes of this book I’ll restrict myself to talking about the basic graphics tools.\nAt this point, I think we’ve covered more than enough background material. The point that I’m trying to make by providing this discussion isn’t to scare you with all these horrible details, but rather to try to convey to you the fact that R doesn’t really provide a single coherent graphics system. Instead, R itself provides a platform, and different people have built different graphical tools using that platform. As a consequence of this fact, there’s two different universes of graphics, and a great multitude of packages that live in them. At this stage you don’t need to understand these complexities, but it’s useful to know that they’re there. But for now, I think we can be happy with a simpler view of things: we’ll draw pictures on screen using the traditional graphics system, and as much as possible we’ll stick to high level commands only.\nSo let’s start painting."
  },
  {
    "objectID": "materials/visuals.html#introplotting",
    "href": "materials/visuals.html#introplotting",
    "title": "Drawing graphs",
    "section": "",
    "text": "Before I discuss any specialised graphics, let’s start by drawing a few very simple graphs just to get a feel for what it’s like to draw pictures using R. To that end, let’s create a small vector Fibonacci that contains a few numbers we’d like R to draw for us. Then, we’ll ask R to plot() those numbers. The result is Figure @ref(fig:firstplot).\n\nFibonacci &lt;- c( 1,1,2,3,5,8,13 )\nplot( Fibonacci )\n\n\n\n\nOur first plot\n\n\n\n\nAs you can see, what R has done is plot the values stored in the Fibonacci variable on the vertical axis (y-axis) and the corresponding index on the horizontal axis (x-axis). In other words, since the 4th element of the vector has a value of 3, we get a dot plotted at the location (4,3). That’s pretty straightforward, and the image in Figure @ref(fig:firstplot) is probably pretty close to what you would have had in mind when I suggested that we plot the Fibonacci data. However, there’s quite a lot of customisation options available to you, so we should probably spend a bit of time looking at some of those options. So, be warned: this ends up being a fairly long section, because there’s so many possibilities open to you. Don’t let it overwhelm you though… while all of the options discussed here are handy to know about, you can get by just fine only knowing a few of them. The only reason I’ve included all this stuff right at the beginning is that it ends up making the rest of the chapter a lot more readable!\n\n\nBefore we go into any discussion of customising plots, we need a little more background. The important thing to note when using the plot() function, is that it’s another example of a generic function (Section @ref(generics), much like print() and summary(), and so its behaviour changes depending on what kind of input you give it. However, the plot() function is somewhat fancier than the other two, and its behaviour depends on two arguments, x (the first input, which is required) and y (which is optional). This makes it (a) extremely powerful once you get the hang of it, and (b) hilariously unpredictable, when you’re not sure what you’re doing. As much as possible, I’ll try to make clear what type of inputs produce what kinds of outputs. For now, however, it’s enough to note that I’m only doing very basic plotting, and as a consequence all of the work is being done by the plot.default() function.\nWhat kinds of customisations might we be interested in? If you look at the help documentation for the default plotting method (i.e., type ?plot.default or help(\"plot.default\")) you’ll see a very long list of arguments that you can specify to customise your plot. I’ll talk about several of them in a moment, but first I want to point out something that might seem quite wacky. When you look at all the different options that the help file talks about, you’ll notice that some of the options that it refers to are “proper” arguments to the plot.default() function, but it also goes on to mention a bunch of things that look like they’re supposed to be arguments, but they’re not listed in the “Usage” section of the file, and the documentation calls them graphical parameters instead. Even so, it’s usually possible to treat them as if they were arguments of the plotting function. Very odd. In order to stop my readers trying to find a brick and look up my home address, I’d better explain what’s going on; or at least give the basic gist behind it.\nWhat exactly is a graphical parameter? Basically, the idea is that there are some characteristics of a plot which are pretty universal: for instance, regardless of what kind of graph you’re drawing, you probably need to specify what colour to use for the plot, right? So you’d expect there to be something like a col argument to every single graphics function in R? Well, sort of. In order to avoid having hundreds of arguments for every single function, what R does is refer to a bunch of these “graphical parameters” which are pretty general purpose. Graphical parameters can be changed directly by using the low-level par() function, which I discuss briefly in Section @ref(par) though not in a lot of detail. If you look at the help files for graphical parameters (i.e., type ?par) you’ll see that there’s lots of them. Fortunately, (a) the default settings are generally pretty good so you can ignore the majority of the parameters, and (b) as you’ll see as we go through this chapter, you very rarely need to use par() directly, because you can “pretend” that graphical parameters are just additional arguments to your high-level function (e.g. plot.default()). In short… yes, R does have these wacky “graphical parameters” which can be quite confusing. But in most basic uses of the plotting functions, you can act as if they were just undocumented additional arguments to your function.\n\n\n\nOne of the first things that you’ll find yourself wanting to do when customising your plot is to label it better. You might want to specify more appropriate axis labels, add a title or add a subtitle. The arguments that you need to specify to make this happen are:\n\nmain. A character string containing the title.\nsub. A character string containing the subtitle.\nxlab. A character string containing the x-axis label.\nylab. A character string containing the y-axis label.\n\nThese aren’t graphical parameters, they’re arguments to the high-level function. However, because the high-level functions all rely on the same low-level function to do the drawing5 the names of these arguments are identical for pretty much every high-level function I’ve come across. Let’s have a look at what happens when we make use of all these arguments. Here’s the command. The picture that this draws is shown in Figure @ref(fig:secondplot).\n\nplot( x = Fibonacci,\n               main = \"You specify title using the 'main' argument\",\n               sub = \"The subtitle appears here! (Use the 'sub' argument for this)\",\n               xlab = \"The x-axis label is 'xlab'\",\n                ylab = \"The y-axis label is 'ylab'\" \n            )\n\n\n\n\nHow to add your own title, subtitle, x-axis label and y-axis label to the plot.\n\n\n\n\nIt’s more or less as you’d expect. The plot itself is identical to the one we drew in Figure @ref(fig:firstplot), except for the fact that we’ve changed the axis labels, and added a title and a subtitle. Even so, there’s a couple of interesting features worth calling your attention to. Firstly, notice that the subtitle is drawn below the plot, which I personally find annoying; as a consequence I almost never use subtitles. You may have a different opinion, of course, but the important thing is that you remember where the subtitle actually goes. Secondly, notice that R has decided to use boldface text and a larger font size for the title. This is one of my most hated default settings in R graphics, since I feel that it draws too much attention to the title. Generally, while I do want my reader to look at the title, I find that the R defaults are a bit overpowering, so I often like to change the settings. To that end, there are a bunch of graphical parameters that you can use to customise the font style:\n\nFont styles: font.main, font.sub, font.lab, font.axis. These four parameters control the font style used for the plot title (font.main), the subtitle (font.sub), the axis labels (font.lab: note that you can’t specify separate styles for the x-axis and y-axis without using low level commands), and the numbers next to the tick marks on the axis (font.axis). Somewhat irritatingly, these arguments are numbers instead of meaningful names: a value of 1 corresponds to plain text, 2 means boldface, 3 means italic and 4 means bold italic.\nFont colours: col.main, col.sub, col.lab, col.axis. These parameters do pretty much what the name says: each one specifies a colour in which to type each of the different bits of text. Conveniently, R has a very large number of named colours (type colours() to see a list of over 650 colour names that R knows), so you can use the English language name of the colour to select it.6 Thus, the parameter value here string like \"red\", \"gray25\" or \"springgreen4\" (yes, R really does recognise four different shades of “spring green”).\nFont size: cex.main, cex.sub, cex.lab, cex.axis. Font size is handled in a slightly curious way in R. The “cex” part here is short for “character expansion”, and it’s essentially a magnification value. By default, all of these are set to a value of 1, except for the font title: cex.main has a default magnification of 1.2, which is why the title font is 20% bigger than the others.\nFont family: family. This argument specifies a font family to use: the simplest way to use it is to set it to \"sans\", \"serif\", or \"mono\", corresponding to a san serif font, a serif font, or a monospaced font. If you want to, you can give the name of a specific font, but keep in mind that different operating systems use different fonts, so it’s probably safest to keep it simple. Better yet, unless you have some deep objections to the R defaults, just ignore this parameter entirely. That’s what I usually do.\n\nTo give you a sense of how you can use these parameters to customise your titles, the following command can be used to draw Figure @ref(fig:thirdplot):\n\nplot( x = Fibonacci,                           # the data to plot\n          main = \"The first 7 Fibonacci numbers\",  # the title\n          xlab = \"Position in the sequence\",       # x-axis label\n          ylab = \"The Fibonacci number\",           # y-axis \n          font.main = 1,\n          cex.main = 1,\n          font.axis = 2,\n          col.lab = \"gray50\" )\n\n\n\n\nHow to customise the appearance of the titles and labels.\n\n\n\n\nAlthough this command is quite long, it’s not complicated: all it does is override a bunch of the default parameter values. The only difficult aspect to this is that you have to remember what each of these parameters is called, and what all the different values are. And in practice I never remember: I have to look up the help documentation every time, or else look it up in this book.\n\n\n\nAdding and customising the titles associated with the plot is one way in which you can play around with what your picture looks like. Another thing that you’ll want to do is customise the appearance of the actual plot! To start with, let’s look at the single most important options that the plot() function (or, recalling that we’re dealing with a generic function, in this case the plot.default() function, since that’s the one doing all the work) provides for you to use, which is the type argument. The type argument specifies the visual style of the plot. The possible values for this are:\n\ntype = \"p\". Draw the points only.\ntype = \"l\". Draw a line through the points.\ntype = \"o\". Draw the line over the top of the points.\ntype = \"b\". Draw both points and lines, but don’t overplot.\ntype = \"h\". Draw “histogram-like” vertical bars.\ntype = \"s\". Draw a staircase, going horizontally then vertically.\ntype = \"S\". Draw a Staircase, going vertically then horizontally.\ntype = \"c\". Draw only the connecting lines from the “b” version.\ntype = \"n\". Draw nothing. (Apparently this is useful sometimes?)\n\nThe simplest way to illustrate what each of these really looks like is just to draw them. To that end, Figure @ref(fig:simpleplots) shows the same Fibonacci data, drawn using six different types of plot. As you can see, by altering the type argument you can get a qualitatively different appearance to your plot. In other words, as far as R is concerned, the only difference between a scatterplot (like the ones we drew in Section @ref(correl) and a line plot is that you draw a scatterplot by setting type = \"p\" and you draw a line plot by setting type = \"l\". However, that doesn’t imply that you should think of them as begin equivalent to each other. As you can see by looking at Figure @ref(fig:simpleplots), a line plot implies that there is some notion of continuity from one point to the next, whereas a scatterplot does not.\n\n\n\n\n\nChanging the type of the plot.\n\n\n\n\n\n\n\nIn Section @ref(figtitles) we talked about a group of graphical parameters that are related to the formatting of titles, axis labels etc. The second group of parameters I want to discuss are those related to the formatting of the plot itself:\n\nColour of the plot: col. As we saw with the previous colour-related parameters, the simplest way to specify this parameter is using a character string: e.g., col = \"blue\". It’s a pretty straightforward parameter to specify: the only real subtlety is that every high-level function tends to draw a different “thing” as it’s output, and so this parameter gets interpreted a little differently by different functions. However, for the plot.default() function it’s pretty simple: the col argument refers to the colour of the points and/or lines that get drawn!\nCharacter used to plot points: pch. The plot character parameter is a number, usually between 1 and 25. What it does is tell R what symbol to use to draw the points that it plots. The simplest way to illustrate what the different values do is with a picture. Figure @ref(fig:pch) shows the first 25 plotting characters. The default plotting character is a hollow circle (i.e., pch = 1).\n\n\n\n\n\n\nChanging the plotted characters\n\n\n\n\n\nPlot size: cex. This parameter describes a character expansion factor (i.e., magnification) for the plotted characters. By default cex=1, but if you want bigger symbols in your graph you should specify a larger value.\nLine type: lty. The line type parameter describes the kind of line that R draws. It has seven values which you can specify using a number between 0 and 7, or using a meaningful character string: \"blank\", \"solid\", \"dashed\", \"dotted\", \"dotdash\", \"longdash\", or \"twodash\". Note that the “blank” version (value 0) just means that R doesn’t draw the lines at all. The other six versions are shown in Figure @ref(fig:lty).\n\n\n\n\n\n\nLine types\n\n\n\n\n\nLine width: lwd. The last graphical parameter in this category that I want to mention is the line width parameter, which is just a number specifying the width of the line. The default value is 1. Not surprisingly, larger values produce thicker lines and smaller values produce thinner lines. Try playing around with different values of lwd to see what happens.\n\nTo illustrate what you can do by altering these parameters, let’s try the following command, the output is shown in Figure @ref(fig:fifthplot).\n\nplot( x = Fibonacci,\n         type = \"b\",\n         col = \"blue\",\n         pch = 19,\n         cex=5,\n         lty=2,\n         lwd=4)\n\n\n\n\nCustomising various aspects to the plot itself.\n\n\n\n\n\n\n\nThere are several other possibilities worth discussing. Ignoring graphical parameters for the moment, there’s a few other arguments to the plot.default() function that you might want to use. As before, many of these are standard arguments that are used by a lot of high level graphics functions:\n\nChanging the axis scales: xlim, ylim. Generally R does a pretty good job of figuring out where to set the edges of the plot. However, you can override its choices by setting the xlim and ylim arguments. For instance, if I decide I want the vertical scale of the plot to run from 0 to 100, then I’d set ylim = c(0, 100).\nSuppress labelling: ann. This is a logical-valued argument that you can use if you don’t want R to include any text for a title, subtitle or axis label. To do so, set ann = FALSE. This will stop R from including any text that would normally appear in those places. Note that this will override any of your manual titles. For example, if you try to add a title using the main argument, but you also specify ann = FALSE, no title will appear.\nSuppress axis drawing: axes. Again, this is a logical valued argument. Suppose you don’t want R to draw any axes at all. To suppress the axes, all you have to do is add axes = FALSE. This will remove the axes and the numbering, but not the axis labels (i.e. the xlab and ylab text). Note that you can get finer grain control over this by specifying the xaxt and yaxt graphical parameters instead (see below).\nInclude a framing box: frame.plot. Suppose you’ve removed the axes by setting axes = FALSE, but you still want to have a simple box drawn around the plot; that is, you only wanted to get rid of the numbering and the tick marks, but you want to keep the box. To do that, you set frame.plot = TRUE.\n\nNote that this list isn’t exhaustive. There are a few other arguments to the plot.default function that you can play with if you want to, but those are the ones you are probably most likely to want to use. As always, however, if these aren’t enough options for you, there’s also a number of other graphical parameters that you might want to play with as well. That’s the focus of the next section. In the meantime, here’s a command that makes use of all these different options. The output is shown in Figure @ref(fig:fourthplot), and it’s pretty much exactly as you’d expect. The axis scales on both the horizontal and vertical dimensions have been expanded, the axes have been suppressed as have the annotations, but I’ve kept a box around the plot.\n\nplot( x = Fibonacci,       # the data\n       xlim = c(0, 15),     # expand the x-scale\n       ylim = c(0, 15),     # expand the y-scale\n       ann = FALSE,         # delete all annotations\n       axes = FALSE,        # delete the axes\n       frame.plot = TRUE    # but include a framing box\n )\n\n\n\n\nAltering the scale and appearance of the plot axes.\n\n\n\n\nBefore moving on, I should point out that there are several graphical parameters relating to the axes, the box, and the general appearance of the plot which allow finer grain control over the appearance of the axes and the annotations.\n\nSuppressing the axes individually: xaxt, yaxt. These graphical parameters are basically just fancier versions of the axes argument we discussed earlier. If you want to stop R from drawing the vertical axis but you’d like it to keep the horizontal axis, set yaxt = \"n\". I trust that you can figure out how to keep the vertical axis and suppress the horizontal one!\nBox type: bty. In the same way that xaxt, yaxt are just fancy versions of axes, the box type parameter is really just a fancier version of the frame.plot argument, allowing you to specify exactly which out of the four borders you want to keep. The way we specify this parameter is a bit stupid, in my opinion: the possible values are \"o\" (the default), \"l\", \"7\", \"c\", \"u\", or \"]\", each of which will draw only those edges that the corresponding character suggests. That is, the letter \"c\" has a top, a bottom and a left, but is blank on the right hand side, whereas \"7\" has a top and a right, but is blank on the left and the bottom. Alternatively a value of \"n\" means that no box will be drawn.\nOrientation of the axis labels las. I presume that the name of this parameter is an acronym of label style or something along those lines; but what it actually does is govern the orientation of the text used to label the individual tick marks (i.e., the numbering, not the xlab and ylab axis labels). There are four possible values for las: A value of 0 means that the labels of both axes are printed parallel to the axis itself (the default). A value of 1 means that the text is always horizontal. A value of 2 means that the labelling text is printed at right angles to the axis. Finally, a value of 3 means that the text is always vertical.\n\nAgain, these aren’t the only possibilities. There are a few other graphical parameters that I haven’t mentioned that you could use to customise the appearance of the axes,7 but that’s probably enough (or more than enough) for now. To give a sense of how you could use these parameters, let’s try the following command. The output is shown in Figure @ref(fig:sixthplot). As you can see, this isn’t a very useful plot at all. However, it does illustrate the graphical parameters we’re talking about, so I suppose it serves its purpose.\n\n    plot( x = Fibonacci, # the data\n         xaxt = \"n\",       # don't draw the x-axis  \n         bty = \"]\",        # keep bottom, right and top of box only\n         las = 1 )         # rotate the text\n\n\n\n\nOther ways to customise the axes\n\n\n\n\n\n\n\nAt this point, a lot of readers will be probably be thinking something along the lines of, “if there’s this much detail just for drawing a simple plot, how horrible is it going to get when we start looking at more complicated things?” Perhaps, contrary to my earlier pleas for mercy, you’ve found a brick to hurl and are right now leafing through an Adelaide phone book trying to find my address. Well, fear not! And please, put the brick down. In a lot of ways, we’ve gone through the hardest part: we’ve already covered vast majority of the plot customisations that you might want to do. As you’ll see, each of the other high level plotting commands we’ll talk about will only have a smallish number of additional options. Better yet, even though I’ve told you about a billion different ways of tweaking your plot, you don’t usually need them. So in practice, now that you’ve read over it once to get the gist, the majority of the content of this section is stuff you can safely forget: just remember to come back to this section later on when you want to tweak your plot."
  },
  {
    "objectID": "materials/visuals.html#hist",
    "href": "materials/visuals.html#hist",
    "title": "Drawing graphs",
    "section": "",
    "text": "Now that we’ve tamed (or possibly fled from) the beast that is R graphical parameters, let’s talk more seriously about some real life graphics that you’ll want to draw. We begin with the humble histogram. Histograms are one of the simplest and most useful ways of visualising data. They make most sense when you have an interval or ratio scale (e.g., the afl.margins data from Chapter @ref(descriptives) and what you want to do is get an overall impression of the data. Most of you probably know how histograms work, since they’re so widely used, but for the sake of completeness I’ll describe them. All you do is divide up the possible values into bins, and then count the number of observations that fall within each bin. This count is referred to as the frequency of the bin, and is displayed as a bar: in the AFL winning margins data, there are 33 games in which the winning margin was less than 10 points, and it is this fact that is represented by the height of the leftmost bar in Figure @ref(fig:hist1a). Drawing this histogram in R is pretty straightforward. The function you need to use is called hist(), and it has pretty reasonable default settings. In fact, Figure @ref(fig:hist1a) is exactly what you get if you just type this:\n\nhist( afl.margins )\n\n\n\n\n\n\nThe default histogram that R produces\n\n\n\n\nAlthough this image would need a lot of cleaning up in order to make a good presentation graphic (i.e., one you’d include in a report), it nevertheless does a pretty good job of describing the data. In fact, the big strength of a histogram is that (properly used) it does show the entire spread of the data, so you can get a pretty good sense about what it looks like. The downside to histograms is that they aren’t very compact: unlike some of the other plots I’ll talk about it’s hard to cram 20-30 histograms into a single image without overwhelming the viewer. And of course, if your data are nominal scale (e.g., the afl.finalists data) then histograms are useless.\nThe main subtlety that you need to be aware of when drawing histograms is determining where the breaks that separate bins should be located, and (relatedly) how many breaks there should be. In Figure @ref(fig:hist1a), you can see that R has made pretty sensible choices all by itself: the breaks are located at 0, 10, 20, … 120, which is exactly what I would have done had I been forced to make a choice myself. On the other hand, consider the two histograms in Figure @ref(fig:hist1b) and @ref(fig:hist1c), which I produced using the following two commands:\n\nhist( x = afl.margins, breaks = 3 )\n\n\n\n\nA histogram with too few bins\n\n\n\n\n\nhist( x = afl.margins, breaks = 0:116 )\n\n\n\n\nA histogram with too many bins\n\n\n\n\nIn Figure @ref(fig:hist1c), the bins are only 1 point wide. As a result, although the plot is very informative (it displays the entire data set with no loss of information at all!) the plot is very hard to interpret, and feels quite cluttered. On the other hand, the plot in Figure @ref(fig:hist1b) has a bin width of 50 points, and has the opposite problem: it’s very easy to “read” this plot, but it doesn’t convey a lot of information. One gets the sense that this histogram is hiding too much. In short, the way in which you specify the breaks has a big effect on what the histogram looks like, so it’s important to make sure you choose the breaks sensibly. In general R does a pretty good job of selecting the breaks on its own, since it makes use of some quite clever tricks that statisticians have devised for automatically selecting the right bins for a histogram, but nevertheless it’s usually a good idea to play around with the breaks a bit to see what happens.\nThere is one fairly important thing to add regarding how the breaks argument works. There are two different ways you can specify the breaks. You can either specify how many breaks you want (which is what I did for panel b when I typed breaks = 3) and let R figure out where they should go, or you can provide a vector that tells R exactly where the breaks should be placed (which is what I did for panel c when I typed breaks = 0:116). The behaviour of the hist() function is slightly different depending on which version you use. If all you do is tell it how many breaks you want, R treats it as a “suggestion” not as a demand. It assumes you want “approximately 3” breaks, but if it doesn’t think that this would look very pretty on screen, it picks a different (but similar) number. It does this for a sensible reason – it tries to make sure that the breaks are located at sensible values (like 10) rather than stupid ones (like 7.224414). And most of the time R is right: usually, when a human researcher says “give me 3 breaks”, he or she really does mean “give me approximately 3 breaks, and don’t put them in stupid places”. However, sometimes R is dead wrong. Sometimes you really do mean “exactly 3 breaks”, and you know precisely where you want them to go. So you need to invoke “real person privilege”, and order R to do what it’s bloody well told. In order to do that, you have to input the full vector that tells R exactly where you want the breaks. If you do that, R will go back to behaving like the nice little obedient calculator that it’s supposed to be.\n\n\nOkay, so at this point we can draw a basic histogram, and we can alter the number and even the location of the breaks. However, the visual style of the histograms shown in Figures @ref(fig:hist1a), @ref(fig:hist1b), and @ref(fig:hist1c) could stand to be improved. We can fix this by making use of some of the other arguments to the hist() function. Most of the things you might want to try doing have already been covered in Section @ref(introplotting), but there’s a few new things:\n\nShading lines: density, angle. You can add diagonal lines to shade the bars: the density value is a number indicating how many lines per inch R should draw (the default value of NULL means no lines), and the angle is a number indicating how many degrees from horizontal the lines should be drawn at (default is angle = 45 degrees).\nSpecifics regarding colours: col, border. You can also change the colours: in this instance the col parameter sets the colour of the shading (either the shading lines if there are any, or else the colour of the interior of the bars if there are not), and the border argument sets the colour of the edges of the bars.\nLabelling the bars: labels. You can also attach labels to each of the bars using the labels argument. The simplest way to do this is to set labels = TRUE, in which case R will add a number just above each bar, that number being the exact number of observations in the bin. Alternatively, you can choose the labels yourself, by inputting a vector of strings, e.g., labels = c(\"label 1\",\"label 2\",\"etc\")\n\nNot surprisingly, this doesn’t exhaust the possibilities. If you type help(\"hist\") or ?hist and have a look at the help documentation for histograms, you’ll see a few more options. A histogram that makes use of the histogram-specific customisations as well as several of the options we discussed in Section @ref(introplotting) is shown in Figure @ref(fig:hist1d). The R command that I used to draw it is this:\n\nhist( x = afl.margins, \n      main = \"2010 AFL margins\", # title of the plot\n      xlab = \"Margin\",           # set the x-axis label\n      density = 10,              # draw shading lines: 10 per inch\n      angle = 40,                # set the angle of the shading lines is 40 degrees\n      border = \"gray20\",         # set the colour of the borders of the bars\n      col = \"gray80\",            # set the colour of the shading lines\n      labels = TRUE,             # add frequency labels to each bar\n      ylim = c(0,40)             # change the scale of the y-axis\n)\n\n\n\n\nA histogram with histogram specific customisations\n\n\n\n\nOverall, this is a much nicer histogram than the default ones."
  },
  {
    "objectID": "materials/visuals.html#stem",
    "href": "materials/visuals.html#stem",
    "title": "Drawing graphs",
    "section": "",
    "text": "Histograms are one of the most widely used methods for displaying the observed values for a variable. They’re simple, pretty, and very informative. However, they do take a little bit of effort to draw. Sometimes it can be quite useful to make use of simpler, if less visually appealing, options. One such alternative is the stem and leaf plot. To a first approximation you can think of a stem and leaf plot as a kind of text-based histogram. Stem and leaf plots aren’t used as widely these days as they were 30 years ago, since it’s now just as easy to draw a histogram as it is to draw a stem and leaf plot. Not only that, they don’t work very well for larger data sets. As a consequence you probably won’t have as much of a need to use them yourself, though you may run into them in older publications. These days, the only real world situation where I use them is if I have a small data set with 20-30 data points and I don’t have a computer handy, because it’s pretty easy to quickly sketch a stem and leaf plot by hand.\nWith all that as background, lets have a look at stem and leaf plots. The AFL margins data contains 176 observations, which is at the upper end for what you can realistically plot this way. The function in R for drawing stem and leaf plots is called stem() and if we ask for a stem and leaf plot of the afl.margins data, here’s what we get:\n\nstem( afl.margins )\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n   0 | 001111223333333344567788888999999\n   1 | 0000011122234456666899999\n   2 | 00011222333445566667788999999\n   3 | 01223555566666678888899\n   4 | 012334444477788899\n   5 | 00002233445556667\n   6 | 0113455678\n   7 | 01123556\n   8 | 122349\n   9 | 458\n  10 | 148\n  11 | 6\n\n\nThe values to the left of the | are called stems and the values to the right are called leaves. If you just look at the shape that the leaves make, you can see something that looks a lot like a histogram made out of numbers, just rotated by 90 degrees. But if you know how to read the plot, there’s quite a lot of additional information here. In fact, it’s also giving you the actual values of all of the observations in the data set. To illustrate, let’s have a look at the last line in the stem and leaf plot, namely 11 | 6. Specifically, let’s compare this to the largest values of the afl.margins data set:\n\nmax( afl.margins )\n\n[1] 116\n\n\nHm… 11 | 6 versus 116. Obviously the stem and leaf plot is trying to tell us that the largest value in the data set is 116. Similarly, when we look at the line that reads 10 | 148, the way we interpret it to note that the stem and leaf plot is telling us that the data set contains observations with values 101, 104 and 108. Finally, when we see something like 5 | 00002233445556667 the four 0s in the the stem and leaf plot are telling us that there are four observations with value 50.\nI won’t talk about them in a lot of detail, but I should point out that some customisation options are available for stem and leaf plots in R. The two arguments that you can use to do this are:\n\nscale. Changing the scale of the plot (default value is 1), which is analogous to changing the number of breaks in a histogram. Reducing the scale causes R to reduce the number of stem values (i.e., the number of breaks, if this were a histogram) that the plot uses.\nwidth. The second way that to can customise a stem and leaf plot is to alter the width (default value is 80). Changing the width alters the maximum number of leaf values that can be displayed for any given stem.\n\nHowever, since stem and leaf plots aren’t as important as they used to be, I’ll leave it to the interested reader to investigate these options. Try the following two commands to see what happens:\n\nstem( x = afl.margins, scale = .25 )\nstem( x = afl.margins, width = 20 )\n\nThe only other thing to note about stem and leaf plots is the line in which R tells you where the decimal point is. If our data set had included only the numbers .11, .15, .23, .35 and .59 and we’d drawn a stem and leaf plot of these data, then R would move the decimal point: the stem values would be 1,2,3,4 and 5, but R would tell you that the decimal point has moved to the left of the | symbol. If you want to see this in action, try the following command:\n\nstem( x = afl.margins / 1000 )\n\nThe stem and leaf plot itself will look identical to the original one we drew, except for the fact that R will tell you that the decimal point has moved."
  },
  {
    "objectID": "materials/visuals.html#boxplots",
    "href": "materials/visuals.html#boxplots",
    "title": "Drawing graphs",
    "section": "",
    "text": "Another alternative to histograms is a boxplot, sometimes called a “box and whiskers” plot. Like histograms, they’re most suited to interval or ratio scale data. The idea behind a boxplot is to provide a simple visual depiction of the median, the interquartile range, and the range of the data. And because they do so in a fairly compact way, boxplots have become a very popular statistical graphic, especially during the exploratory stage of data analysis when you’re trying to understand the data yourself. Let’s have a look at how they work, again using the afl.margins data as our example. Firstly, let’s actually calculate these numbers ourselves using the summary() function:8\n\nsummary( afl.margins )\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00   12.75   30.50   35.30   50.50  116.00 \n\n\nSo how does a boxplot capture these numbers? The easiest way to describe what a boxplot looks like is just to draw one. The function for doing this in R is (surprise, surprise) boxplot(). As always there’s a lot of optional arguments that you can specify if you want, but for the most part you can just let R choose the defaults for you. That said, I’m going to override one of the defaults to start with by specifying the range option, but for the most part you won’t want to do this (I’ll explain why in a minute). With that as preamble, let’s try the following command:\n\nboxplot( x = afl.margins, range = 100 )\n\n\n\n\nA basic boxplot\n\n\n\n\nWhat R draws is shown in Figure @ref(fig:boxplot1a), the most basic boxplot possible. When you look at this plot, this is how you should interpret it: the thick line in the middle of the box is the median; the box itself spans the range from the 25th percentile to the 75th percentile; and the “whiskers” cover the full range from the minimum value to the maximum value. This is summarised in the annotated plot in Figure @ref(fig:boxplot1b).\n\n\n\n\n\nAn annotated boxplot\n\n\n\n\nIn practice, this isn’t quite how boxplots usually work. In most applications, the “whiskers” don’t cover the full range from minimum to maximum. Instead, they actually go out to the most extreme data point that doesn’t exceed a certain bound. By default, this value is 1.5 times the interquartile range, corresponding to a range value of 1.5. Any observation whose value falls outside this range is plotted as a circle instead of being covered by the whiskers, and is commonly referred to as an outlier. For our AFL margins data, there is one observation (a game with a margin of 116 points) that falls outside this range. As a consequence, the upper whisker is pulled back to the next largest observation (a value of 108), and the observation at 116 is plotted as a circle. This is illustrated in Figure @ref(fig:boxplot2a). Since the default value is range = 1.5 we can draw this plot using the simple command\n\nboxplot( afl.margins )\n\n\n\n\nBy default, R will only extent the whiskers a distance of 1.5 times the interquartile range, and will plot any points that fall outside that range separately\n\n\n\n\n\n\nI’ll talk a little more about the relationship between boxplots and outliers in the Section @ref(boxplotoutliers), but before I do let’s take the time to clean this figure up. Boxplots in R are extremely customisable. In addition to the usual range of graphical parameters that you can tweak to make the plot look nice, you can also exercise nearly complete control over every element to the plot. Consider the boxplot in Figure @ref(fig:boxplot2b): in this version of the plot, not only have I added labels (xlab, ylab) and removed the stupid border (frame.plot), I’ve also dimmed all of the graphical elements of the boxplot except the central bar that plots the median (border) so as to draw more attention to the median rather than the rest of the boxplot.\n\n\n\n\n\nA boxplot with boxplot specific customisations\n\n\n\n\nYou’ve seen all these options in previous sections in this chapter, so hopefully those customisations won’t need any further explanation. However, I’ve done two new things as well: I’ve deleted the cross-bars at the top and bottom of the whiskers (known as the “staples” of the plot), and converted the whiskers themselves to solid lines. The arguments that I used to do this are called by the ridiculous names of staplewex and whisklty,9 and I’ll explain these in a moment.\nBut first, here’s the actual command I used to draw this figure:\n\nboxplot( x = afl.margins,           # the data\n          xlab = \"AFL games, 2010\",  # x-axis label\n          ylab = \"Winning Margin\",   # y-axis label\n          border = \"grey50\",         # dim the border of the box\n          frame.plot = FALSE,        # don't draw a frame\n          staplewex = 0,             # don't draw staples\n          whisklty = 1               # solid line for whisker \n )\n\nOverall, I think the resulting boxplot is a huge improvement in visual design over the default version. In my opinion at least, there’s a fairly minimalist aesthetic that governs good statistical graphics. Ideally, every visual element that you add to a plot should convey part of the message. If your plot includes things that don’t actually help the reader learn anything new, you should consider removing them. Personally, I can’t see the point of the cross-bars on a standard boxplot, so I’ve deleted them.\nOkay, what commands can we use to customise the boxplot? If you type ?boxplot and flick through the help documentation, you’ll notice that it does mention staplewex as an argument, but there’s no mention of whisklty. The reason for this is that the function that handles the drawing is called bxp(), so if you type ?bxp all the gory details appear. Here’s the short summary. In order to understand why these arguments have such stupid names, you need to recognise that they’re put together from two components. The first part of the argument name specifies one part of the box plot: staple refers to the staples of the plot (i.e., the cross-bars), and whisk refers to the whiskers. The second part of the name specifies a graphical parameter: wex is a width parameter, and lty is a line type parameter. The parts of the plot you can customise are:\n\nbox. The box that covers the interquartile range.\nmed. The line used to show the median.\nwhisk. The vertical lines used to draw the whiskers.\nstaple. The cross bars at the ends of the whiskers.\nout. The points used to show the outliers.\n\nThe actual graphical parameters that you might want to specify are slightly different for each visual element, just because they’re different shapes from each other. As a consequence, the following options are available:\n\nWidth expansion: boxwex, staplewex, outwex. These are scaling factors that govern the width of various parts of the plot. The default scaling factor is (usually) 0.8 for the box, and 0.5 for the other two. Note that in the case of the outliers this parameter is meaningless unless you decide to draw lines plotting the outliers rather than use points.\nLine type: boxlty, medlty, whisklty, staplelty, outlty. These govern the line type for the relevant elements. The values for this are exactly the same as those used for the regular lty parameter, with two exceptions. There’s an additional option where you can set medlty = \"blank\" to suppress the median line completely (useful if you want to draw a point for the median rather than plot a line). Similarly, by default the outlier line type is set to outlty = \"blank\", because the default behaviour is to draw outliers as points instead of lines.\nLine width: boxlwd, medlwd, whisklwd, staplelwd, outlwd. These govern the line widths for the relevant elements, and behave the same way as the regular lwd parameter. The only thing to note is that the default value for medlwd value is three times the value of the others.\nLine colour: boxcol, medcol, whiskcol, staplecol, outcol. These govern the colour of the lines used to draw the relevant elements. Specify a colour in the same way that you usually do.\nFill colour: boxfill. What colour should we use to fill the box?\nPoint character: medpch, outpch. These behave like the regular pch parameter used to select the plot character. Note that you can set outpch = NA to stop R from plotting the outliers at all, and you can also set medpch = NA to stop it from drawing a character for the median (this is the default!)\nPoint expansion: medcex, outcex. Size parameters for the points used to plot medians and outliers. These are only meaningful if the corresponding points are actually plotted. So for the default boxplot, which includes outlier points but uses a line rather than a point to draw the median, only the outcex parameter is meaningful.\nBackground colours: medbg, outbg. Again, the background colours are only meaningful if the points are actually plotted.\n\nTaken as a group, these parameters allow you almost complete freedom to select the graphical style for your boxplot that you feel is most appropriate to the data set you’re trying to describe. That said, when you’re first starting out there’s no shame in using the default settings! But if you want to master the art of designing beautiful figures, it helps to try playing around with these parameters to see what works and what doesn’t. Finally, I should mention a few other arguments that you might want to make use of:\n\nhorizontal. Set this to TRUE to display the plot horizontally rather than vertically.\nvarwidth. Set this to TRUE to get R to scale the width of each box so that the areas are proportional to the number of observations that contribute to the boxplot. This is only useful if you’re drawing multiple boxplots at once (see Section @ref(multipleboxplots).\nshow.names. Set this to TRUE to get R to attach labels to the boxplots.\nnotch. If you set notch = TRUE, R will draw little notches in the sides of each box. If the notches of two boxplots don’t overlap, then there is a “statistically significant” difference between the corresponding medians. If you haven’t read Chapter @ref(hypothesistesting), ignore this argument – we haven’t discussed statistical significance, so this doesn’t mean much to you. I’m mentioning it only because you might want to come back to the topic later on. (see also the notch.frac option when you type ?bxp).\n\n\n\n\nBecause the boxplot automatically (unless you change the range argument) separates out those observations that lie within a certain range, people often use them as an informal method for detecting outliers: observations that are “suspiciously” distant from the rest of the data. Here’s an example. Suppose that I’d drawn the boxplot for the AFL margins data, and it came up looking like Figure @ref(fig:boxplotoutlier).\n\n\n\n\n\nA boxplot showing one very suspicious outlier! I’ve drawn this plot in a similar, minimalist style to the one in Figure @ref(fig:boxplot2b), but I’ve used the horizontal argument to draw it sideways in order to save space.\n\n\n\n\nIt’s pretty clear that something funny is going on with one of the observations. Apparently, there was one game in which the margin was over 300 points! That doesn’t sound right to me. Now that I’ve become suspicious, it’s time to look a bit more closely at the data. One function that can be handy for this is the which() function; it takes as input a vector of logicals, and outputs the indices of the TRUE cases. This is particularly useful in the current context because it lets me do this:\n\nsuspicious.cases &lt;- afl.margins &gt; 300\nwhich( suspicious.cases )\n\n[1] 137\n\n\nalthough in real life I probably wouldn’t bother creating the suspicious.cases variable: I’d just cut out the middle man and use a command like which( afl.margins &gt; 300 ). In any case, what this has done is shown me that the outlier corresponds to game 137. Then, I find the recorded margin for that game:\n\nafl.margins[137]\n\n[1] 333\n\n\nHm. That definitely doesn’t sound right. So then I go back to the original data source (the internet!) and I discover that the actual margin of that game was 33 points. Now it’s pretty clear what happened. Someone must have typed in the wrong number. Easily fixed, just by typing afl.margins[137] &lt;- 33. While this might seem like a silly example, I should stress that this kind of thing actually happens a lot. Real world data sets are often riddled with stupid errors, especially when someone had to type something into a computer at some point. In fact, there’s actually a name for this phase of data analysis, since in practice it can waste a huge chunk of our time: data cleaning. It involves searching for typos, missing data and all sorts of other obnoxious errors in raw data files.10\nWhat about the real data? Does the value of 116 constitute a funny observation not? Possibly. As it turns out the game in question was Fremantle v Hawthorn, and was played in round 21 (the second last home and away round of the season). Fremantle had already qualified for the final series and for them the outcome of the game was irrelevant; and the team decided to rest several of their star players. As a consequence, Fremantle went into the game severely underpowered. In contrast, Hawthorn had started the season very poorly but had ended on a massive winning streak, and for them a win could secure a place in the finals. With the game played on Hawthorn’s home turf11 and with so many unusual factors at play, it is perhaps no surprise that Hawthorn annihilated Fremantle by 116 points. Two weeks later, however, the two teams met again in an elimination final on Fremantle’s home ground, and Fremantle won comfortably by 30 points.12\nSo, should we exclude the game from subsequent analyses? If this were a psychology experiment rather than an AFL season, I’d be quite tempted to exclude it because there’s pretty strong evidence that Fremantle weren’t really trying very hard: and to the extent that my research question is based on an assumption that participants are genuinely trying to do the task. On the other hand, in a lot of studies we’re actually interested in seeing the full range of possible behaviour, and that includes situations where people decide not to try very hard: so excluding that observation would be a bad idea. In the context of the AFL data, a similar distinction applies. If I’d been trying to make tips about who would perform well in the finals, I would have (and in fact did) disregard the Round 21 massacre, because it’s way too misleading. On the other hand, if my interest is solely in the home and away season itself, I think it would be a shame to throw away information pertaining to one of the most distinctive (if boring) games of the year. In other words, the decision about whether to include outliers or exclude them depends heavily on why you think the data look they way they do, and what you want to use the data for. Statistical tools can provide an automatic method for suggesting candidates for deletion, but you really need to exercise good judgment here. As I’ve said before, R is a mindless automaton. It doesn’t watch the footy, so it lacks the broader context to make an informed decision. You are not a mindless automaton, so you should exercise judgment: if the outlier looks legitimate to you, then keep it. In any case, I’ll return to the topic again in Section @ref(regressiondiagnostics), so let’s return to our discussion of how to draw boxplots.\n\n\n\nOne last thing. What if you want to draw multiple boxplots at once? Suppose, for instance, I wanted separate boxplots showing the AFL margins not just for 2010, but for every year between 1987 and 2010. To do that, the first thing we’ll have to do is find the data. These are stored in the aflsmall2.Rdata file. So let’s load it and take a quick peek at what’s inside:\n\nload( \"\"materials/data/aflsmall2.Rdata\"\" )\nwho( TRUE )\n#   -- Name --   -- Class --   -- Size --\n#   afl2         data.frame    4296 x 2  \n#    $margin     numeric       4296      \n#    $year       numeric       4296     \n\nNotice that afl2 data frame is pretty big. It contains 4296 games, which is far more than I want to see printed out on my computer screen. To that end, R provides you with a few useful functions to print out only a few of the row in the data frame. The first of these is head() which prints out the first 6 rows, of the data frame, like this:\n\nhead( afl2 )\n\n  margin year\n1     33 1987\n2     59 1987\n3     45 1987\n4     91 1987\n5     39 1987\n6      1 1987\n\n\nYou can also use the tail() function to print out the last 6 rows. The car package also provides a handy little function called some() which prints out a random subset of the rows.\nIn any case, the important thing is that we have the afl2 data frame which contains the variables that we’re interested in. What we want to do is have R draw boxplots for the margin variable, plotted separately for each separate year. The way to do this using the boxplot() function is to input a formula rather than a variable as the input. In this case, the formula we want is margin ~ year. So our boxplot command now looks like this. The result is shown in Figure @ref(fig:multipleboxplots).13\n\nboxplot( formula = margin ~ year,\n         data = afl2\n)\n\n\n\n\nBoxplots showing the AFL winning margins for the 24 years from 1987 to 2010 inclusive. This is the default plot created by R, with no annotations added and no changes to the visual design. It’s pretty readable, though at a minimum you’d want to include some basic annotations labelling the axes. Compare and contrast with Figure @ref(fig:multipleboxplots2)\n\n\n\n\nEven this, the default version of the plot, gives a sense of why it’s sometimes useful to choose boxplots instead of histograms. Even before taking the time to turn this basic output into something more readable, it’s possible to get a good sense of what the data look like from year to year without getting overwhelmed with too much detail. Now imagine what would have happened if I’d tried to cram 24 histograms into this space: no chance at all that the reader is going to learn anything useful.\nThat being said, the default boxplot leaves a great deal to be desired in terms of visual clarity. The outliers are too visually prominent, the dotted lines look messy, and the interesting content (i.e., the behaviour of the median and the interquartile range across years) gets a little obscured. Fortunately, this is easy to fix, since we’ve already covered a lot of tools you can use to customise your output. After playing around with several different versions of the plot, the one I settled on is shown in Figure @ref(fig:multipleboxplots2). The command I used to produce it is long, but not complicated:\n\nboxplot( formula =  margin ~ year,   # the formula\n           data = afl2,                # the data set\n           xlab = \"AFL season\",        # x axis label\n           ylab = \"Winning Margin\",    # y axis label\n           frame.plot = FALSE,         # don't draw a frame\n           staplewex = 0,              # don't draw staples\n           staplecol = \"white\",        # (fixes a tiny display issue)\n           boxwex = .75,               # narrow the boxes slightly\n           boxfill = \"grey80\",         # lightly shade the boxes\n           whisklty = 1,               # solid line for whiskers \n           whiskcol = \"grey70\",        # dim the whiskers\n           boxcol = \"grey70\",          # dim the box borders\n           outcol = \"grey70\",          # dim the outliers\n           outpch = 20,                # outliers as solid dots\n           outcex = .5,                # shrink the outliers\n           medlty = \"blank\",           # no line for the medians\n           medpch = 20,                # instead, draw solid dots\n           medlwd = 1.5                # make them larger\n )\n\n\n\n\nA cleaned up version of Figure @ref(fig:multipleboxplots). Notice that I’ve used a very minimalist design for the boxplots, so as to focus the eye on the medians. I’ve also converted the medians to solid dots, to convey a sense that year to year variation in the median should be thought of as a single coherent plot (similar to what we did when plotting the Fibonacci variable earlier). The size of outliers has been shrunk, because they aren’t actually very interesting. In contrast, I’ve added a fill colour to the boxes, to make it easier to look at the trend in the interquartile range across years.\n\n\n\n\nOf course, given that the command is that long, you might have guessed that I didn’t spend ages typing all that rubbish in over and over again. Instead, I wrote a script, which I kept tweaking until it produced the figure that I wanted. We’ll talk about scripts later in Section @ref(scripts), but given the length of the command I thought I’d remind you that there’s an easier way of trying out different commands than typing them all in over and over."
  },
  {
    "objectID": "materials/visuals.html#scatterplots",
    "href": "materials/visuals.html#scatterplots",
    "title": "Drawing graphs",
    "section": "",
    "text": "Scatterplots are a simple but effective tool for visualising data. We’ve already seen scatterplots in this chapter, when using the plot() function to draw the Fibonacci variable as a collection of dots (Section @ref(introplotting). However, for the purposes of this section I have a slightly different notion in mind. Instead of just plotting one variable, what I want to do with my scatterplot is display the relationship between two variables, like we saw with the figures in the section on correlation (Section @ref(correl). It’s this latter application that we usually have in mind when we use the term “scatterplot”. In this kind of plot, each observation corresponds to one dot: the horizontal location of the dot plots the value of the observation on one variable, and the vertical location displays its value on the other variable. In many situations you don’t really have a clear opinions about what the causal relationship is (e.g., does A cause B, or does B cause A, or does some other variable C control both A and B). If that’s the case, it doesn’t really matter which variable you plot on the x-axis and which one you plot on the y-axis. However, in many situations you do have a pretty strong idea which variable you think is most likely to be causal, or at least you have some suspicions in that direction. If so, then it’s conventional to plot the cause variable on the x-axis, and the effect variable on the y-axis. With that in mind, let’s look at how to draw scatterplots in R, using the same parenthood data set (i.e. parenthood.Rdata) that I used when introducing the idea of correlations.\nSuppose my goal is to draw a scatterplot displaying the relationship between the amount of sleep that I get (dan.sleep) and how grumpy I am the next day (dan.grump). As you might expect given our earlier use of plot() to display the Fibonacci data, the function that we use is the plot() function, but because it’s a generic function all the hard work is still being done by the plot.default() function. In any case, there are two different ways in which we can get the plot that we’re after. The first way is to specify the name of the variable to be plotted on the x axis and the variable to be plotted on the y axis. When we do it this way, the command looks like this:\n\nplot( x = parenthood$dan.sleep,   # data on the x-axis\n      y = parenthood$dan.grump    # data on the y-axis\n )  \n\n\n\n\nthe default scatterplot that R produces\n\n\n\n\nThe second way do to it is to use a “formula and data frame” format, but I’m going to avoid using it.14 For now, let’s just stick with the x and y version. If we do this, the result is the very basic scatterplot shown in Figure @ref(fig:scattera). This serves fairly well, but there’s a few customisations that we probably want to make in order to have this work properly. As usual, we want to add some labels, but there’s a few other things we might want to do as well. Firstly, it’s sometimes useful to rescale the plots. In Figure @ref(fig:scattera) R has selected the scales so that the data fall neatly in the middle. But, in this case, we happen to know that the grumpiness measure falls on a scale from 0 to 100, and the hours slept falls on a natural scale between 0 hours and about 12 or so hours (the longest I can sleep in real life). So the command I might use to draw this is:\n\nplot( x = parenthood$dan.sleep,          # data on the x-axis\n       y = parenthood$dan.grump,         # data on the y-axis\n       xlab = \"My sleep (hours)\",        # x-axis label\n       ylab = \"My grumpiness (0-100)\",   # y-axis label\n       xlim = c(0,12),                   # scale the x-axis\n       ylim = c(0,100),                  # scale the y-axis\n       pch = 20,                         # change the plot type\n       col = \"gray50\",                   # dim the dots slightly\n       frame.plot = FALSE                # don't draw a box\n )\n\nThis command produces the scatterplot in Figure @ref(fig:scatterb), or at least very nearly. What it doesn’t do is draw the line through the middle of the points. Sometimes it can be very useful to do this, and I can do so using lines(), which is a low level plotting function. Better yet, the arguments that I need to specify are pretty much the exact same ones that I use when calling the plot() function. That is, suppose that I want to draw a line that goes from the point (4,93) to the point (9.5,37). Then the x locations can be specified by the vector c(4,9.5) and the y locations correspond to the vector c(93,37). In other words, I use this command:\n\nplot( x = parenthood$dan.sleep,          # data on the x-axis\n       y = parenthood$dan.grump,         # data on the y-axis\n       xlab = \"My sleep (hours)\",        # x-axis label\n       ylab = \"My grumpiness (0-100)\",   # y-axis label\n       xlim = c(0,12),                   # scale the x-axis\n       ylim = c(0,100),                  # scale the y-axis\n       pch = 20,                         # change the plot type\n       col = \"gray50\",                   # dim the dots slightly\n       frame.plot = FALSE                # don't draw a box\n)\n lines( x = c(4,9.5),   # the horizontal locations\n        y = c(93,37),   # the vertical locations\n        lwd = 2         # line width\n)\n\n\n\n\nA scatterplot with scatter plot specific customisations\n\n\n\n\nAnd when I do so, R plots the line over the top of the plot that I drew using the previous command. In most realistic data analysis situations you absolutely don’t want to just guess where the line through the points goes, since there’s about a billion different ways in which you can get R to do a better job. However, it does at least illustrate the basic idea.\nOne possibility, if you do want to get R to draw nice clean lines through the data for you, is to use the scatterplot() function in the car package. Before we can use scatterplot() we need to load the package:\n\nlibrary( car )\n\nLoading required package: carData\n\n\nHaving done so, we can now use the function. The command we need is this one:\n\nscatterplot( dan.grump ~ dan.sleep,\n              data = parenthood, \n              smooth = FALSE\n)\n\n\n\n\n\n\nA fancy scatterplot drawn using the scatterplot() function in the car package.\n\n\n\n\nThe first two arguments should be familiar: the first input is a formula dan.grump ~ dan.sleep telling R what variables to plot,15 and the second specifies a data frame. The third argument smooth I’ve set to FALSE to stop the scatterplot() function from drawing a fancy “smoothed” trendline (since it’s a bit confusing to beginners). The scatterplot itself is shown in Figure @ref(fig:fancyscatter). As you can see, it’s not only drawn the scatterplot, but its also drawn boxplots for each of the two variables, as well as a simple line of best fit showing the relationship between the two variables.\n\n\nOften you find yourself wanting to look at the relationships between several variables at once. One useful tool for doing so is to produce a scatterplot matrix, analogous to the correlation matrix.\n\ncor( x = parenthood ) # calculate correlation matrix\n\n             dan.sleep  baby.sleep   dan.grump         day\ndan.sleep   1.00000000  0.62794934 -0.90338404 -0.09840768\nbaby.sleep  0.62794934  1.00000000 -0.56596373 -0.01043394\ndan.grump  -0.90338404 -0.56596373  1.00000000  0.07647926\nday        -0.09840768 -0.01043394  0.07647926  1.00000000\n\n\nWe can get a the corresponding scatterplot matrix by using the pairs() function:16\n\npairs( x = parenthood ) # draw corresponding scatterplot matrix  \n\n\n\n\nA scatterplot matrix from the `pairs() function\n\n\n\n\nThe output of the pairs() command is shown in Figure @ref(fig:pairs). An alternative way of calling the pairs() function, which can be useful in some situations, is to specify the variables to include using a one-sided formula. For instance, this\n\npairs( formula = ~ dan.sleep + baby.sleep + dan.grump,\n        data = parenthood\n)\n\nwould produce a \\(3 \\times 3\\) scatterplot matrix that only compare dan.sleep, dan.grump and baby.sleep. Obviously, the first version is much easier, but there are cases where you really only want to look at a few of the variables, so it’s nice to use the formula interface."
  },
  {
    "objectID": "materials/visuals.html#bargraph",
    "href": "materials/visuals.html#bargraph",
    "title": "Drawing graphs",
    "section": "",
    "text": "Another form of graph that you often want to plot is the bar graph. The main function that you can use in R to draw them is the barplot() function.17 And to illustrate the use of the function, I’ll use the finalists variable that I introduced in Section @ref(mode). What I want to do is draw a bar graph that displays the number of finals that each team has played in over the time spanned by the afl data set. So, let’s start by creating a vector that contains this information. I’ll use the tabulate() function to do this (which will be discussed properly in Section @ref(freqtables), since it creates a simple numeric vector:\n\nfreq &lt;- tabulate( afl.finalists )\nprint( freq )\n\n [1] 26 25 26 28 32  0  6 39 27 28 28 17  6 24 26 38 24\n\n\nThis isn’t exactly the prettiest of frequency tables, of course. I’m only doing it this way so that you can see the barplot() function in it’s “purest” form: when the input is just an ordinary numeric vector. That being said, I’m obviously going to need the team names to create some labels, so let’s create a variable with those. I’ll do this using the levels() function, which outputs the names of all the levels of a factor (see Section @ref(factors):\n\nteams &lt;- levels( afl.finalists )\nprint( teams )\n\n [1] \"Adelaide\"         \"Brisbane\"         \"Carlton\"          \"Collingwood\"     \n [5] \"Essendon\"         \"Fitzroy\"          \"Fremantle\"        \"Geelong\"         \n [9] \"Hawthorn\"         \"Melbourne\"        \"North Melbourne\"  \"Port Adelaide\"   \n[13] \"Richmond\"         \"St Kilda\"         \"Sydney\"           \"West Coast\"      \n[17] \"Western Bulldogs\"\n\n\nOkay, so now that we have the information we need, let’s draw our bar graph. The main argument that you need to specify for a bar graph is the height of the bars, which in our case correspond to the values stored in the freq variable:\n\nbarplot( height = freq )  # specifying the argument name\nbarplot( freq )           # the lazier version\n\nEither of these two commands will produce the simple bar graph shown in Figure @ref(fig:bar1a).\n\n\n\n\n\nthe simplest version of a bargraph, containing the data but no labels\n\n\n\n\nAs you can see, R has drawn a pretty minimal plot. It doesn’t have any labels, obviously, because we didn’t actually tell the barplot() function what the labels are! To do this, we need to specify the names.arg argument. The names.arg argument needs to be a vector of character strings containing the text that needs to be used as the label for each of the items. In this case, the teams vector is exactly what we need, so the command we’re looking for is:\n\n    barplot( height = freq, names.arg = teams ) \n\n\n\n\nwe’ve added the labels, but because the text runs horizontally R only includes a few of them\n\n\n\n\nThis is an improvement, but not much of an improvement. R has only included a few of the labels, because it can’t fit them in the plot. This is the same behaviour we saw earlier with the multiple-boxplot graph in Figure @ref(fig:multipleboxplots). However, in Figure @ref(fig:multipleboxplots) it wasn’t an issue: it’s pretty obvious from inspection that the two unlabelled plots in between 1987 and 1990 must correspond to the data from 1988 and 1989. However, the fact that barplot() has omitted the names of every team in between Adelaide and Fitzroy is a lot more problematic.\nThe simplest way to fix this is to rotate the labels, so that the text runs vertically not horizontally. To do this, we need to alter set the las parameter, which I discussed briefly in Section @ref(introplotting). What I’ll do is tell R to rotate the text so that it’s always perpendicular to the axes (i.e., I’ll set las = 2). When I do that, as per the following command…\n\n    barplot(height = freq,  # the frequencies\n            names.arg = teams,  # the label\n            las = 2)            # rotate the labels\n\n\n\n\nwe’ve rotated the labels, but now the text is too long to fit\n\n\n\n\n… the result is the bar graph shown in Figure @ref(fig:bar1c). We’ve fixed the problem, but we’ve created a new one: the axis labels don’t quite fit anymore. To fix this, we have to be a bit cleverer again. A simple fix would be to use shorter names rather than the full name of all teams, and in many situations that’s probably the right thing to do. However, at other times you really do need to create a bit more space to add your labels, so I’ll show you how to do that.\n\n\nAltering the margins to the plot is actually a somewhat more complicated exercise than you might think. In principle it’s a very simple thing to do: the size of the margins is governed by a graphical parameter called mar, so all we need to do is alter this parameter. First, let’s look at what the mar argument specifies. The mar argument is a vector containing four numbers: specifying the amount of space at the bottom, the left, the top and then the right. The units are “number of ‘lines’”. The default value for mar is c(5.1, 4.1, 4.1, 2.1), meaning that R leaves 5.1 “lines” empty at the bottom, 4.1 lines on the left and the bottom, and only 2.1 lines on the right. In order to make more room at the bottom, what I need to do is change the first of these numbers. A value of 10.1 should do the trick.\nSo far this doesn’t seem any different to the other graphical parameters that we’ve talked about. However, because of the way that the traditional graphics system in R works, you need to specify what the margins will be before calling your high-level plotting function. Unlike the other cases we’ve see, you can’t treat mar as if it were just another argument in your plotting function. Instead, you have to use the par() function to change the graphical parameters beforehand, and only then try to draw your figure. In other words, the first thing I would do is this:\n\npar( mar = c( 10.1, 4.1, 4.1, 2.1) )\n\nThere’s no visible output here, but behind the scenes R has changed the graphical parameters associated with the current device (remember, in R terminology all graphics are drawn onto a “device”). Now that this is done, we could use the exact same command as before, but this time you’d see that the labels all fit, because R now leaves twice as much room for the labels at the bottom. However, since I’ve now figured out how to get the labels to display properly, I might as well play around with some of the other options, all of which are things you’ve seen before:\n\nbarplot( height = freq,\n        names.arg = teams,\n        las=2,\n        ylab = \"Number of Finals\",\n        main = \"Finals Played, 1987-2010\",  \n        density = 10,\n        angle = 20)\n\n\n\n\nwe fix this by expanding the margin at the bottom, and add several other customisations to make the chart a bit nicer\n\n\n\n\nHowever, one thing to remember about the par() function is that it doesn’t just change the graphical parameters for the current plot. Rather, the changes pertain to any subsequent plot that you draw onto the same device. This might be exactly what you want, in which case there’s no problem. But if not, you need to reset the graphical parameters to their original settings. To do this, you can either close the device (e.g., close the window, or click the “Clear All” button in the Plots panel in Rstudio) or you can reset the graphical parameters to their original values, using a command like this:\n\npar( mar = c(5.1, 4.1, 4.1, 2.1) )"
  },
  {
    "objectID": "materials/visuals.html#saveimage",
    "href": "materials/visuals.html#saveimage",
    "title": "Drawing graphs",
    "section": "",
    "text": "Hold on, you might be thinking. What’s the good of being able to draw pretty pictures in R if I can’t save them and send them to friends to brag about how awesome my data is? How do I save the picture? This is another one of those situations where the easiest thing to do is to use the RStudio tools.\nIf you’re running R through Rstudio, then the easiest way to save your image is to click on the “Export” button in the Plot panel (i.e., the area in Rstudio where all the plots have been appearing). When you do that you’ll see a menu that contains the options “Save Plot as PDF” and “Save Plot as Image”. Either version works. Both will bring up dialog boxes that give you a few options that you can play with, but besides that it’s pretty simple.\nThis works pretty nicely for most situations. So, unless you’re filled with a burning desire to learn the low level details, feel free to skip the rest of this section.\n\n\nAs I say, the menu-based options should be good enough for most people most of the time. However, one day you might want to be a bit more sophisticated, and make use of R’s image writing capabilities at a lower level. In this section I’ll give you a very basic introduction to this. In all honesty, this barely scratches the surface, but it will help a little bit in getting you started if you want to learn the details.\nOkay, as I hinted earlier, whenever you’re drawing pictures in R you’re deemed to be drawing to a device of some kind. There are devices that correspond to a figure drawn on screen, and there are devices that correspond to graphics files that R will produce for you. For the purposes of this section I’ll assume that you’re using the default application in either Windows or Mac OS, not Rstudio. The reason for this is that my experience with the graphical device provided by Rstudio has led me to suspect that it still has a bunch on non-standard (or possibly just undocumented) features, and so I don’t quite trust that it always does what I expect. I’ve no doubt they’ll smooth it out later, but I can honestly say that I don’t quite get what’s going on with the RStudioGD device. In any case, we can ask R to list all of the graphics devices that currently exist, simply by using the command dev.list(). If there are no figure windows open, then you’ll see this:\n\ndev.list()\n# NULL\n\nwhich just means that R doesn’t have any graphics devices open. However, suppose if you’ve just drawn a histogram and you type the same command, R will now give you a different answer. For instance, if you’re using Windows:\n\nhist( afl.margins )\ndev.list()\n# windows \n#      2\n\nWhat this means is that there is one graphics device (device 2) that is currently open, and it’s a figure window. If you did the same thing on a Mac, you get basically the same answer, except that the name of the device would be quartz rather than windows. If you had several graphics windows open (which, incidentally, you can do by using the dev.new() command) then you’d see something like this:\n\ndev.list()\n# windows windows windows  \n#       2       3       4 \n\nOkay, so that’s the basic idea behind graphics devices. The key idea here is that graphics files (like JPEG images etc) are also graphics devices as far as R is concerned. So what you want to do is to copy the contents of one graphics device to another one. There’s a command called dev.copy() that does this, but what I’ll explain to you is a simpler one called dev.print(). It’s pretty simple:\n\ndev.print( device = jpeg,              # what are we printing to?\n            filename = \"thisfile.jpg\",  # name of the image file\n            width = 480,                # how many pixels wide should it be\n            height = 300                # how many pixels high should it be\n)\n\nThis takes the “active” figure window, copies it to a jpeg file (which R treats as a device) and then closes that device. The filename = \"thisfile.jpg\" part tells R what to name the graphics file, and the width = 480 and height = 300 arguments tell R to draw an image that is 300 pixels high and 480 pixels wide. If you want a different kind of file, just change the device argument from jpeg to something else. R has devices for png, tiff and bmp that all work in exactly the same way as the jpeg command, but produce different kinds of files. Actually, for simple cartoonish graphics like this histogram, you’d be better advised to use PNG or TIFF over JPEG. The JPEG format is very good for natural images, but is wasteful for simple line drawings. The information above probably covers most things you might want to. However, if you want more information about what kinds of options you can specify using R, have a look at the help documentation by typing ?jpeg or ?tiff or whatever."
  },
  {
    "objectID": "materials/visuals.html#summary",
    "href": "materials/visuals.html#summary",
    "title": "Drawing graphs",
    "section": "",
    "text": "Perhaps I’m a simple minded person, but I love pictures. Every time I write a new scientific paper, one of the first things I do is sit down and think about what the pictures will be. In my head, an article is really just a sequence of pictures, linked together by a story. All the rest of it is just window dressing. What I’m really trying to say here is that the human visual system is a very powerful data analysis tool. Give it the right kind of information and it will supply a human reader with a massive amount of knowledge very quickly. Not for nothing do we have the saying “a picture is worth a thousand words”. With that in mind, I think that this is one of the most important chapters in the book. The topics covered were:\n\nBasic overview to R graphics. In Section @ref(rgraphics) we talked about how graphics in R are organised, and then moved on to the basics of how they’re drawn in Section @ref(introplotting).\nCommon plots. Much of the chapter was focused on standard graphs that statisticians like to produce: histograms (Section @ref(hist)), stem and leaf plots (Section @ref(stem)), boxplots (Section @ref(boxplots)), scatterplots (Section @ref(scatterplots)) and bar graphs (Section @ref(bargraph)).\nSaving image files. The last part of the chapter talked about how to export your pictures (Section @ref(saveimage))\n\nOne final thing to point out. At the start of the chapter I mentioned that R has several completely distinct systems for drawing figures. In this chapter I’ve focused on the traditional graphics system. It’s the easiest one to get started with: you can draw a histogram with a command as simple as hist(x). However, it’s not the most powerful tool for the job, and after a while most R users start looking to shift to fancier systems. One of the most popular graphics systems is provided by the ggplot2 package (see ), which is loosely based on “The grammar of graphics” [@Wilkinson2006]. It’s not for novices: you need to have a pretty good grasp of R before you can start using it, and even then it takes a while to really get the hang of it. But when you’re finally at that stage, it’s worth taking the time to teach yourself, because it’s a much cleaner system."
  },
  {
    "objectID": "materials/visuals.html#footnotes",
    "href": "materials/visuals.html#footnotes",
    "title": "Drawing graphs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe origin of this quote is Tufte’s lovely book The Visual Display of Quantitative Information.↩︎\nI should add that this isn’t unique to R. Like everything in R there’s a pretty steep learning curve to learning how to draw graphs, and like always there’s a massive payoff at the end in terms of the quality of what you can produce. But to be honest, I’ve seen the same problems show up regardless of what system people use. I suspect that the hardest thing to do is to force yourself to take the time to think deeply about what your graphs are doing. I say that in full knowledge that only about half of my graphs turn out as well as they ought to. Understanding what makes a good graph is easy: actually designing a good graph is hard.↩︎\nOr, since you can always use the up and down keys to scroll through your recent command history, you can just pull up your most recent commands and edit them to fix your mistake. It becomes even easier once you start using scripts (Section @ref(scripts), since all you have to do is edit your script and then run it again.↩︎\nOf course, even that is a slightly misleading description, since some R graphics tools make use of external graphical rendering systems like OpenGL (e.g., the rgl package). I absolutely will not be talking about OpenGL or the like in this book, but as it happens there is one graph in this book that relies on them: Figure @ref(fig:multipleregression).↩︎\nThe low-level function that does this is called title() in case you ever need to know, and you can type ?title to find out a bit more detail about what these arguments do.↩︎\nOn the off chance that this isn’t enough freedom for you, you can select a colour directly as a “red, green, blue” specification using the rgb() function, or as a “hue, saturation, value” specification using the hsv() function.↩︎\nAlso, there’s a low level function called axis() that allows a lot more control over the appearance of the axes.↩︎\nR being what it is, it’s no great surprise that there’s also a fivenum() function that does much the same thing.↩︎\nI realise there’s a kind of logic to the way R names are constructed, but they still sound dumb. When I typed this sentence, all I could think was that it sounded like the name of a kids movie if it had been written by Lewis Carroll: “The frabjous gambolles of Staplewex and Whisklty” or something along those lines.↩︎\nSometimes it’s convenient to have the boxplot automatically label the outliers for you. The original boxplot() function doesn’t allow you to do this; however, the Boxplot() function in the car package does. The design of the Boxplot() function is very similar to boxplot(). It just adds a few new arguments that allow you to tweak the labelling scheme. I’ll leave it to the reader to check this out.↩︎\nSort of. The game was played in Launceston, which is a de facto home away from home for Hawthorn.↩︎\nContrast this situation with the next largest winning margin in the data set, which was Geelong’s 108 point demolition of Richmond in round 6 at their home ground, Kardinia Park. Geelong have been one of the most dominant teams over the last several years, a period during which they strung together an incredible 29-game winning streak at Kardinia Park. Richmond have been useless for several years. This is in no meaningful sense an outlier. Geelong have been winning by these margins (and Richmond losing by them) for quite some time. Frankly I’m surprised that the result wasn’t more lopsided: as happened to Melbourne in 2011 when Geelong won by a modest 186 points.↩︎\nActually, there’s other ways to do this. If the input argument x is a list object (see Section @ref(lists), the boxplot() function will draw a separate boxplot for each variable in that list. Relatedly, since the plot() function – which we’ll discuss shortly – is a generic (see Section @ref(generics), you might not be surprised to learn that one of its special cases is a boxplot: specifically, if you use plot() where the first argument x is a factor and the second argument y is numeric, then the result will be a boxplot, showing the values in y, with a separate boxplot for each level. For instance, something like plot(x = afl2\\$year, y = afl2\\$margin) would work. ↩︎\nThe reason is that there’s an annoying design flaw in the way the plot() function handles this situation. The problem is that the plot.formula() function uses different names to for the arguments than the plot() function expects. As a consequence, you can’t specify the formula argument by name. If you just specify a formula as the first argument without using the name it works fine, because the plot() function thinks the formula corresponds to the x argument, and the plot.formula() function thinks it corresponds to the formula argument; and surprisingly, everything works nicely. But the moment that you, the user, tries to be unambiguous about the name, one of those two functions is going to cry.↩︎\nYou might be wondering why I haven’t specified the argument name for the formula. The reason is that there’s a bug in how the scatterplot() function is written: under the hood there’s one function that expects the argument to be named x and another one that expects it to be called formula. I don’t know why the function was written this way, but it’s not an isolated problem: this particular kind of bug repeats itself in a couple of other functions (you’ll see it again in Chapter @ref(ttest). The solution in such cases is to omit the argument name: that way, one function “thinks” that you’ve specified x and the other one “thinks” you’ve specified formula and everything works the way it’s supposed to. It’s not a great state of affairs, I’ll admit, but it sort of works.↩︎\nYet again, we could have produced this output using the plot() function: when the x argument is a data frame containing numeric variables only, then the output is a scatterplot matrix. So, once again, what I could have done is just type plot( parenthood ).↩︎\nOnce again, it’s worth noting the link to the generic plot() function. If the x argument to plot() is a factor (and no y argument is given), the result is a bar graph. So you could use plot( afl.finalists ) and get the same output as barplot( afl.finalists ).↩︎"
  },
  {
    "objectID": "materials/slides_3b.html#a-guide-to-your-process",
    "href": "materials/slides_3b.html#a-guide-to-your-process",
    "title": "Intro to Data Visualisation II",
    "section": "A Guide to Your Process",
    "text": "A Guide to Your Process\nScheduling\nLearning Objectives\nPractice\nSupporting Information\nClass Discussion"
  },
  {
    "objectID": "materials/slides_3b.html#todays-plan",
    "href": "materials/slides_3b.html#todays-plan",
    "title": "Intro to Data Visualisation II",
    "section": "Today’s Plan",
    "text": "Today’s Plan\n\nIntroductions\nData Science Background\nWhy R?\nProblem Solving Tips (for Coding)\nComputer File Paths\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "materials/slides_3b.html#todays-learning-objectives",
    "href": "materials/slides_3b.html#todays-learning-objectives",
    "title": "Intro to Data Visualisation II",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\nAfter today’s session you will be able to:\n\n\nDefine “data science”\nExplain why the course is taught in R\nIdentify useful code problem solving techniques\nDemonstrate comprehension of computer file paths"
  },
  {
    "objectID": "materials/slides_3b.html#about-me",
    "href": "materials/slides_3b.html#about-me",
    "title": "Intro to Data Visualisation II",
    "section": "About Me",
    "text": "About Me\n\n\nGordon or Doctor Wright (he / him)\n\n\n\n\nEducation:\n\nB.Sc. Biology + M.Sc. Ecology & Evolutionary Biology\n\n\n\n\n\nCareer Goals:\n\nMarine Biologist Arrow Right College Faculty Arrow Right ??? Arrow Right Data Scientist\n\n\n\n\n\nHobbies:\n\nDungeons & Dragons; Reading; Hiking; Movies; Videogames"
  },
  {
    "objectID": "materials/slides_3b.html#introductions",
    "href": "materials/slides_3b.html#introductions",
    "title": "Intro to Data Visualisation II",
    "section": "Introductions",
    "text": "Introductions\nTell me a bit about yourselves!\n\n\nWhat is your preferred name?\nWhat year are you in school?\nWhat’s one of your hobbies that brings you joy?"
  },
  {
    "objectID": "materials/slides_3b.html#introductions-continued",
    "href": "materials/slides_3b.html#introductions-continued",
    "title": "Intro to Data Visualisation II",
    "section": "Introductions (Continued)",
    "text": "Introductions (Continued)\n\nWhy did you sign up for the course?\nWhat skill(s) are you most excited to learn?\nWhat previous coding / data science experience do you have?\n\nAbsolutely fine if this is your first foray into data science!"
  },
  {
    "objectID": "materials/slides_3b.html#my-goal-for-you",
    "href": "materials/slides_3b.html#my-goal-for-you",
    "title": "Intro to Data Visualisation II",
    "section": "My Goal for You",
    "text": "My Goal for You"
  },
  {
    "objectID": "materials/slides_3b.html#data-science-definition",
    "href": "materials/slides_3b.html#data-science-definition",
    "title": "Intro to Data Visualisation II",
    "section": "Data Science Definition",
    "text": "Data Science Definition\n\n\n\nData science combines programming and statistics with subject matter expertise to identify patterns and insights hidden in data"
  },
  {
    "objectID": "materials/slides_3b.html#why-is-this-course-taught-in-r",
    "href": "materials/slides_3b.html#why-is-this-course-taught-in-r",
    "title": "Intro to Data Visualisation II",
    "section": "Why is this Course Taught in R?",
    "text": "Why is this Course Taught in R?\nR is a programming language that is awesome for environmental data scientists\n\n\n\nBenefits of R:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFree\nReproducible\nAccessible\nPopular\nVersatile\n\n\n\n\nPiggy Bank\nRepeat\nUniversal Access\nStar\nMasks Theater"
  },
  {
    "objectID": "materials/slides_3b.html#rs-popularity",
    "href": "materials/slides_3b.html#rs-popularity",
    "title": "Intro to Data Visualisation II",
    "section": "R’s Popularity",
    "text": "R’s Popularity\n\n\n\n\n\n\n\n\n\n\n\nData from Lai et al. 2019"
  },
  {
    "objectID": "materials/slides_3b.html#r-data-science-value",
    "href": "materials/slides_3b.html#r-data-science-value",
    "title": "Intro to Data Visualisation II",
    "section": "R Data Science Value",
    "text": "R Data Science Value\n\n\n\nArrows Left Right Reproducibility\n\nR is written in “scripts”\n\nScripts = step-by-step instructions\n\nScripts can be run by any R user\nAllows perfect replication of process\nPrograms that require clicking buttons are not (as) reproducible\n\nWould depend on accompanying written/verbal instructions\n\n\n\nCollaboration Handshake\n\nR scripts can be co-developed!\nThey can then be shared like a paper draft\nMany tools exist to formalize sharing\n\nWe’ll cover one in this course!\n\nUnscripted programs would again require written/verbal instructions\n\nThen hoping someone clicks the right buttons in the right way"
  },
  {
    "objectID": "materials/slides_3b.html#temperature-check",
    "href": "materials/slides_3b.html#temperature-check",
    "title": "Intro to Data Visualisation II",
    "section": "Temperature Check",
    "text": "Temperature Check\nHow are you Feeling?"
  },
  {
    "objectID": "materials/slides_3b.html#problem-solving-in-r",
    "href": "materials/slides_3b.html#problem-solving-in-r",
    "title": "Intro to Data Visualisation II",
    "section": "Problem Solving in R",
    "text": "Problem Solving in R"
  },
  {
    "objectID": "materials/slides_3b.html#problem-solving-methods",
    "href": "materials/slides_3b.html#problem-solving-methods",
    "title": "Intro to Data Visualisation II",
    "section": "Problem Solving Methods",
    "text": "Problem Solving Methods\n\nProblem solving is an important life skill generally\n\nAlso useful for data science!\n\n\n\n\n\n\nI do not recommend using AI as a problem-solving method\n\n\n\n\n\n\nLet’s discuss some useful strategies:\n\n‘Rubber duck’ method\nGoogle (seriously!)\nTeamwork\nTake a break\nWhatever methods you use!"
  },
  {
    "objectID": "materials/slides_3b.html#ai-aside",
    "href": "materials/slides_3b.html#ai-aside",
    "title": "Intro to Data Visualisation II",
    "section": "“AI” Aside",
    "text": "“AI” Aside\n\nI strongly discourage the use of AI tools in this class\n\nE.g., ChatGPT, GitHub CoPilot, etc.\n\n\n\n\n\n\nTwo primary reasons:\n\nIt undermines your learning\nThere is an ethical dimension we don’t have time to cover in this class\n\n\n\n\n\n\n\nHowever, I’m an educator not a cop\n\nI won’t be policing you in order to enforce my take on AI"
  },
  {
    "objectID": "materials/slides_3b.html#method-1-rubber-duck",
    "href": "materials/slides_3b.html#method-1-rubber-duck",
    "title": "Intro to Data Visualisation II",
    "section": "Method 1: Rubber Duck",
    "text": "Method 1: Rubber Duck\n\n\n\n\nExplain each line of your code to the duck\n\nGo into as much detail as possible\nRe-read lines carefully as you explain\n\n\n\n\n\n\n\nYou’ll catch typos/errors that you had missed!\n\n\n\n\n\n\nWhy “rubber duck” instead of “friend”?\n\nBecause it would likely be a dull experience for your friend Face Smile"
  },
  {
    "objectID": "materials/slides_3b.html#method-2-google",
    "href": "materials/slides_3b.html#method-2-google",
    "title": "Intro to Data Visualisation II",
    "section": "Method 2: Google",
    "text": "Method 2: Google\n\nThis is a serious suggestion!\n\nGoogle is truly an amazing resource for this\n\n\n\n\n\n\n\n\n\n\nIf you get an error:\n\nCopy the entire error message\nPaste it into Google\nCheck the first few links to see how others solved that issue"
  },
  {
    "objectID": "materials/slides_3b.html#google-tips",
    "href": "materials/slides_3b.html#google-tips",
    "title": "Intro to Data Visualisation II",
    "section": "Google Tips",
    "text": "Google Tips\n\nIgnore the “AI Overview”\n\nThis is frequently wrong and/or misleading\nClick actual links!\n\n\n\n\n\n\nUse a plus sign (+) between search terms\n\nE.g., “R + &lt;error message text&gt;”\n\n\n\n\n\n\n\nWhen specific wording matters, use quotes!\n\nE.g., “I want results with exactly this phrase”"
  },
  {
    "objectID": "materials/slides_3b.html#method-3-team-up",
    "href": "materials/slides_3b.html#method-3-team-up",
    "title": "Intro to Data Visualisation II",
    "section": "Method 3: Team Up!",
    "text": "Method 3: Team Up!\n\nGroup work is a classic method of problem solving\n\n\n\n\n\nEmail/text classmates about errors you’re encountering\n\nSet up a weekly time to meet and work together\n\n\n\n\n\n\n\nGroup work & assignments\n\nI really encourage you to work together to solve problems\nBUT assignments should be produced by you alone\nThere are no group assignments in this course!"
  },
  {
    "objectID": "materials/slides_3b.html#method-4-take-a-break",
    "href": "materials/slides_3b.html#method-4-take-a-break",
    "title": "Intro to Data Visualisation II",
    "section": "Method 4: Take a Break",
    "text": "Method 4: Take a Break"
  },
  {
    "objectID": "materials/slides_3b.html#method-4-take-a-break-1",
    "href": "materials/slides_3b.html#method-4-take-a-break-1",
    "title": "Intro to Data Visualisation II",
    "section": "Method 4: Take a Break",
    "text": "Method 4: Take a Break\n\nCoding issues can be super frustrating\n\nTotally normal to feel this way\n\n\n\n\n\n\nIf you are struggling to solve a problem, take a few minutes to step re-set\n\nPhysically step away and do something active\nDo one of your hobbies for a few minutes\nWork on something else\n\n\n\n\n\n\n\nReturn to the problem an hour or so later and try again!"
  },
  {
    "objectID": "materials/slides_3b.html#method-5-yours",
    "href": "materials/slides_3b.html#method-5-yours",
    "title": "Intro to Data Visualisation II",
    "section": "Method 5: Yours!",
    "text": "Method 5: Yours!\n\nAs students, you’re experienced problem solvers already!\n\n\n\n\n\nCode problems can likely be solved by the strategies you already use!\n\n\n\n\n\n\nHow do you solve problems you encounter in other courses or at work?"
  },
  {
    "objectID": "materials/slides_3b.html#temperature-check-1",
    "href": "materials/slides_3b.html#temperature-check-1",
    "title": "Intro to Data Visualisation II",
    "section": "Temperature Check",
    "text": "Temperature Check\nHow are you Feeling?"
  },
  {
    "objectID": "materials/slides_3b.html#computer-file-paths",
    "href": "materials/slides_3b.html#computer-file-paths",
    "title": "Intro to Data Visualisation II",
    "section": "Computer File Paths",
    "text": "Computer File Paths\n\nComputers store files in “folders”\n\nFolders can be nested inside other folders\n\n\n\n\n\n\nThe name of all folders leading to a particular file is that file’s “file path”\n\nFile path starts at the biggest folder (“top” folder) and ends at the file\nEach folder name is separated by slashes (\\ or /)\n\n\n\n\n\n\n\nFor example: ~ / Downloads / BIO-316_syllabus.docx"
  },
  {
    "objectID": "materials/slides_3b.html#file-path-example",
    "href": "materials/slides_3b.html#file-path-example",
    "title": "Intro to Data Visualisation II",
    "section": "File Path Example",
    "text": "File Path Example\nWhat is the file path for the notes document in this image?\n\n~ / Documents / BIO 316 / BIO316-Notes-Week 1.docx"
  },
  {
    "objectID": "materials/slides_3b.html#practice-file-paths",
    "href": "materials/slides_3b.html#practice-file-paths",
    "title": "Intro to Data Visualisation II",
    "section": "Practice: File Paths",
    "text": "Practice: File Paths\n\nPick a file on your computer\n\nNot one in the “Downloads” folder (file paths are too short)\n\n\n\n\n\n\nWhat is that file’s path?\n\nHint: ~ / … / … / …\n\n\n\n\n\n\n\nWhen you have it, show me the file and tell me its path"
  },
  {
    "objectID": "materials/slides_3b.html#upcoming-due-dates",
    "href": "materials/slides_3b.html#upcoming-due-dates",
    "title": "Intro to Data Visualisation II",
    "section": "Upcoming Due Dates",
    "text": "Upcoming Due Dates\n\n\nDue before lab\nDue ASAP\n\nInstall R (see here)\nInstall RStudio (see here)\nRead the syllabus (esp. point values + assignment descriptions)\n\nDue by midnight\n\nMuddiest Point #1\n\n\nDue before lecture\n(By midnight)\n\nRead the British Ecological Society’s Reproducible Code Guide (p.1-12)\n\n\n\n\n\n\nData Visualisation II"
  },
  {
    "objectID": "materials/slides_2a.html#schedule",
    "href": "materials/slides_2a.html#schedule",
    "title": "Intro to Data Visualisation",
    "section": "Schedule",
    "text": "Schedule\n\nWeek 2: Data Visualisation 1\nWeek 3: Data Visualisation 2\nWeek 4: Open Science\nWeek 5: Mixed Methods Intro\nReading Week\nWeek 6: Qualitative Research (3 weeks)\n\nLecture, Labs, Data Skills, Application."
  },
  {
    "objectID": "materials/slides_2a.html#this-is-research-methods",
    "href": "materials/slides_2a.html#this-is-research-methods",
    "title": "Intro to Data Visualisation",
    "section": "This is Research Methods",
    "text": "This is Research Methods\nBut don’t assume all that we talk about here is solely useful in research contexts\nIt is a new programme this year, and one that we know to be dramatically more potent than what has gone before"
  },
  {
    "objectID": "materials/slides_2a.html#the-big-setup",
    "href": "materials/slides_2a.html#the-big-setup",
    "title": "Intro to Data Visualisation",
    "section": "The Big Setup",
    "text": "The Big Setup\n\nOne day, you’ll want to change a mind, capture interest, persuade, convince, or motivate.\nIt could be an idea, a product, a pitch, or even a truth.\nI want you to be ready for that moment with an overflowing toolbox!"
  },
  {
    "objectID": "materials/slides_2a.html#i-didnt-really-give-the-title-much-effort",
    "href": "materials/slides_2a.html#i-didnt-really-give-the-title-much-effort",
    "title": "Intro to Data Visualisation",
    "section": "I didn’t really give the title much effort",
    "text": "I didn’t really give the title much effort\n\n\nDid I inspire wonder?\n\nWhat if I had chosen a different title?\n\nTruth, Beauty, and Power\nSeeing is Believing: The Power of Data Visualization in Psychology\nFrom Data to Drama: The Art of Psychological Storytelling"
  },
  {
    "objectID": "materials/slides_2a.html#ˈdeɪtə-vɪʒuəlɪˈzeɪʃən-noun",
    "href": "materials/slides_2a.html#ˈdeɪtə-vɪʒuəlɪˈzeɪʃən-noun",
    "title": "Intro to Data Visualisation",
    "section": "/ˈdeɪtə vɪʒuəlɪˈzeɪʃən/ [noun]",
    "text": "/ˈdeɪtə vɪʒuəlɪˈzeɪʃən/ [noun]\n\n\nTurning numbers into pictures?\nHelping yourself understand the numbers?\nHelping others understand the numbers?\nDrawing emergent properties from the numbers?\nInterpolating, extrapolating, predicting the future?\nCapturing the audience’s imagination and telling a story?"
  },
  {
    "objectID": "materials/slides_2a.html#standing-on-the-shoulders-of-giants",
    "href": "materials/slides_2a.html#standing-on-the-shoulders-of-giants",
    "title": "Intro to Data Visualisation",
    "section": "Standing on the Shoulders of Giants",
    "text": "Standing on the Shoulders of Giants\n\n\n\n\n\n\nHave a think about…\n\n\n\n\n\n\nSong lyrics you can’t forget\nYour favorite comedian\nA non-verbal behaviour that’s powerful\nYour favorite psychologist\n\n\n\n\n\nYour favourite teacher\nThe best communicator you know\nSomeone you consider a genius\nA piece of ‘art’ that captivates you"
  },
  {
    "objectID": "materials/slides_2a.html#the-goat",
    "href": "materials/slides_2a.html#the-goat",
    "title": "Intro to Data Visualisation",
    "section": "The Goat",
    "text": "The Goat\nHans Rosling - Joy of Stats 5 mins\nHans Rosling TED Talk 20 mins\nGapminder Website"
  },
  {
    "objectID": "materials/slides_2a.html#my-quick-effort-last-night",
    "href": "materials/slides_2a.html#my-quick-effort-last-night",
    "title": "Intro to Data Visualisation",
    "section": "My quick effort last night",
    "text": "My quick effort last night"
  },
  {
    "objectID": "materials/slides_2a.html#platos-triad-the-true-the-good-and-the-beautiful",
    "href": "materials/slides_2a.html#platos-triad-the-true-the-good-and-the-beautiful",
    "title": "Intro to Data Visualisation",
    "section": "Plato’s Triad: The True, The Good, and The Beautiful",
    "text": "Plato’s Triad: The True, The Good, and The Beautiful\n\nPlato believed in the intrinsic connection between truth, goodness, and beauty — a concept that has influenced Western thought for centuries.\nFor Plato, these three qualities are inseparable in the realm of Forms.\nThe truth is inherently good, and what is good is also inherently beautiful.\nThus, if something is false, it cannot be truly beautiful or good."
  },
  {
    "objectID": "materials/slides_2a.html#obligatory-plato-quote",
    "href": "materials/slides_2a.html#obligatory-plato-quote",
    "title": "Intro to Data Visualisation",
    "section": "Obligatory Plato Quote",
    "text": "Obligatory Plato Quote\n\n\n\n\n\n\nKey Principle\n\n\n“Form and content must work together to give rise to beauty and truth.” - Plato"
  },
  {
    "objectID": "materials/slides_2a.html#rule-1-know-your-audience",
    "href": "materials/slides_2a.html#rule-1-know-your-audience",
    "title": "Intro to Data Visualisation",
    "section": "Rule #1! Know Your Audience",
    "text": "Rule #1! Know Your Audience\n“First seek to understand, then be understood”\n\n\nStephen Covey’s 5th Habit of Highly Effective People\nCrucial in academia and research\nApplies to various fields: Sales and marketing, Education, Leadership"
  },
  {
    "objectID": "materials/slides_2a.html#why-it-matters-in-research",
    "href": "materials/slides_2a.html#why-it-matters-in-research",
    "title": "Intro to Data Visualisation",
    "section": "Why it matters in research",
    "text": "Why it matters in research\n\n\nTailors your communication effectively\nIncreases impact of your research\nHelps bridge knowledge gaps\nFacilitates interdisciplinary collaboration\nImproves public engagement with science"
  },
  {
    "objectID": "materials/slides_2a.html#financial-times",
    "href": "materials/slides_2a.html#financial-times",
    "title": "Intro to Data Visualisation",
    "section": "Financial Times",
    "text": "Financial Times\nVisual Vocabulary (best to right-click and open in a new window)\n\n\n\n\n\n\nAvailable as pdf on VLE\n\n\nFind in Week 02 Content as a pdf which is perhaps more user-friendly"
  },
  {
    "objectID": "materials/slides_2a.html#data-as-stories",
    "href": "materials/slides_2a.html#data-as-stories",
    "title": "Intro to Data Visualisation",
    "section": "1. Data as Stories",
    "text": "1. Data as Stories\n\nConcept:\nData visualization isn’t just about numbers; it’s about telling a compelling story. The visual elements should guide the viewer through a narrative—showing change over time, revealing relationships, or highlighting key insights.\nApplication:\nCase studies or reports where data unfolds in a logical, easy-to-follow sequence, such as telling the story of a community’s health or an individual’s progress.\nExample:\nAn interactive webpage showing the impact of social media on mental health through a narrative of personal experiences and trends."
  },
  {
    "objectID": "materials/slides_2a.html#infographics",
    "href": "materials/slides_2a.html#infographics",
    "title": "Intro to Data Visualisation",
    "section": "2. Infographics",
    "text": "2. Infographics\n\nConcept:\nInfographics combine text, visuals, and data to create a highly accessible, single-view communication tool. They are visually engaging, often using illustrations, icons, and small amounts of data to convey key points at a glance.\nApplication:\nIdeal for social media, brochures, or posters where quick digestion of information is needed. Frequently used in public campaigns, marketing, and education.\nExample:\nAn infographic explaining the psychology of sleep, showing how sleep cycles work and tips to improve sleep habits."
  },
  {
    "objectID": "materials/slides_2a.html#animations",
    "href": "materials/slides_2a.html#animations",
    "title": "Intro to Data Visualisation",
    "section": "3. Animations",
    "text": "3. Animations\n\nConcept:\nAnimations bring data to life by adding movement, showing how data evolves over time or revealing complex processes step-by-step. They are effective in capturing attention and illustrating dynamic changes.\nApplication:\nExcellent for presentations, video content, or interactive dashboards. Can be used in education to break down complex processes (e.g., brain activity) or in marketing to show trends over time.\nExample:\nAn animated chart showing the rise of global temperatures over decades, or an animation walking through the steps of a psychological experiment."
  },
  {
    "objectID": "materials/slides_2a.html#dashboards",
    "href": "materials/slides_2a.html#dashboards",
    "title": "Intro to Data Visualisation",
    "section": "4. Dashboards",
    "text": "4. Dashboards\n\nConcept:\nDashboards provide a comprehensive, real-time view of data, often integrating multiple graphs and charts into one interface. They allow users to interact with the data, filter information, and explore different aspects on demand.\nApplication:\nUseful in business, healthcare, or education where decision-makers need up-to-date, actionable insights. Dashboards can provide an overview of research data, employee performance, or patient metrics.\nExample:\nA live dashboard tracking the results of a clinical trial, showing participant progress, side effects, and key outcome measures."
  },
  {
    "objectID": "materials/slides_2a.html#data-driven-art",
    "href": "materials/slides_2a.html#data-driven-art",
    "title": "Intro to Data Visualisation",
    "section": "5. Data-Driven Art",
    "text": "5. Data-Driven Art\n\nConcept:\nData-driven art turns data into creative visual expressions, where aesthetics and meaning are equally important. It communicates complex ideas through artistic mediums, helping people emotionally connect with the data.\nApplication:\nUseful in public exhibitions, museums, or campaigns designed to evoke emotional responses. Can be particularly impactful when the goal is to inspire action or reflection.\nExample:\nA visual installation representing the world’s most polluted cities, where the intensity of color and shape corresponds to pollution levels."
  },
  {
    "objectID": "materials/slides_2a.html#interactive-visualizations",
    "href": "materials/slides_2a.html#interactive-visualizations",
    "title": "Intro to Data Visualisation",
    "section": "6. Interactive Visualizations",
    "text": "6. Interactive Visualizations\n\nConcept:\nInteractive visualizations allow the audience to explore the data themselves. Users can hover over elements, click to reveal additional information, and adjust parameters in real time to see how different variables influence outcomes.\nApplication:\nExcellent for websites, educational tools, and data journalism. This method is perfect for exploring complex datasets where users need control over what they want to see.\nExample:\nA web tool where users can explore the effects of different factors (e.g., age, gender, education) on job satisfaction."
  },
  {
    "objectID": "materials/slides_2a.html#maps-and-geospatial-visualizations",
    "href": "materials/slides_2a.html#maps-and-geospatial-visualizations",
    "title": "Intro to Data Visualisation",
    "section": "7. Maps and Geospatial Visualizations",
    "text": "7. Maps and Geospatial Visualizations\n\nConcept:\nData visualized on maps can be incredibly powerful, especially when geographic context is essential to the story. These visuals can show patterns across different locations, reveal relationships between data and place, and help identify regional trends.\nApplication:\nUsed in public health (tracking disease outbreaks), environmental research (tracking deforestation), and social science (mapping crime rates).\nExample:\nA heatmap showing the spread of a disease outbreak, highlighting areas of high infection rates over time."
  },
  {
    "objectID": "materials/slides_2a.html#data-journalism-and-narrated-data",
    "href": "materials/slides_2a.html#data-journalism-and-narrated-data",
    "title": "Intro to Data Visualisation",
    "section": "8. Data Journalism and Narrated Data",
    "text": "8. Data Journalism and Narrated Data\n\nConcept:\nNarrated data takes storytelling a step further by adding voiceovers or text to guide the viewer through the visualization, ensuring that key points are not missed and context is clearly explained.\nApplication:\nIdeal for educational content, presentations, and documentaries where understanding the deeper meaning behind the data is essential.\nExample:\nA video presentation explaining the impact of climate change, with data visualizations narrated by an expert who explains each trend."
  },
  {
    "objectID": "materials/slides_2a.html#introduction-to-data-visualization",
    "href": "materials/slides_2a.html#introduction-to-data-visualization",
    "title": "Intro to Data Visualisation",
    "section": "Introduction to Data Visualization",
    "text": "Introduction to Data Visualization\n\nWhat is Data Visualization?\n\nDefinition: The graphical representation of data to find insights, patterns, and trends.\nImportance: Simplifying complex data, making research more accessible, and aiding in decision-making."
  },
  {
    "objectID": "materials/slides_2a.html#introduction-to-data-visualization-1",
    "href": "materials/slides_2a.html#introduction-to-data-visualization-1",
    "title": "Intro to Data Visualisation",
    "section": "Introduction to Data Visualization",
    "text": "Introduction to Data Visualization\n\nWhy is it Important for Psychologists?\n\nFor Self: Making sense of information.\nOutwards: Presenting results to a wider audience (academics, professionals, the public)."
  },
  {
    "objectID": "materials/slides_2a.html#nuts-and-bolts",
    "href": "materials/slides_2a.html#nuts-and-bolts",
    "title": "Intro to Data Visualisation",
    "section": "Nuts and Bolts",
    "text": "Nuts and Bolts"
  },
  {
    "objectID": "materials/slides_2a.html#types-of-data",
    "href": "materials/slides_2a.html#types-of-data",
    "title": "Intro to Data Visualisation",
    "section": "Types of Data",
    "text": "Types of Data\n\n\nCategorical Data\n\nNominal (e.g., types of therapy, animal species)\nOrdinal (e.g., Likert scales, levels of education)\n\nContinuous Data\n\nInterval (e.g., temperature, time of day)\nRatio (e.g., height, reaction time)\n\nDiscrete vs Continuous\n\nThe difference between countable categories and measurements with infinite possible values."
  },
  {
    "objectID": "materials/slides_2a.html#common-types-of-graphs-and-when-to-use-them",
    "href": "materials/slides_2a.html#common-types-of-graphs-and-when-to-use-them",
    "title": "Intro to Data Visualisation",
    "section": "Common Types of Graphs and When to Use Them",
    "text": "Common Types of Graphs and When to Use Them"
  },
  {
    "objectID": "materials/slides_2a.html#bar-charts",
    "href": "materials/slides_2a.html#bar-charts",
    "title": "Intro to Data Visualisation",
    "section": "Bar Charts",
    "text": "Bar Charts\n\n\n\n\n\nBest for comparing discrete categories.\nExample: Comparing the number of participants in different experimental groups.\nEffective for showing relative differences between categories.\nCan be displayed vertically or horizontally.\nSuitable for both nominal and ordinal data."
  },
  {
    "objectID": "materials/slides_2a.html#histograms",
    "href": "materials/slides_2a.html#histograms",
    "title": "Intro to Data Visualisation",
    "section": "Histograms",
    "text": "Histograms\n\n\n\n\n\nUseful for visualizing the distribution of continuous data.\nExample: Showing the frequency distribution of test scores."
  },
  {
    "objectID": "materials/slides_2a.html#pie-charts",
    "href": "materials/slides_2a.html#pie-charts",
    "title": "Intro to Data Visualisation",
    "section": "Pie Charts",
    "text": "Pie Charts\n\n\n\n\n\nUse for displaying proportions of a whole, though less frequently used in scientific contexts."
  },
  {
    "objectID": "materials/slides_2a.html#line-graphs",
    "href": "materials/slides_2a.html#line-graphs",
    "title": "Intro to Data Visualisation",
    "section": "Line Graphs",
    "text": "Line Graphs\n\n\n\n\n\nGreat for showing trends over time or continuous data relationships.\nExample: Reaction time changes over time."
  },
  {
    "objectID": "materials/slides_2a.html#scatterplots",
    "href": "materials/slides_2a.html#scatterplots",
    "title": "Intro to Data Visualisation",
    "section": "Scatterplots",
    "text": "Scatterplots\n\n\n\n\n\nIdeal for representing relationships between two continuous variables.\nExample: Relationship between stress levels and performance."
  },
  {
    "objectID": "materials/slides_2a.html#boxplots",
    "href": "materials/slides_2a.html#boxplots",
    "title": "Intro to Data Visualisation",
    "section": "Boxplots",
    "text": "Boxplots\n\n\n\n\n\nExcellent for summarizing the distribution, central tendency, and variability of continuous data."
  },
  {
    "objectID": "materials/slides_2a.html#density-plots",
    "href": "materials/slides_2a.html#density-plots",
    "title": "Intro to Data Visualisation",
    "section": "Density Plots",
    "text": "Density Plots\n\n\n\n\n\nUseful for visualizing large, complex datasets, or relationships between multiple variables."
  },
  {
    "objectID": "materials/slides_2a.html#violin-plots",
    "href": "materials/slides_2a.html#violin-plots",
    "title": "Intro to Data Visualisation",
    "section": "Violin Plots",
    "text": "Violin Plots\n\n\n\n\n\nCombines boxplot and density information to visualize data distribution."
  },
  {
    "objectID": "materials/slides_2a.html#design-principles-for-effective-data-visualizations-1",
    "href": "materials/slides_2a.html#design-principles-for-effective-data-visualizations-1",
    "title": "Intro to Data Visualisation",
    "section": "Design Principles for Effective Data Visualizations 1",
    "text": "Design Principles for Effective Data Visualizations 1\nClarity and Simplicity Avoid unnecessary elements (e.g., chartjunk, 3D effects). Focus on the key message and make the visualization intuitive to interpret.\nConsistency Use consistent colors, fonts, and labels across different graphs."
  },
  {
    "objectID": "materials/slides_2a.html#design-principles-for-effective-data-visualizations-2",
    "href": "materials/slides_2a.html#design-principles-for-effective-data-visualizations-2",
    "title": "Intro to Data Visualisation",
    "section": "Design Principles for Effective Data Visualizations 2",
    "text": "Design Principles for Effective Data Visualizations 2\nLabeling and Annotations Clear axis titles, units, and legends to make the graph self-explanatory.\nColor Choices Consider color blindness (red-green blindness common) and use color schemes that enhance readability."
  },
  {
    "objectID": "materials/slides_2a.html#design-principles-for-effective-data-visualizations-3",
    "href": "materials/slides_2a.html#design-principles-for-effective-data-visualizations-3",
    "title": "Intro to Data Visualisation",
    "section": "Design Principles for Effective Data Visualizations 3",
    "text": "Design Principles for Effective Data Visualizations 3\nAppropriate Scale Avoid misleading visuals by ensuring that scales are not exaggerated or compressed.\nHierarchy and Focus Guide the viewer’s attention to the most important parts of the graph (using size, bolding, or contrasting colors).\nTelling a Story Visuals should flow logically and lead the reader through the narrative of your data."
  },
  {
    "objectID": "materials/slides_2a.html#how-to-choose-the-right-graph-for-your-data",
    "href": "materials/slides_2a.html#how-to-choose-the-right-graph-for-your-data",
    "title": "Intro to Data Visualisation",
    "section": "How to Choose the Right Graph for Your Data",
    "text": "How to Choose the Right Graph for Your Data\nUnderstanding Your Audience Tailor complexity based on who is consuming the data (public, peers, experts).\nData-Driven Decision Making Choose the graph based on the data structure and the key message you want to convey.\nAvoid Common Pitfalls Misleading graphs (incorrect scales, truncated axes) can misinform rather than clarify. Overcomplicating simple messages with too many visual elements or excessive detail."
  },
  {
    "objectID": "materials/slides_2a.html#psychological-aspects-of-data-visualization",
    "href": "materials/slides_2a.html#psychological-aspects-of-data-visualization",
    "title": "Intro to Data Visualisation",
    "section": "Psychological Aspects of Data Visualization",
    "text": "Psychological Aspects of Data Visualization\nCognitive Load Minimizing mental effort required to interpret visuals.\nGestalt Principles of Perception How humans perceive visual elements (e.g., proximity, similarity, closure) and how to leverage these in visual design.\nEmotional Impact of Visuals Using visual design to invoke emotions (e.g., empathy, urgency), particularly in health or social psychology data."
  },
  {
    "objectID": "materials/slides_2a.html#examples-and-case-studies",
    "href": "materials/slides_2a.html#examples-and-case-studies",
    "title": "Intro to Data Visualisation",
    "section": "Examples and Case Studies",
    "text": "Examples and Case Studies\nGood vs Bad Visualizations Comparing effective vs poorly designed graphs. Examples from psychological research where data visualizations impacted understanding.\nInteractive Exercise Students create their own data visualization using sample datasets (e.g., reaction times, Likert scale responses)."
  },
  {
    "objectID": "materials/slides_2a.html#tech-stack",
    "href": "materials/slides_2a.html#tech-stack",
    "title": "Intro to Data Visualisation",
    "section": "Tech Stack",
    "text": "Tech Stack"
  },
  {
    "objectID": "materials/slides_2a.html#technical-overview-of-research-methods",
    "href": "materials/slides_2a.html#technical-overview-of-research-methods",
    "title": "Intro to Data Visualisation",
    "section": "Technical Overview of Research Methods",
    "text": "Technical Overview of Research Methods\n\n\nCombine programming power with spreadsheet familiarity\nLearn to choose the right tool for each task\nDevelop a versatile skillset for data analysis and presentation"
  },
  {
    "objectID": "materials/slides_2a.html#r-programming-positrstudio",
    "href": "materials/slides_2a.html#r-programming-positrstudio",
    "title": "Intro to Data Visualisation",
    "section": "R Programming (Posit/RStudio)",
    "text": "R Programming (Posit/RStudio)\n\n\nPrimary tool for data analysis\nPowerful statistical capabilities\nExtensive libraries for data manipulation\nCreate publication-quality graphics\nLearn more about R"
  },
  {
    "objectID": "materials/slides_2a.html#quarto-for-publishing",
    "href": "materials/slides_2a.html#quarto-for-publishing",
    "title": "Intro to Data Visualisation",
    "section": "Quarto for Publishing",
    "text": "Quarto for Publishing\n\n\nCreate dynamic documents\nPublish websites, blogs, presentations\nSeamless integration with R\nExplore Quarto"
  },
  {
    "objectID": "materials/slides_2a.html#excel",
    "href": "materials/slides_2a.html#excel",
    "title": "Intro to Data Visualisation",
    "section": "Excel",
    "text": "Excel\n\n\nWidely used in various industries\nFamiliar interface for many users\nUseful for initial data exploration\nComplementary to R for certain tasks"
  },
  {
    "objectID": "materials/slides_2a.html#why-teach-myself-excel",
    "href": "materials/slides_2a.html#why-teach-myself-excel",
    "title": "Intro to Data Visualisation",
    "section": "Why teach myself Excel?",
    "text": "Why teach myself Excel?\n\n\n\n\n\n\nNote\n\n\n\nUniversal tool in business and research\nQuick for small datasets and simple analyses - not great for more advanced\nValuable skill for future employment"
  },
  {
    "objectID": "materials/slides_2a.html#there-are-many-other-tools",
    "href": "materials/slides_2a.html#there-are-many-other-tools",
    "title": "Intro to Data Visualisation",
    "section": "There are many other tools",
    "text": "There are many other tools\n\nCanva\nTableau\nPowerBI\nGoogle variants\nPython (Jupyter Notebooks)\nMatLab (Some Neuroscience applications)"
  },
  {
    "objectID": "materials/slides_2a.html#what-do-you-currently-use-to-write",
    "href": "materials/slides_2a.html#what-do-you-currently-use-to-write",
    "title": "Intro to Data Visualisation",
    "section": "What do you currently use to write?",
    "text": "What do you currently use to write?\nHow do you write your essays or lab reports?\n\nMicrosoft Word?\nGoogle Docs?\nMarkdown?"
  },
  {
    "objectID": "materials/slides_2a.html#what-do-you-currently-use-to-calculate",
    "href": "materials/slides_2a.html#what-do-you-currently-use-to-calculate",
    "title": "Intro to Data Visualisation",
    "section": "What do you currently use to calculate?",
    "text": "What do you currently use to calculate?\nHow do you currently play with numbers?\n\nExcel?\nSPSS?\nR?\nPython?"
  },
  {
    "objectID": "materials/slides_2a.html#what-is-quarto",
    "href": "materials/slides_2a.html#what-is-quarto",
    "title": "Intro to Data Visualisation",
    "section": "What is Quarto?",
    "text": "What is Quarto?\nQuarto is an open-source scientific and technical publishing system that allows you to combine text, images, code, plots, and tables in a fully-reproducible document.\nQuarto has support for multiple languages including R, Python, Julia, and Observable.\nIt also works for a range of output formats such as PDFs, HTML documents, websites, presentations,…"
  },
  {
    "objectID": "materials/slides_2a.html#why-use-quarto-why-use-r",
    "href": "materials/slides_2a.html#why-use-quarto-why-use-r",
    "title": "Intro to Data Visualisation",
    "section": "Why use Quarto? Why use R?",
    "text": "Why use Quarto? Why use R?\n\nMore journals require code to be submitted (for transparency and reproducibility). Keeping the code with the paper makes this easier.\nCopying and pasting is tedious (and a great source of accidental errors).\nIf you fix an error in code or data, the results and figures in the paper update automatically.\nEasy to share publicly.\nOpen source so anyone can use it."
  },
  {
    "objectID": "materials/slides_2a.html#what-about-r-markdown",
    "href": "materials/slides_2a.html#what-about-r-markdown",
    "title": "Intro to Data Visualisation",
    "section": "What about R Markdown?",
    "text": "What about R Markdown?\nR Markdown isn’t going anywhere but…\n\nQuarto has better multi-language support\nMore user-friendly\nBetter control of the output layouts"
  },
  {
    "objectID": "materials/slides_2a.html#output-types",
    "href": "materials/slides_2a.html#output-types",
    "title": "Intro to Data Visualisation",
    "section": "Output types",
    "text": "Output types\n\n\nDocuments: HTML, PDF, MS Word, Markdown\nPresentations: Revealjs, PowerPoint, Beamer\nWebsites and blogs\nBooks, code notebooks and dashboards\nInteractive apps e.g. Shiny (in R, Python, Julia, Observablejs)"
  },
  {
    "objectID": "materials/slides_2a.html#examples",
    "href": "materials/slides_2a.html#examples",
    "title": "Intro to Data Visualisation",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "materials/slides_2a.html#lets-start-looking-at-some-examples",
    "href": "materials/slides_2a.html#lets-start-looking-at-some-examples",
    "title": "Intro to Data Visualisation",
    "section": "Let’s start looking at some examples",
    "text": "Let’s start looking at some examples\nThe first question is “What do you want to show?”\nFrom Data to Viz website\nQuarto Publishing System"
  },
  {
    "objectID": "materials/slides_2a.html#definition",
    "href": "materials/slides_2a.html#definition",
    "title": "Intro to Data Visualisation",
    "section": "Definition",
    "text": "Definition\nHistogram"
  },
  {
    "objectID": "materials/slides_2a.html#what-for",
    "href": "materials/slides_2a.html#what-for",
    "title": "Intro to Data Visualisation",
    "section": "What for",
    "text": "What for\nHistograms are used to study the distribution of one or a few variables. Checking the distribution of your variables one by one is probably the first task you should do when you get a new dataset. It delivers a good quantity of information. Several distribution shapes exist, here is an illustration of the 6 most common ones:"
  },
  {
    "objectID": "materials/slides_2a.html#examples-1",
    "href": "materials/slides_2a.html#examples-1",
    "title": "Intro to Data Visualisation",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "materials/slides_2a.html#useful-for",
    "href": "materials/slides_2a.html#useful-for",
    "title": "Intro to Data Visualisation",
    "section": "Useful for:",
    "text": "Useful for:\nChecking this distribution also helps you discovering mistakes in the data. For example, the comb distribution can often denote a rounding that has been applied to the variable or another mistake.\nAs a second step, histogram allow to compare the distribution of a few variables. Don’t compare more than 3 or 4, it would make the figure cluttered and unreadable. This comparison can be done showing the 2 variables on the same graphic and using transparency."
  },
  {
    "objectID": "materials/slides_2a.html#example",
    "href": "materials/slides_2a.html#example",
    "title": "Intro to Data Visualisation",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "materials/slides_2a.html#variation",
    "href": "materials/slides_2a.html#variation",
    "title": "Intro to Data Visualisation",
    "section": "Variation",
    "text": "Variation"
  },
  {
    "objectID": "materials/slides_2a.html#common-mistakes",
    "href": "materials/slides_2a.html#common-mistakes",
    "title": "Intro to Data Visualisation",
    "section": "Common mistakes",
    "text": "Common mistakes"
  },
  {
    "objectID": "materials/slides_2a.html#definition-1",
    "href": "materials/slides_2a.html#definition-1",
    "title": "Intro to Data Visualisation",
    "section": "Definition",
    "text": "Definition\nDensity Plot"
  },
  {
    "objectID": "materials/slides_2a.html#what-for-1",
    "href": "materials/slides_2a.html#what-for-1",
    "title": "Intro to Data Visualisation",
    "section": "What for",
    "text": "What for\nDensity plots are used to study the distribution of one or a few variables. Checking the distribution of your variables one by one is probably the first task you should do when you get a new dataset. It delivers a good quantity of information. Several distribution shapes exist, here is an illustration of the 6 most common ones:"
  },
  {
    "objectID": "materials/slides_2a.html#examples-2",
    "href": "materials/slides_2a.html#examples-2",
    "title": "Intro to Data Visualisation",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "materials/slides_2a.html#useful-for-1",
    "href": "materials/slides_2a.html#useful-for-1",
    "title": "Intro to Data Visualisation",
    "section": "Useful for:",
    "text": "Useful for:\nChecking this distribution also helps you discovering mistakes in the data. For example, the comb distribution can often denote a rounding that has been applied to the variable or another mistake.\nAs a second step, density plots allow to compare the distribution of a few variables. Don’t compare more than 3 or 4, it would make the figure cluttered and unreadable. This comparison can be done showing the 2 variables on the same graphic and using transparency."
  },
  {
    "objectID": "materials/slides_2a.html#example-1",
    "href": "materials/slides_2a.html#example-1",
    "title": "Intro to Data Visualisation",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "materials/slides_2a.html#variation-1",
    "href": "materials/slides_2a.html#variation-1",
    "title": "Intro to Data Visualisation",
    "section": "Variation",
    "text": "Variation"
  },
  {
    "objectID": "materials/slides_2a.html#common-mistakes-1",
    "href": "materials/slides_2a.html#common-mistakes-1",
    "title": "Intro to Data Visualisation",
    "section": "Common mistakes",
    "text": "Common mistakes"
  },
  {
    "objectID": "materials/slides_2a.html#definition-2",
    "href": "materials/slides_2a.html#definition-2",
    "title": "Intro to Data Visualisation",
    "section": "Definition",
    "text": "Definition\nScatter Plot"
  },
  {
    "objectID": "materials/slides_2a.html#what-for-2",
    "href": "materials/slides_2a.html#what-for-2",
    "title": "Intro to Data Visualisation",
    "section": "What for",
    "text": "What for\nA scatterplot is made to study the relationship between 2 variables. Thus it is often accompanied by a correlation coefficient calculation, that usually tries to measure the linear relationship.\nHowever other types of relationship can be detected using scatterplots, and a common task consists to fit a model explaining Y in function of X. Here are a few patterns you can detect doing a scatterplot."
  },
  {
    "objectID": "materials/slides_2a.html#examples-3",
    "href": "materials/slides_2a.html#examples-3",
    "title": "Intro to Data Visualisation",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "materials/slides_2a.html#useful-for-2",
    "href": "materials/slides_2a.html#useful-for-2",
    "title": "Intro to Data Visualisation",
    "section": "Useful for:",
    "text": "Useful for:\n\nDetecting relationships between two variables\nIdentifying patterns or trends in data\nSpotting outliers or unusual data points\nComparing different groups or categories within the data"
  },
  {
    "objectID": "materials/slides_2a.html#variation-2",
    "href": "materials/slides_2a.html#variation-2",
    "title": "Intro to Data Visualisation",
    "section": "Variation",
    "text": "Variation"
  },
  {
    "objectID": "materials/slides_2a.html#common-mistakes-2",
    "href": "materials/slides_2a.html#common-mistakes-2",
    "title": "Intro to Data Visualisation",
    "section": "Common mistakes",
    "text": "Common mistakes"
  },
  {
    "objectID": "materials/slides_2a.html#definition-3",
    "href": "materials/slides_2a.html#definition-3",
    "title": "Intro to Data Visualisation",
    "section": "Definition",
    "text": "Definition\nBar Plot"
  },
  {
    "objectID": "materials/slides_2a.html#what-for-3",
    "href": "materials/slides_2a.html#what-for-3",
    "title": "Intro to Data Visualisation",
    "section": "What for",
    "text": "What for\nA barplot shows the relationship between a numeric and a categoric variable. In the previous graphic, each country is a level of the categoric variable, and the quantity of weapon sold is the numeric variable. An ordered barplot is a very good choice here since it displays both the ranking of countries and their specific value.\nA barplot can also display values for several levels of grouping. Here’s an example of a grouped barplot:"
  },
  {
    "objectID": "materials/slides_2a.html#examples-4",
    "href": "materials/slides_2a.html#examples-4",
    "title": "Intro to Data Visualisation",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "materials/slides_2a.html#useful-for-3",
    "href": "materials/slides_2a.html#useful-for-3",
    "title": "Intro to Data Visualisation",
    "section": "Useful for:",
    "text": "Useful for:\n\nComparing values across categories\nShowing the distribution of a numeric variable for different groups\nDisplaying rankings or ordered data\nVisualizing part-to-whole relationships (in stacked bar charts)"
  },
  {
    "objectID": "materials/slides_2a.html#variation-3",
    "href": "materials/slides_2a.html#variation-3",
    "title": "Intro to Data Visualisation",
    "section": "Variation",
    "text": "Variation"
  },
  {
    "objectID": "materials/slides_2a.html#common-mistakes-3",
    "href": "materials/slides_2a.html#common-mistakes-3",
    "title": "Intro to Data Visualisation",
    "section": "Common mistakes",
    "text": "Common mistakes"
  },
  {
    "objectID": "materials/slides_2a.html#definition-4",
    "href": "materials/slides_2a.html#definition-4",
    "title": "Intro to Data Visualisation",
    "section": "Definition",
    "text": "Definition\nLine Chart"
  },
  {
    "objectID": "materials/slides_2a.html#what-for-4",
    "href": "materials/slides_2a.html#what-for-4",
    "title": "Intro to Data Visualisation",
    "section": "What for",
    "text": "What for\nLine charts can be used to show the evolution of one or several variables. Here is an example showing the evolution of three baby name frequencies in the US between 1880 and 2015:"
  },
  {
    "objectID": "materials/slides_2a.html#useful-for-4",
    "href": "materials/slides_2a.html#useful-for-4",
    "title": "Intro to Data Visualisation",
    "section": "Useful for:",
    "text": "Useful for:\n\nVisualizing trends over time\nComparing multiple variables or categories over a continuous axis\nShowing the rate of change between data points\nIdentifying patterns, cycles, or anomalies in data"
  },
  {
    "objectID": "materials/slides_2a.html#variation-4",
    "href": "materials/slides_2a.html#variation-4",
    "title": "Intro to Data Visualisation",
    "section": "Variation",
    "text": "Variation"
  },
  {
    "objectID": "materials/slides_2a.html#common-caveats",
    "href": "materials/slides_2a.html#common-caveats",
    "title": "Intro to Data Visualisation",
    "section": "Common caveats",
    "text": "Common caveats"
  },
  {
    "objectID": "materials/slides_2a.html#definition-5",
    "href": "materials/slides_2a.html#definition-5",
    "title": "Intro to Data Visualisation",
    "section": "Definition",
    "text": "Definition\nLine Chart"
  },
  {
    "objectID": "materials/slides_2a.html#common-caveats-1",
    "href": "materials/slides_2a.html#common-caveats-1",
    "title": "Intro to Data Visualisation",
    "section": "Common caveats",
    "text": "Common caveats"
  },
  {
    "objectID": "materials/slides_2a.html#definition-6",
    "href": "materials/slides_2a.html#definition-6",
    "title": "Intro to Data Visualisation",
    "section": "Definition",
    "text": "Definition\nWord Cloud"
  },
  {
    "objectID": "materials/slides_2a.html#what-for-5",
    "href": "materials/slides_2a.html#what-for-5",
    "title": "Intro to Data Visualisation",
    "section": "What for",
    "text": "What for\nWord clouds are useful for:\n\nQuickly perceiving the most prominent terms\nLocating a term alphabetically to determine its relative prominence\nCreating visually appealing representations of text data\n\nThey are widely used in media and well understood by the public."
  },
  {
    "objectID": "materials/slides_2a.html#useful-for-5",
    "href": "materials/slides_2a.html#useful-for-5",
    "title": "Intro to Data Visualisation",
    "section": "Useful for:",
    "text": "Useful for:\n\nSummarizing large amounts of text data\nHighlighting key themes or topics in a dataset\nCreating engaging visuals for presentations or reports\nComparing word frequencies across different texts or sources"
  },
  {
    "objectID": "materials/slides_2a.html#example-2",
    "href": "materials/slides_2a.html#example-2",
    "title": "Intro to Data Visualisation",
    "section": "Example",
    "text": "Example\nHere’s an alternative representation using a lollipop plot, which addresses some of the limitations of word clouds:\n\n\n'data.frame':   11529 obs. of  3 variables:\n $ artist: chr  \"booba\" \"booba\" \"booba\" \"booba\" ...\n $ song  : chr  \"113\" \"113\" \"113\" \"113\" ...\n $ word  : chr  \"paroles.net\" \"above\" \"lyrics\" \"function\" ..."
  },
  {
    "objectID": "materials/slides_2a.html#variation-5",
    "href": "materials/slides_2a.html#variation-5",
    "title": "Intro to Data Visualisation",
    "section": "Variation",
    "text": "Variation\nMany variations exist for word clouds:\n\nDifferent shapes, sometimes using the shape of an object related to the topic\nVarying text orientation, font, size, and colors\nInteractive word clouds that change or provide additional information on hover"
  },
  {
    "objectID": "materials/slides_2a.html#common-mistakes-4",
    "href": "materials/slides_2a.html#common-mistakes-4",
    "title": "Intro to Data Visualisation",
    "section": "Common mistakes",
    "text": "Common mistakes\n\nRelying too heavily on word clouds for accurate data representation\nIgnoring the limitations of area as a metaphor for numeric values\nNot accounting for the bias created by longer words appearing larger\nUsing word clouds when more precise visualizations (like bar charts or lollipop plots) would be more appropriate\nCreating overly complex or cluttered word clouds that are difficult to read or interpret\n\nWord clouds, while visually appealing, are often criticized for their lack of accuracy in conveying information. Consider using them primarily for aesthetic purposes or in conjunction with more precise data visualization methods."
  },
  {
    "objectID": "materials/descriptives.html",
    "href": "materials/descriptives.html",
    "title": "Descriptive statistics",
    "section": "",
    "text": "Any time that you get a new data set to look at, one of the first tasks that you have to do is find ways of summarising the data in a compact, easily understood fashion. This is what descriptive statistics (as opposed to inferential statistics) is all about. In fact, to many people the term “statistics” is synonymous with descriptive statistics. It is this topic that we’ll consider in this chapter, but before going into any details, let’s take a moment to get a sense of why we need descriptive statistics. To do this, let’s load the aflsmall.Rdata file, and use the who() function in the lsr package to see what variables are stored in the file:\n\nload( \"materials/data/aflsmall.Rdata\" )\nlibrary(lsr)\nwho()\n\n   -- Name --      -- Class --   -- Size --\n   afl.finalists   factor        400       \n   afl.margins     numeric       176       \n\n\nThere are two variables here, afl.finalists and afl.margins. We’ll focus a bit on these two variables in this chapter, so I’d better tell you what they are. Unlike most of data sets in this book, these are actually real data, relating to the Australian Football League (AFL) 1 The afl.margins variable contains the winning margin (number of points) for all 176 home and away games played during the 2010 season. The afl.finalists variable contains the names of all 400 teams that played in all 200 finals matches played during the period 1987 to 2010. Let’s have a look at the afl.margins variable:\n\nprint(afl.margins)\n\n  [1]  56  31  56   8  32  14  36  56  19   1   3 104  43  44  72   9  28  25\n [19]  27  55  20  16  16   7  23  40  48  64  22  55  95  15  49  52  50  10\n [37]  65  12  39  36   3  26  23  20  43 108  53  38   4   8   3  13  66  67\n [55]  50  61  36  38  29   9  81   3  26  12  36  37  70   1  35  12  50  35\n [73]   9  54  47   8  47   2  29  61  38  41  23  24   1   9  11  10  29  47\n [91]  71  38  49  65  18   0  16   9  19  36  60  24  25  44  55   3  57  83\n[109]  84  35   4  35  26  22   2  14  19  30  19  68  11  75  48  32  36  39\n[127]  50  11   0  63  82  26   3  82  73  19  33  48   8  10  53  20  71  75\n[145]  76  54  44   5  22  94  29   8  98   9  89   1 101   7  21  52  42  21\n[163] 116   3  44  29  27  16   6  44   3  28  38  29  10  10\n\n\nThis output doesn’t make it easy to get a sense of what the data are actually saying. Just “looking at the data” isn’t a terribly effective way of understanding data. In order to get some idea about what’s going on, we need to calculate some descriptive statistics (this chapter) and draw some nice pictures (Chapter @ref(graphics). Since the descriptive statistics are the easier of the two topics, I’ll start with those, but nevertheless I’ll show you a histogram of the afl.margins data, since it should help you get a sense of what the data we’re trying to describe actually look like. But for what it’s worth, this histogram – which is shown in Figure @ref(fig:histogram1) – was generated using the hist() function. We’ll talk a lot more about how to draw histograms in Section @ref(hist). For now, it’s enough to look at the histogram and note that it provides a fairly interpretable representation of the afl.margins data.\n\nload(\"materials/data/aflsmall.Rdata\")\n# draw the plot\nhist( afl.margins, breaks=seq(0,120,10), border=\"white\",\n      col=\"grey\",\n      xlab=\"Winning Margin\", main=\"\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nA histogram of the AFL 2010 winning margin data (the afl.margins variable). As you might expect, the larger the margin, the less frequently you tend to see it.\n\n\n\n\n\n\nDrawing pictures of the data, as I did in Figure @ref(fig:histogram1) is an excellent way to convey the “gist” of what the data is trying to tell you, it’s often extremely useful to try to condense the data into a few simple “summary” statistics. In most situations, the first thing that you’ll want to calculate is a measure of central tendency. That is, you’d like to know something about the “average” or “middle” of your data lies. The two most commonly used measures are the mean, median and mode; occasionally people will also report a trimmed mean. I’ll explain each of these in turn, and then discuss when each of them is useful.\n\n\nThe mean of a set of observations is just a normal, old-fashioned average: add all of the values up, and then divide by the total number of values. The first five AFL margins were 56, 31, 56, 8 and 32, so the mean of these observations is just: \\[\n\\frac{56 + 31 + 56 + 8 + 32}{5} = \\frac{183}{5} = 36.60\n\\] Of course, this definition of the mean isn’t news to anyone: averages (i.e., means) are used so often in everyday life that this is pretty familiar stuff. However, since the concept of a mean is something that everyone already understands, I’ll use this as an excuse to start introducing some of the mathematical notation that statisticians use to describe this calculation, and talk about how the calculations would be done in R.\nThe first piece of notation to introduce is \\(N\\), which we’ll use to refer to the number of observations that we’re averaging (in this case \\(N = 5\\)). Next, we need to attach a label to the observations themselves. It’s traditional to use \\(X\\) for this, and to use subscripts to indicate which observation we’re actually talking about. That is, we’ll use \\(X_1\\) to refer to the first observation, \\(X_2\\) to refer to the second observation, and so on, all the way up to \\(X_N\\) for the last one. Or, to say the same thing in a slightly more abstract way, we use \\(X_i\\) to refer to the \\(i\\)-th observation. Just to make sure we’re clear on the notation, the following table lists the 5 observations in the afl.margins variable, along with the mathematical symbol used to refer to it, and the actual value that the observation corresponds to:\n\n\n\n\n\nthe observation\nits symbol\nthe observed value\n\n\n\n\nwinning margin, game 1\n\\(X_1\\)\n56 points\n\n\nwinning margin, game 2\n\\(X_2\\)\n31 points\n\n\nwinning margin, game 3\n\\(X_3\\)\n56 points\n\n\nwinning margin, game 4\n\\(X_4\\)\n8 points\n\n\nwinning margin, game 5\n\\(X_5\\)\n32 points\n\n\n\n\n\nOkay, now let’s try to write a formula for the mean. By tradition, we use \\(\\bar{X}\\) as the notation for the mean. So the calculation for the mean could be expressed using the following formula: \\[\n\\bar{X} = \\frac{X_1 + X_2 + ... + X_{N-1} + X_N}{N}\n\\] This formula is entirely correct, but it’s terribly long, so we make use of the summation symbol \\(\\scriptstyle\\sum\\) to shorten it.2 If I want to add up the first five observations, I could write out the sum the long way, \\(X_1 + X_2 + X_3 + X_4 +X_5\\) or I could use the summation symbol to shorten it to this: \\[\n\\sum_{i=1}^5 X_i\n\\] Taken literally, this could be read as “the sum, taken over all \\(i\\) values from 1 to 5, of the value \\(X_i\\)”. But basically, what it means is “add up the first five observations”. In any case, we can use this notation to write out the formula for the mean, which looks like this: \\[\n\\bar{X} = \\frac{1}{N} \\sum_{i=1}^N X_i\n\\]\nIn all honesty, I can’t imagine that all this mathematical notation helps clarify the concept of the mean at all. In fact, it’s really just a fancy way of writing out the same thing I said in words: add all the values up, and then divide by the total number of items. However, that’s not really the reason I went into all that detail. My goal was to try to make sure that everyone reading this book is clear on the notation that we’ll be using throughout the book: \\(\\bar{X}\\) for the mean, \\(\\scriptstyle\\sum\\) for the idea of summation, \\(X_i\\) for the \\(i\\)th observation, and \\(N\\) for the total number of observations. We’re going to be re-using these symbols a fair bit, so it’s important that you understand them well enough to be able to “read” the equations, and to be able to see that it’s just saying “add up lots of things and then divide by another thing”.\n\n\n\nOkay that’s the maths, how do we get the magic computing box to do the work for us? If you really wanted to, you could do this calculation directly in R. For the first 5 AFL scores, do this just by typing it in as if R were a calculator…\n\n(56 + 31 + 56 + 8 + 32) / 5\n\n[1] 36.6\n\n\n… in which case R outputs the answer 36.6, just as if it were a calculator. However, that’s not the only way to do the calculations, and when the number of observations starts to become large, it’s easily the most tedious. Besides, in almost every real world scenario, you’ve already got the actual numbers stored in a variable of some kind, just like we have with the afl.margins variable. Under those circumstances, what you want is a function that will just add up all the values stored in a numeric vector. That’s what the sum() function does. If we want to add up all 176 winning margins in the data set, we can do so using the following command:3\n\nsum( afl.margins )\n\n[1] 6213\n\n\nIf we only want the sum of the first five observations, then we can use square brackets to pull out only the first five elements of the vector. So the command would now be:\n\nsum( afl.margins[1:5] )\n\n[1] 183\n\n\nTo calculate the mean, we now tell R to divide the output of this summation by five, so the command that we need to type now becomes the following:\n\nsum( afl.margins[1:5] ) / 5\n\n[1] 36.6\n\n\nAlthough it’s pretty easy to calculate the mean using the sum() function, we can do it in an even easier way, since R also provides us with the mean() function. To calculate the mean for all 176 games, we would use the following command:\n\nmean( x = afl.margins )\n\n[1] 35.30114\n\n\nHowever, since x is the first argument to the function, I could have omitted the argument name. In any case, just to show you that there’s nothing funny going on, here’s what we would do to calculate the mean for the first five observations:\n\nmean( afl.margins[1:5] )\n\n[1] 36.6\n\n\nAs you can see, this gives exactly the same answers as the previous calculations.\n\n\n\nThe second measure of central tendency that people use a lot is the median, and it’s even easier to describe than the mean. The median of a set of observations is just the middle value. As before let’s imagine we were interested only in the first 5 AFL winning margins: 56, 31, 56, 8 and 32. To figure out the median, we sort these numbers into ascending order: \\[\n8, 31, \\mathbf{32}, 56, 56\n\\] From inspection, it’s obvious that the median value of these 5 observations is 32, since that’s the middle one in the sorted list (I’ve put it in bold to make it even more obvious). Easy stuff. But what should we do if we were interested in the first 6 games rather than the first 5? Since the sixth game in the season had a winning margin of 14 points, our sorted list is now \\[\n8, 14, \\mathbf{31}, \\mathbf{32}, 56, 56\n\\] and there are two middle numbers, 31 and 32. The median is defined as the average of those two numbers, which is of course 31.5. As before, it’s very tedious to do this by hand when you’ve got lots of numbers. To illustrate this, here’s what happens when you use R to sort all 176 winning margins. First, I’ll use the sort() function (discussed in Chapter @ref(datahandling)) to display the winning margins in increasing numerical order:\n\nsort( x = afl.margins )\n\n  [1]   0   0   1   1   1   1   2   2   3   3   3   3   3   3   3   3   4   4\n [19]   5   6   7   7   8   8   8   8   8   9   9   9   9   9   9  10  10  10\n [37]  10  10  11  11  11  12  12  12  13  14  14  15  16  16  16  16  18  19\n [55]  19  19  19  19  20  20  20  21  21  22  22  22  23  23  23  24  24  25\n [73]  25  26  26  26  26  27  27  28  28  29  29  29  29  29  29  30  31  32\n [91]  32  33  35  35  35  35  36  36  36  36  36  36  37  38  38  38  38  38\n[109]  39  39  40  41  42  43  43  44  44  44  44  44  47  47  47  48  48  48\n[127]  49  49  50  50  50  50  52  52  53  53  54  54  55  55  55  56  56  56\n[145]  57  60  61  61  63  64  65  65  66  67  68  70  71  71  72  73  75  75\n[163]  76  81  82  82  83  84  89  94  95  98 101 104 108 116\n\n\nThe middle values are 30 and 31, so the median winning margin for 2010 was 30.5 points. In real life, of course, no-one actually calculates the median by sorting the data and then looking for the middle value. In real life, we use the median command:\n\nmedian( x = afl.margins )\n\n[1] 30.5\n\n\nwhich outputs the median value of 30.5.\n\n\n\n\n\n\n\n\nAn illustration of the difference between how the mean and the median should be interpreted. The mean is basically the “centre of gravity” of the data set: if you imagine that the histogram of the data is a solid object, then the point on which you could balance it (as if on a see-saw) is the mean. In contrast, the median is the middle observation. Half of the observations are smaller, and half of the observations are larger.\n\n\n\n\nKnowing how to calculate means and medians is only a part of the story. You also need to understand what each one is saying about the data, and what that implies for when you should use each one. This is illustrated in Figure @ref(fig:meanmedian) the mean is kind of like the “centre of gravity” of the data set, whereas the median is the “middle value” in the data. What this implies, as far as which one you should use, depends a little on what type of data you’ve got and what you’re trying to achieve. As a rough guide:\n\nIf your data are nominal scale, you probably shouldn’t be using either the mean or the median. Both the mean and the median rely on the idea that the numbers assigned to values are meaningful. If the numbering scheme is arbitrary, then it’s probably best to use the mode (Section @ref(mode)) instead.\nIf your data are ordinal scale, you’re more likely to want to use the median than the mean. The median only makes use of the order information in your data (i.e., which numbers are bigger), but doesn’t depend on the precise numbers involved. That’s exactly the situation that applies when your data are ordinal scale. The mean, on the other hand, makes use of the precise numeric values assigned to the observations, so it’s not really appropriate for ordinal data.\nFor interval and ratio scale data, either one is generally acceptable. Which one you pick depends a bit on what you’re trying to achieve. The mean has the advantage that it uses all the information in the data (which is useful when you don’t have a lot of data), but it’s very sensitive to extreme values, as we’ll see in Section @ref(trimmedmean).\n\nLet’s expand on that last part a little. One consequence is that there’s systematic differences between the mean and the median when the histogram is asymmetric (skewed; see Section @ref(skewandkurtosis)). This is illustrated in Figure @ref(fig:meanmedian) notice that the median (right hand side) is located closer to the “body” of the histogram, whereas the mean (left hand side) gets dragged towards the “tail” (where the extreme values are). To give a concrete example, suppose Bob (income $50,000), Kate (income $60,000) and Jane (income $65,000) are sitting at a table: the average income at the table is $58,333 and the median income is $60,000. Then Bill sits down with them (income $100,000,000). The average income has now jumped to $25,043,750 but the median rises only to $62,500. If you’re interested in looking at the overall income at the table, the mean might be the right answer; but if you’re interested in what counts as a typical income at the table, the median would be a better choice here.\n\n\n\nTo try to get a sense of why you need to pay attention to the differences between the mean and the median, let’s consider a real life example. Since I tend to mock journalists for their poor scientific and statistical knowledge, I should give credit where credit is due. This is from an excellent article on the ABC news website4 24 September, 2010:\n\nSenior Commonwealth Bank executives have travelled the world in the past couple of weeks with a presentation showing how Australian house prices, and the key price to income ratios, compare favourably with similar countries. “Housing affordability has actually been going sideways for the last five to six years,” said Craig James, the chief economist of the bank’s trading arm, CommSec.\n\nThis probably comes as a huge surprise to anyone with a mortgage, or who wants a mortgage, or pays rent, or isn’t completely oblivious to what’s been going on in the Australian housing market over the last several years. Back to the article:\n\nCBA has waged its war against what it believes are housing doomsayers with graphs, numbers and international comparisons. In its presentation, the bank rejects arguments that Australia’s housing is relatively expensive compared to incomes. It says Australia’s house price to household income ratio of 5.6 in the major cities, and 4.3 nationwide, is comparable to many other developed nations. It says San Francisco and New York have ratios of 7, Auckland’s is 6.7, and Vancouver comes in at 9.3.\n\nMore excellent news! Except, the article goes on to make the observation that…\n\nMany analysts say that has led the bank to use misleading figures and comparisons. If you go to page four of CBA’s presentation and read the source information at the bottom of the graph and table, you would notice there is an additional source on the international comparison – Demographia. However, if the Commonwealth Bank had also used Demographia’s analysis of Australia’s house price to income ratio, it would have come up with a figure closer to 9 rather than 5.6 or 4.3\n\nThat’s, um, a rather serious discrepancy. One group of people say 9, another says 4-5. Should we just split the difference, and say the truth lies somewhere in between? Absolutely not: this is a situation where there is a right answer and a wrong answer. Demographia are correct, and the Commonwealth Bank is incorrect. As the article points out\n\n[An] obvious problem with the Commonwealth Bank’s domestic price to income figures is they compare average incomes with median house prices (unlike the Demographia figures that compare median incomes to median prices). The median is the mid-point, effectively cutting out the highs and lows, and that means the average is generally higher when it comes to incomes and asset prices, because it includes the earnings of Australia’s wealthiest people. To put it another way: the Commonwealth Bank’s figures count Ralph Norris’ multi-million dollar pay packet on the income side, but not his (no doubt) very expensive house in the property price figures, thus understating the house price to income ratio for middle-income Australians.\n\nCouldn’t have put it better myself. The way that Demographia calculated the ratio is the right thing to do. The way that the Bank did it is incorrect. As for why an extremely quantitatively sophisticated organisation such as a major bank made such an elementary mistake, well… I can’t say for sure, since I have no special insight into their thinking, but the article itself does happen to mention the following facts, which may or may not be relevant:\n\n[As] Australia’s largest home lender, the Commonwealth Bank has one of the biggest vested interests in house prices rising. It effectively owns a massive swathe of Australian housing as security for its home loans as well as many small business loans.\n\nMy, my.\n\n\n\nOne of the fundamental rules of applied statistics is that the data are messy. Real life is never simple, and so the data sets that you obtain are never as straightforward as the statistical theory says.5 This can have awkward consequences. To illustrate, consider this rather strange looking data set: \\[\n-100,2,3,4,5,6,7,8,9,10\n\\] If you were to observe this in a real life data set, you’d probably suspect that something funny was going on with the \\(-100\\) value. It’s probably an outlier, a value that doesn’t really belong with the others. You might consider removing it from the data set entirely, and in this particular case I’d probably agree with that course of action. In real life, however, you don’t always get such cut-and-dried examples. For instance, you might get this instead: \\[\n-15,2,3,4,5,6,7,8,9,12\n\\] The \\(-15\\) looks a bit suspicious, but not anywhere near as much as that \\(-100\\) did. In this case, it’s a little trickier. It might be a legitimate observation, it might not.\nWhen faced with a situation where some of the most extreme-valued observations might not be quite trustworthy, the mean is not necessarily a good measure of central tendency. It is highly sensitive to one or two extreme values, and is thus not considered to be a robust measure. One remedy that we’ve seen is to use the median. A more general solution is to use a “trimmed mean”. To calculate a trimmed mean, what you do is “discard” the most extreme examples on both ends (i.e., the largest and the smallest), and then take the mean of everything else. The goal is to preserve the best characteristics of the mean and the median: just like a median, you aren’t highly influenced by extreme outliers, but like the mean, you “use” more than one of the observations. Generally, we describe a trimmed mean in terms of the percentage of observation on either side that are discarded. So, for instance, a 10% trimmed mean discards the largest 10% of the observations and the smallest 10% of the observations, and then takes the mean of the remaining 80% of the observations. Not surprisingly, the 0% trimmed mean is just the regular mean, and the 50% trimmed mean is the median. In that sense, trimmed means provide a whole family of central tendency measures that span the range from the mean to the median.\nFor our toy example above, we have 10 observations, and so a 10% trimmed mean is calculated by ignoring the largest value (i.e., 12) and the smallest value (i.e., -15) and taking the mean of the remaining values. First, let’s enter the data\n\ndataset &lt;- c( -15,2,3,4,5,6,7,8,9,12 )\n\nNext, let’s calculate means and medians:\n\nmean( x = dataset )\n\n[1] 4.1\n\nmedian( x = dataset )\n\n[1] 5.5\n\n\nThat’s a fairly substantial difference, but I’m tempted to think that the mean is being influenced a bit too much by the extreme values at either end of the data set, especially the \\(-15\\) one. So let’s just try trimming the mean a bit. If I take a 10% trimmed mean, we’ll drop the extreme values on either side, and take the mean of the rest:\n\nmean( x = dataset, trim = .1)\n\n[1] 5.5\n\n\nwhich in this case gives exactly the same answer as the median. Note that, to get a 10% trimmed mean you write trim = .1, not trim = 10. In any case, let’s finish up by calculating the 5% trimmed mean for the afl.margins data,\n\nmean( x = afl.margins, trim = .05)  \n\n[1] 33.75\n\n\n\n\n\nThe mode of a sample is very simple: it is the value that occurs most frequently. To illustrate the mode using the AFL data, let’s examine a different aspect to the data set. Who has played in the most finals? The afl.finalists variable is a factor that contains the name of every team that played in any AFL final from 1987-2010, so let’s have a look at it. To do this we will use the head() command. head() is useful when you’re working with a data.frame with a lot of rows since you can use it to tell you how many rows to return. There have been a lot of finals in this period so printing afl.finalists using print(afl.finalists) will just fill us the screen. The command below tells R we just want the first 25 rows of the data.frame.\n\nhead(afl.finalists, 25)\n\n [1] Hawthorn    Melbourne   Carlton     Melbourne   Hawthorn    Carlton    \n [7] Melbourne   Carlton     Hawthorn    Melbourne   Melbourne   Hawthorn   \n[13] Melbourne   Essendon    Hawthorn    Geelong     Geelong     Hawthorn   \n[19] Collingwood Melbourne   Collingwood West Coast  Collingwood Essendon   \n[25] Collingwood\n17 Levels: Adelaide Brisbane Carlton Collingwood Essendon Fitzroy ... Western Bulldogs\n\n\nThere are actually 400 entries (aren’t you glad we didn’t print them all?). We could read through all 400, and count the number of occasions on which each team name appears in our list of finalists, thereby producing a frequency table. However, that would be mindless and boring: exactly the sort of task that computers are great at. So let’s use the table() function (discussed in more detail in Section @ref(freqtables)) to do this task for us:\n\ntable( afl.finalists )\n\nafl.finalists\n        Adelaide         Brisbane          Carlton      Collingwood \n              26               25               26               28 \n        Essendon          Fitzroy        Fremantle          Geelong \n              32                0                6               39 \n        Hawthorn        Melbourne  North Melbourne    Port Adelaide \n              27               28               28               17 \n        Richmond         St Kilda           Sydney       West Coast \n               6               24               26               38 \nWestern Bulldogs \n              24 \n\n\nNow that we have our frequency table, we can just look at it and see that, over the 24 years for which we have data, Geelong has played in more finals than any other team. Thus, the mode of the finalists data is \"Geelong\". The core packages in R don’t have a function for calculating the mode6. However, I’ve included a function in the lsr package that does this. The function is called modeOf(), and here’s how you use it:\n\nmodeOf( x = afl.finalists )\n\n[1] \"Geelong\"\n\n\nThere’s also a function called maxFreq() that tells you what the modal frequency is. If we apply this function to our finalists data, we obtain the following:\n\nmaxFreq( x = afl.finalists )\n\n[1] 39\n\n\nTaken together, we observe that Geelong (39 finals) played in more finals than any other team during the 1987-2010 period.\nOne last point to make with respect to the mode. While it’s generally true that the mode is most often calculated when you have nominal scale data (because means and medians are useless for those sorts of variables), there are some situations in which you really do want to know the mode of an ordinal, interval or ratio scale variable. For instance, let’s go back to thinking about our afl.margins variable. This variable is clearly ratio scale (if it’s not clear to you, it may help to re-read Section @ref(scales)), and so in most situations the mean or the median is the measure of central tendency that you want. But consider this scenario… a friend of yours is offering a bet. They pick a football game at random, and (without knowing who is playing) you have to guess the exact margin. If you guess correctly, you win $50. If you don’t, you lose $1. There are no consolation prizes for “almost” getting the right answer. You have to guess exactly the right margin7 For this bet, the mean and the median are completely useless to you. It is the mode that you should bet on. So, we calculate this modal value\n\nmodeOf( x = afl.margins )\n\n[1] 3\n\nmaxFreq( x = afl.margins )\n\n[1] 8\n\n\nSo the 2010 data suggest you should bet on a 3 point margin, and since this was observed in 8 of the 176 game (4.5% of games) the odds are firmly in your favour.\n\n\n\n\nThe statistics that we’ve discussed so far all relate to central tendency. That is, they all talk about which values are “in the middle” or “popular” in the data. However, central tendency is not the only type of summary statistic that we want to calculate. The second thing that we really want is a measure of the variability of the data. That is, how “spread out” are the data? How “far” away from the mean or median do the observed values tend to be? For now, let’s assume that the data are interval or ratio scale, so we’ll continue to use the afl.margins data. We’ll use this data to discuss several different measures of spread, each with different strengths and weaknesses.\n\n\nThe range of a variable is very simple: it’s the biggest value minus the smallest value. For the AFL winning margins data, the maximum value is 116, and the minimum value is 0. We can calculate these values in R using the max() and min() functions:\n\nmax( afl.margins )\n\n[1] 116\n\nmin( afl.margins )\n\n[1] 0\n\n\nwhere I’ve omitted the output because it’s not interesting. The other possibility is to use the range() function; which outputs both the minimum value and the maximum value in a vector, like this:\n\nrange( afl.margins )\n\n[1]   0 116\n\n\nAlthough the range is the simplest way to quantify the notion of “variability”, it’s one of the worst. Recall from our discussion of the mean that we want our summary measure to be robust. If the data set has one or two extremely bad values in it, we’d like our statistics not to be unduly influenced by these cases. If we look once again at our toy example of a data set containing very extreme outliers… \\[\n-100,2,3,4,5,6,7,8,9,10\n\\] … it is clear that the range is not robust, since this has a range of 110, but if the outlier were removed we would have a range of only 8.\n\n\n\nThe interquartile range (IQR) is like the range, but instead of calculating the difference between the biggest and smallest value, it calculates the difference between the 25th quantile and the 75th quantile. Probably you already know what a quantile is (they’re more commonly called percentiles), but if not: the 10th percentile of a data set is the smallest number \\(x\\) such that 10% of the data is less than \\(x\\). In fact, we’ve already come across the idea: the median of a data set is its 50th quantile / percentile! R actually provides you with a way of calculating quantiles, using the (surprise, surprise) quantile() function. Let’s use it to calculate the median AFL winning margin:\n\nquantile( x = afl.margins, probs = .5)\n\n 50% \n30.5 \n\n\nAnd not surprisingly, this agrees with the answer that we saw earlier with the median() function. Now, we can actually input lots of quantiles at once, by specifying a vector for the probs argument. So lets do that, and get the 25th and 75th percentile:\n\nquantile( x = afl.margins, probs = c(.25,.75) )\n\n  25%   75% \n12.75 50.50 \n\n\nAnd, by noting that \\(50.5 - 12.75 = 37.75\\), we can see that the interquartile range for the 2010 AFL winning margins data is 37.75. Of course, that seems like too much work to do all that typing, so R has a built in function called IQR() that we can use:\n\nIQR( x = afl.margins )\n\n[1] 37.75\n\n\nWhile it’s obvious how to interpret the range, it’s a little less obvious how to interpret the IQR. The simplest way to think about it is like this: the interquartile range is the range spanned by the “middle half” of the data. That is, one quarter of the data falls below the 25th percentile, one quarter of the data is above the 75th percentile, leaving the “middle half” of the data lying in between the two. And the IQR is the range covered by that middle half.\n\n\n\nThe two measures we’ve looked at so far, the range and the interquartile range, both rely on the idea that we can measure the spread of the data by looking at the quantiles of the data. However, this isn’t the only way to think about the problem. A different approach is to select a meaningful reference point (usually the mean or the median) and then report the “typical” deviations from that reference point. What do we mean by “typical” deviation? Usually, the mean or median value of these deviations! In practice, this leads to two different measures, the “mean absolute deviation (from the mean)” and the “median absolute deviation (from the median)”. From what I’ve read, the measure based on the median seems to be used in statistics, and does seem to be the better of the two, but to be honest I don’t think I’ve seen it used much in psychology. The measure based on the mean does occasionally show up in psychology though. In this section I’ll talk about the first one, and I’ll come back to talk about the second one later.\nSince the previous paragraph might sound a little abstract, let’s go through the mean absolute deviation from the mean a little more slowly. One useful thing about this measure is that the name actually tells you exactly how to calculate it. Let’s think about our AFL winning margins data, and once again we’ll start by pretending that there’s only 5 games in total, with winning margins of 56, 31, 56, 8 and 32. Since our calculations rely on an examination of the deviation from some reference point (in this case the mean), the first thing we need to calculate is the mean, \\(\\bar{X}\\). For these five observations, our mean is \\(\\bar{X} = 36.6\\). The next step is to convert each of our observations \\(X_i\\) into a deviation score. We do this by calculating the difference between the observation \\(X_i\\) and the mean \\(\\bar{X}\\). That is, the deviation score is defined to be \\(X_i - \\bar{X}\\). For the first observation in our sample, this is equal to \\(56 - 36.6 = 19.4\\). Okay, that’s simple enough. The next step in the process is to convert these deviations to absolute deviations. As we discussed earlier when talking about the abs() function in R (Section @ref(usingfunctions)), we do this by converting any negative values to positive ones. Mathematically, we would denote the absolute value of \\(-3\\) as \\(|-3|\\), and so we say that \\(|-3| = 3\\). We use the absolute value function here because we don’t really care whether the value is higher than the mean or lower than the mean, we’re just interested in how close it is to the mean. To help make this process as obvious as possible, the table below shows these calculations for all five observations:\n\n\n\n\n\nthe observation\nits symbol\nthe observed value\n\n\n\n\nwinning margin, game 1\n\\(X_1\\)\n56 points\n\n\nwinning margin, game 2\n\\(X_2\\)\n31 points\n\n\nwinning margin, game 3\n\\(X_3\\)\n56 points\n\n\nwinning margin, game 4\n\\(X_4\\)\n8 points\n\n\nwinning margin, game 5\n\\(X_5\\)\n32 points\n\n\n\n\n\nNow that we have calculated the absolute deviation score for every observation in the data set, all that we have to do to calculate the mean of these scores. Let’s do that: \\[\n\\frac{19.4 + 5.6 + 19.4 + 28.6 + 4.6}{5} = 15.52\n\\] And we’re done. The mean absolute deviation for these five scores is 15.52.\nHowever, while our calculations for this little example are at an end, we do have a couple of things left to talk about. Firstly, we should really try to write down a proper mathematical formula. But in order do to this I need some mathematical notation to refer to the mean absolute deviation. Irritatingly, “mean absolute deviation” and “median absolute deviation” have the same acronym (MAD), which leads to a certain amount of ambiguity, and since R tends to use MAD to refer to the median absolute deviation, I’d better come up with something different for the mean absolute deviation. Sigh. What I’ll do is use AAD instead, short for average absolute deviation. Now that we have some unambiguous notation, here’s the formula that describes what we just calculated: \\[\n\\mbox{}(X) = \\frac{1}{N} \\sum_{i = 1}^N |X_i - \\bar{X}|\n\\]\nThe last thing we need to talk about is how to calculate AAD in R. One possibility would be to do everything using low level commands, laboriously following the same steps that I used when describing the calculations above. However, that’s pretty tedious. You’d end up with a series of commands that might look like this:\n\nX &lt;- c(56, 31,56,8,32)   # enter the data\nX.bar &lt;- mean( X )       # step 1. the mean of the data\nAD &lt;- abs( X - X.bar )   # step 2. the absolute deviations from the mean\nAAD &lt;- mean( AD )        # step 3. the mean absolute deviations\nprint( AAD )             # print the results\n\n[1] 15.52\n\n\nEach of those commands is pretty simple, but there’s just too many of them. And because I find that to be too much typing, the lsr package has a very simple function called aad() that does the calculations for you. If we apply the aad() function to our data, we get this:\n\nlibrary(lsr)\naad( X )\n\n[1] 15.52\n\n\nNo suprises there.\n\n\n\nAlthough the mean absolute deviation measure has its uses, it’s not the best measure of variability to use. From a purely mathematical perspective, there are some solid reasons to prefer squared deviations rather than absolute deviations. If we do that, we obtain a measure is called the variance, which has a lot of really nice statistical properties that I’m going to ignore,8 and one massive psychological flaw that I’m going to make a big deal out of in a moment. The variance of a data set \\(X\\) is sometimes written as \\(\\mbox{Var}(X)\\), but it’s more commonly denoted \\(s^2\\) (the reason for this will become clearer shortly). The formula that we use to calculate the variance of a set of observations is as follows: \\[\n\\mbox{Var}(X) = \\frac{1}{N} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2\n\\] \\[\\mbox{Var}(X) = \\frac{\\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2}{N}\\] As you can see, it’s basically the same formula that we used to calculate the mean absolute deviation, except that instead of using “absolute deviations” we use “squared deviations”. It is for this reason that the variance is sometimes referred to as the “mean square deviation”.\nNow that we’ve got the basic idea, let’s have a look at a concrete example. Once again, let’s use the first five AFL games as our data. If we follow the same approach that we took last time, we end up with the following table:\n\n\n\nBasic arithmetic operations in R. These five operators are used very frequently throughout the text, so it’s important to be familiar with them at the outset.\n\n\n\n\n\n\n\n\n\nNotation [English]\n\\(i\\) [which game]\n\\(X_i\\) [value]\n\\(X_i - \\bar{X}\\) [deviation from mean]\n\\((X_i - \\bar{X})^2\\) [absolute deviation]\n\n\n\n\n\n1\n56\n19.4\n376.36\n\n\n\n2\n31\n-5.6\n31.36\n\n\n\n3\n56\n19.4\n376.36\n\n\n\n4\n8\n-28.6\n817.96\n\n\n\n5\n32\n-4.6\n21.16\n\n\n\n\n\nThat last column contains all of our squared deviations, so all we have to do is average them. If we do that by typing all the numbers into R by hand…\n\n( 376.36 + 31.36 + 376.36 + 817.96 + 21.16 ) / 5\n\n[1] 324.64\n\n\n… we end up with a variance of 324.64. Exciting, isn’t it? For the moment, let’s ignore the burning question that you’re all probably thinking (i.e., what the heck does a variance of 324.64 actually mean?) and instead talk a bit more about how to do the calculations in R, because this will reveal something very weird.\nAs always, we want to avoid having to type in a whole lot of numbers ourselves. And as it happens, we have the vector X lying around, which we created in the previous section. With this in mind, we can calculate the variance of X by using the following command,\n\nmean( (X - mean(X) )^2)\n\n[1] 324.64\n\n\nand as usual we get the same answer as the one that we got when we did everything by hand. However, I still think that this is too much typing. Fortunately, R has a built in function called var() which does calculate variances. So we could also do this…\n\nvar(X)\n\n[1] 405.8\n\n\nand you get the same… no, wait… you get a completely different answer. That’s just weird. Is R broken? Is this a typo? Is Dan an idiot?\nAs it happens, the answer is no.9 It’s not a typo, and R is not making a mistake. To get a feel for what’s happening, let’s stop using the tiny data set containing only 5 data points, and switch to the full set of 176 games that we’ve got stored in our afl.margins vector. First, let’s calculate the variance by using the formula that I described above:\n\nmean( (afl.margins - mean(afl.margins) )^2)\n\n[1] 675.9718\n\n\nNow let’s use the var() function:\n\nvar( afl.margins )\n\n[1] 679.8345\n\n\nHm. These two numbers are very similar this time. That seems like too much of a coincidence to be a mistake. And of course it isn’t a mistake. In fact, it’s very simple to explain what R is doing here, but slightly trickier to explain why R is doing it. So let’s start with the “what”. What R is doing is evaluating a slightly different formula to the one I showed you above. Instead of averaging the squared deviations, which requires you to divide by the number of data points \\(N\\), R has chosen to divide by \\(N-1\\). In other words, the formula that R is using is this one\n\\[\n\\frac{1}{N-1} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2\n\\] It’s easy enough to verify that this is what’s happening, as the following command illustrates:\n\nsum( (X-mean(X))^2 ) / 4\n\n[1] 405.8\n\n\nThis is the same answer that R gave us originally when we calculated var(X) originally. So that’s the what. The real question is why R is dividing by \\(N-1\\) and not by \\(N\\). After all, the variance is supposed to be the mean squared deviation, right? So shouldn’t we be dividing by \\(N\\), the actual number of observations in the sample? Well, yes, we should. However, as we’ll discuss in Chapter @ref(estimation), there’s a subtle distinction between “describing a sample” and “making guesses about the population from which the sample came”. Up to this point, it’s been a distinction without a difference. Regardless of whether you’re describing a sample or drawing inferences about the population, the mean is calculated exactly the same way. Not so for the variance, or the standard deviation, or for many other measures besides. What I outlined to you initially (i.e., take the actual average, and thus divide by \\(N\\)) assumes that you literally intend to calculate the variance of the sample. Most of the time, however, you’re not terribly interested in the sample in and of itself. Rather, the sample exists to tell you something about the world. If so, you’re actually starting to move away from calculating a “sample statistic”, and towards the idea of estimating a “population parameter”. However, I’m getting ahead of myself. For now, let’s just take it on faith that R knows what it’s doing, and we’ll revisit the question later on when we talk about estimation in Chapter @ref(estimation).\nOkay, one last thing. This section so far has read a bit like a mystery novel. I’ve shown you how to calculate the variance, described the weird “\\(N-1\\)” thing that R does and hinted at the reason why it’s there, but I haven’t mentioned the single most important thing… how do you interpret the variance? Descriptive statistics are supposed to describe things, after all, and right now the variance is really just a gibberish number. Unfortunately, the reason why I haven’t given you the human-friendly interpretation of the variance is that there really isn’t one. This is the most serious problem with the variance. Although it has some elegant mathematical properties that suggest that it really is a fundamental quantity for expressing variation, it’s completely useless if you want to communicate with an actual human… variances are completely uninterpretable in terms of the original variable! All the numbers have been squared, and they don’t mean anything anymore. This is a huge issue. For instance, according to the table I presented earlier, the margin in game 1 was “376.36 points-squared higher than the average margin”. This is exactly as stupid as it sounds; and so when we calculate a variance of 324.64, we’re in the same situation. I’ve watched a lot of footy games, and never has anyone referred to “points squared”. It’s not a real unit of measurement, and since the variance is expressed in terms of this gibberish unit, it is totally meaningless to a human.\n\n\n\nOkay, suppose that you like the idea of using the variance because of those nice mathematical properties that I haven’t talked about, but – since you’re a human and not a robot – you’d like to have a measure that is expressed in the same units as the data itself (i.e., points, not points-squared). What should you do? The solution to the problem is obvious: take the square root of the variance, known as the standard deviation, also called the “root mean squared deviation”, or RMSD. This solves out problem fairly neatly: while nobody has a clue what “a variance of 324.68 points-squared” really means, it’s much easier to understand “a standard deviation of 18.01 points”, since it’s expressed in the original units. It is traditional to refer to the standard deviation of a sample of data as \\(s\\), though “sd” and “std dev.” are also used at times. Because the standard deviation is equal to the square root of the variance, you probably won’t be surprised to see that the formula is: \\[\ns = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2 }\n\\] and the R function that we use to calculate it is sd(). However, as you might have guessed from our discussion of the variance, what R actually calculates is slightly different to the formula given above. Just like the we saw with the variance, what R calculates is a version that divides by \\(N-1\\) rather than \\(N\\). For reasons that will make sense when we return to this topic in Chapter@refch:estimation I’ll refer to this new quantity as \\(\\hat\\sigma\\) (read as: “sigma hat”), and the formula for this is \\[\n\\hat\\sigma = \\sqrt{ \\frac{1}{N-1} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2 }\n\\] With that in mind, calculating standard deviations in R is simple:\n\nsd( afl.margins ) \n\n[1] 26.07364\n\n\nInterpreting standard deviations is slightly more complex. Because the standard deviation is derived from the variance, and the variance is a quantity that has little to no meaning that makes sense to us humans, the standard deviation doesn’t have a simple interpretation. As a consequence, most of us just rely on a simple rule of thumb: in general, you should expect 68% of the data to fall within 1 standard deviation of the mean, 95% of the data to fall within 2 standard deviation of the mean, and 99.7% of the data to fall within 3 standard deviations of the mean. This rule tends to work pretty well most of the time, but it’s not exact: it’s actually calculated based on an assumption that the histogram is symmetric and “bell shaped”.10 As you can tell from looking at the AFL winning margins histogram in Figure @ref(fig:histogram1), this isn’t exactly true of our data! Even so, the rule is approximately correct. As it turns out, 65.3% of the AFL margins data fall within one standard deviation of the mean. This is shown visually in Figure @ref(fig:aflsd).\n\n\n\n\n\nAn illustration of the standard deviation, applied to the AFL winning margins data. The shaded bars in the histogram show how much of the data fall within one standard deviation of the mean. In this case, 65.3% of the data set lies within this range, which is pretty consistent with the “approximately 68% rule” discussed in the main text.\n\n\n\n\n\n\n\nThe last measure of variability that I want to talk about is the median absolute deviation (MAD). The basic idea behind MAD is very simple, and is pretty much identical to the idea behind the mean absolute deviation (Section @ref(aad)). The difference is that you use the median everywhere. If we were to frame this idea as a pair of R commands, they would look like this:\n\n# mean absolute deviation from the mean:\nmean( abs(afl.margins - mean(afl.margins)) )\n\n[1] 21.10124\n\n# *median* absolute deviation from the *median*:\nmedian( abs(afl.margins - median(afl.margins)) )\n\n[1] 19.5\n\n\nThis has a straightforward interpretation: every observation in the data set lies some distance away from the typical value (the median). So the MAD is an attempt to describe a typical deviation from a typical value in the data set. It wouldn’t be unreasonable to interpret the MAD value of 19.5 for our AFL data by saying something like this:\n\nThe median winning margin in 2010 was 30.5, indicating that a typical game involved a winning margin of about 30 points. However, there was a fair amount of variation from game to game: the MAD value was 19.5, indicating that a typical winning margin would differ from this median value by about 19-20 points.\n\nAs you’d expect, R has a built in function for calculating MAD, and you will be shocked no doubt to hear that it’s called mad(). However, it’s a little bit more complicated than the functions that we’ve been using previously. If you want to use it to calculate MAD in the exact same way that I have described it above, the command that you need to use specifies two arguments: the data set itself x, and a constant that I’ll explain in a moment. For our purposes, the constant is 1, so our command becomes\n\nmad( x = afl.margins, constant = 1 )\n\n[1] 19.5\n\n\nApart from the weirdness of having to type that constant = 1 part, this is pretty straightforward.\nOkay, so what exactly is this constant = 1 argument? I won’t go into all the details here, but here’s the gist. Although the “raw” MAD value that I’ve described above is completely interpretable on its own terms, that’s not actually how it’s used in a lot of real world contexts. Instead, what happens a lot is that the researcher actually wants to calculate the standard deviation. However, in the same way that the mean is very sensitive to extreme values, the standard deviation is vulnerable to the exact same issue. So, in much the same way that people sometimes use the median as a “robust” way of calculating “something that is like the mean”, it’s not uncommon to use MAD as a method for calculating “something that is like the standard deviation”. Unfortunately, the raw MAD value doesn’t do this. Our raw MAD value is 19.5, and our standard deviation was 26.07. However, what some clever person has shown is that, under certain assumptions11, you can multiply the raw MAD value by 1.4826 and obtain a number that is directly comparable to the standard deviation. As a consequence, the default value of constant is 1.4826, and so when you use the mad() command without manually setting a value, here’s what you get:\n\nmad( afl.margins )\n\n[1] 28.9107\n\n\nI should point out, though, that if you want to use this “corrected” MAD value as a robust version of the standard deviation, you really are relying on the assumption that the data are (or at least, are “supposed to be” in some sense) symmetric and basically shaped like a bell curve. That’s really not true for our afl.margins data, so in this case I wouldn’t try to use the MAD value this way.\n\n\n\nWe’ve discussed quite a few measures of spread (range, IQR, MAD, variance and standard deviation), and hinted at their strengths and weaknesses. Here’s a quick summary:\n\nRange. Gives you the full spread of the data. It’s very vulnerable to outliers, and as a consequence it isn’t often used unless you have good reasons to care about the extremes in the data.\nInterquartile range. Tells you where the “middle half” of the data sits. It’s pretty robust, and complements the median nicely. This is used a lot.\nMean absolute deviation. Tells you how far “on average” the observations are from the mean. It’s very interpretable, but has a few minor issues (not discussed here) that make it less attractive to statisticians than the standard deviation. Used sometimes, but not often.\nVariance. Tells you the average squared deviation from the mean. It’s mathematically elegant, and is probably the “right” way to describe variation around the mean, but it’s completely uninterpretable because it doesn’t use the same units as the data. Almost never used except as a mathematical tool; but it’s buried “under the hood” of a very large number of statistical tools.\nStandard deviation. This is the square root of the variance. It’s fairly elegant mathematically, and it’s expressed in the same units as the data so it can be interpreted pretty well. In situations where the mean is the measure of central tendency, this is the default. This is by far the most popular measure of variation.\nMedian absolute deviation. The typical (i.e., median) deviation from the median value. In the raw form it’s simple and interpretable; in the corrected form it’s a robust way to estimate the standard deviation, for some kinds of data sets. Not used very often, but it does get reported sometimes.\n\nIn short, the IQR and the standard deviation are easily the two most common measures used to report the variability of the data; but there are situations in which the others are used. I’ve described all of them in this book because there’s a fair chance you’ll run into most of these somewhere.\n\n\n\n\nThere are two more descriptive statistics that you will sometimes see reported in the psychological literature, known as skew and kurtosis. In practice, neither one is used anywhere near as frequently as the measures of central tendency and variability that we’ve been talking about. Skew is pretty important, so you do see it mentioned a fair bit; but I’ve actually never seen kurtosis reported in a scientific article to date.\n\n\n[1] -0.9239273\n\n\n[1] -0.005588142\n\n\n\n\n\nAn illustration of skewness. On the left we have a negatively skewed data set (skewness \\(= -.93\\)), in the middle we have a data set with no skew (technically, skewness \\(= -.006\\)), and on the right we have a positively skewed data set (skewness \\(= .93\\)).\n\n\n\n\n[1] 0.9254561\n\n\nSince it’s the more interesting of the two, let’s start by talking about the skewness. Skewness is basically a measure of asymmetry, and the easiest way to explain it is by drawing some pictures. As Figure @ref(fig:skewness) illustrates, if the data tend to have a lot of extreme small values (i.e., the lower tail is “longer” than the upper tail) and not so many extremely large values (left panel), then we say that the data are negatively skewed. On the other hand, if there are more extremely large values than extremely small ones (right panel) we say that the data are positively skewed. That’s the qualitative idea behind skewness. The actual formula for the skewness of a data set is as follows \\[\n\\mbox{skewness}(X) = \\frac{1}{N \\hat{\\sigma}^3} \\sum_{i=1}^N (X_i - \\bar{X})^3\n\\] where \\(N\\) is the number of observations, \\(\\bar{X}\\) is the sample mean, and \\(\\hat{\\sigma}\\) is the standard deviation (the “divide by \\(N-1\\)” version, that is). Perhaps more helpfully, it might be useful to point out that the psych package contains a skew() function that you can use to calculate skewness. So if we wanted to use this function to calculate the skewness of the afl.margins data, we’d first need to load the package\n\nlibrary( psych )\n\nwhich now makes it possible to use the following command:\n\nskew( x = afl.margins )\n\n[1] 0.7671555\n\n\nNot surprisingly, it turns out that the AFL winning margins data is fairly skewed.\nThe final measure that is sometimes referred to, though very rarely in practice, is the kurtosis of a data set. Put simply, kurtosis is a measure of the “pointiness” of a data set, as illustrated in Figure @ref(fig:kurtosis).\n\n\n[1] -0.9521099\n\n\n[1] 0.01635112\n\n\n\n\n\nAn illustration of kurtosis. On the left, we have a “platykurtic” data set (kurtosis = \\(-.95\\)), meaning that the data set is “too flat”. In the middle we have a “mesokurtic” data set (kurtosis is almost exactly 0), which means that the pointiness of the data is just about right. Finally, on the right, we have a “leptokurtic” data set (kurtosis \\(= 2.12\\)) indicating that the data set is “too pointy”. Note that kurtosis is measured with respect to a normal curve (black line)\n\n\n\n\n[1] 2.065825\n\n\nBy convention, we say that the “normal curve” (black lines) has zero kurtosis, so the pointiness of a data set is assessed relative to this curve. In this Figure, the data on the left are not pointy enough, so the kurtosis is negative and we call the data platykurtic. The data on the right are too pointy, so the kurtosis is positive and we say that the data is leptokurtic. But the data in the middle are just pointy enough, so we say that it is mesokurtic and has kurtosis zero. This is summarised in the table below:\n\n\n\n\n\ninformal term\ntechnical name\nkurtosis value\n\n\n\n\ntoo flat\nplatykurtic\nnegative\n\n\njust pointy enough\nmesokurtic\nzero\n\n\ntoo pointy\nleptokurtic\npositive\n\n\n\n\n\nThe equation for kurtosis is pretty similar in spirit to the formulas we’ve seen already for the variance and the skewness; except that where the variance involved squared deviations and the skewness involved cubed deviations, the kurtosis involves raising the deviations to the fourth power:12 \\[\n\\mbox{kurtosis}(X) = \\frac{1}{N \\hat\\sigma^4} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^4  - 3\n\\] I know, it’s not terribly interesting to me either. More to the point, the psych package has a function called kurtosi() that you can use to calculate the kurtosis of your data. For instance, if we were to do this for the AFL margins,\n\nkurtosi( x = afl.margins )\n\n[1] 0.02962633\n\n\nwe discover that the AFL winning margins data are just pointy enough.\n\n\n\nUp to this point in the chapter I’ve explained several different summary statistics that are commonly used when analysing data, along with specific functions that you can use in R to calculate each one. However, it’s kind of annoying to have to separately calculate means, medians, standard deviations, skews etc. Wouldn’t it be nice if R had some helpful functions that would do all these tedious calculations at once? Something like summary() or describe(), perhaps? Why yes, yes it would. So much so that both of these functions exist. The summary() function is in the base package, so it comes with every installation of R. The describe() function is part of the psych package, which we loaded earlier in the chapter.\n\n\nThe summary() function is an easy thing to use, but a tricky thing to understand in full, since it’s a generic function (see Section @ref(generics). The basic idea behind the summary() function is that it prints out some useful information about whatever object (i.e., variable, as far as we’re concerned) you specify as the object argument. As a consequence, the behaviour of the summary() function differs quite dramatically depending on the class of the object that you give it. Let’s start by giving it a numeric object:\n\nsummary( object = afl.margins )  \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00   12.75   30.50   35.30   50.50  116.00 \n\n\nFor numeric variables, we get a whole bunch of useful descriptive statistics. It gives us the minimum and maximum values (i.e., the range), the first and third quartiles (25th and 75th percentiles; i.e., the IQR), the mean and the median. In other words, it gives us a pretty good collection of descriptive statistics related to the central tendency and the spread of the data.\nOkay, what about if we feed it a logical vector instead? Let’s say I want to know something about how many “blowouts” there were in the 2010 AFL season. I operationalise the concept of a blowout (see Chapter @ref(studydesign)) as a game in which the winning margin exceeds 50 points. Let’s create a logical variable blowouts in which the \\(i\\)-th element is TRUE if that game was a blowout according to my definition,\n\nblowouts &lt;-  afl.margins &gt; 50\nblowouts\n\n  [1]  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n [13] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE\n [37]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE\n [49] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE\n [61]  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE\n[109]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n[121] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE\n[133] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE\n[145]  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE\n[157]  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n[169] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nSo that’s what the blowouts variable looks like. Now let’s ask R for a summary()\n\nsummary( object = blowouts )\n\n   Mode   FALSE    TRUE \nlogical     132      44 \n\n\nIn this context, the summary() function gives us a count of the number of TRUE values, the number of FALSE values, and the number of missing values (i.e., the NAs). Pretty reasonable behaviour.\nNext, let’s try to give it a factor. If you recall, I’ve defined the afl.finalists vector as a factor, so let’s use that:\n\nsummary( object = afl.finalists )\n\n        Adelaide         Brisbane          Carlton      Collingwood \n              26               25               26               28 \n        Essendon          Fitzroy        Fremantle          Geelong \n              32                0                6               39 \n        Hawthorn        Melbourne  North Melbourne    Port Adelaide \n              27               28               28               17 \n        Richmond         St Kilda           Sydney       West Coast \n               6               24               26               38 \nWestern Bulldogs \n              24 \n\n\nFor factors, we get a frequency table, just like we got when we used the table() function. Interestingly, however, if we convert this to a character vector using the as.character() function (see Section @ref(coercion), we don’t get the same results:\n\nf2 &lt;- as.character( afl.finalists )\nsummary( object = f2 )\n\n   Length     Class      Mode \n      400 character character \n\n\nThis is one of those situations I was referring to in Section @ref(factors), in which it is helpful to declare your nominal scale variable as a factor rather than a character vector. Because I’ve defined afl.finalists as a factor, R knows that it should treat it as a nominal scale variable, and so it gives you a much more detailed (and helpful) summary than it would have if I’d left it as a character vector.\n\n\n\nOkay what about data frames? When you pass a data frame to the summary() function, it produces a slightly condensed summary of each variable inside the data frame. To give you a sense of how this can be useful, let’s try this for a new data set, one that you’ve never seen before. The data is stored in the clinicaltrial.Rdata file, and we’ll use it a lot in Chapter @ref(anova) (you can find a complete description of the data at the start of that chapter). Let’s load it, and see what we’ve got:\n\nload( \"materials/data/clinicaltrial.Rdata\" )\nwho(TRUE)\n\n   -- Name --    -- Class --   -- Size --\n   clin.trial    data.frame    18 x 3    \n    $drug        factor        18        \n    $therapy     factor        18        \n    $mood.gain   numeric       18        \n\n\nThere’s a single data frame called clin.trial which contains three variables, drug, therapy and mood.gain. Presumably then, this data is from a clinical trial of some kind, in which people were administered different drugs; and the researchers looked to see what the drugs did to their mood. Let’s see if the summary() function sheds a little more light on this situation:\n\nsummary( clin.trial )\n\n       drug         therapy    mood.gain     \n placebo :6   no.therapy:9   Min.   :0.1000  \n anxifree:6   CBT       :9   1st Qu.:0.4250  \n joyzepam:6                  Median :0.8500  \n                             Mean   :0.8833  \n                             3rd Qu.:1.3000  \n                             Max.   :1.8000  \n\n\nEvidently there were three drugs: a placebo, something called “anxifree” and something called “joyzepam”; and there were 6 people administered each drug. There were 9 people treated using cognitive behavioural therapy (CBT) and 9 people who received no psychological treatment. And we can see from looking at the summary of the mood.gain variable that most people did show a mood gain (mean \\(=.88\\)), though without knowing what the scale is here it’s hard to say much more than that. Still, that’s not too bad. Overall, I feel that I learned something from that.\n\n\n\nThe describe() function (in the psych package) is a little different, and it’s really only intended to be useful when your data are interval or ratio scale. Unlike the summary() function, it calculates the same descriptive statistics for any type of variable you give it. By default, these are:\n\nvar. This is just an index: 1 for the first variable, 2 for the second variable, and so on.\nn. This is the sample size: more precisely, it’s the number of non-missing values.\nmean. This is the sample mean (Section @ref(mean)).\nsd. This is the (bias corrected) standard deviation (Section @ref(sd)).\nmedian. The median (Section @ref(median)).\ntrimmed. This is trimmed mean. By default it’s the 10% trimmed mean (Section @ref(trimmedmean)).\nmad. The median absolute deviation (Section @ref(mad)).\nmin. The minimum value.\nmax. The maximum value.\nrange. The range spanned by the data (Section @ref(range)).\nskew. The skewness (Section @ref(skewandkurtosis)).\nkurtosis. The kurtosis (Section @ref(skewandkurtosis)).\nse. The standard error of the mean (Chapter @ref(estimation)).\n\nNotice that these descriptive statistics generally only make sense for data that are interval or ratio scale (usually encoded as numeric vectors). For nominal or ordinal variables (usually encoded as factors), most of these descriptive statistics are not all that useful. What the describe() function does is convert factors and logical variables to numeric vectors in order to do the calculations. These variables are marked with * and most of the time, the descriptive statistics for those variables won’t make much sense. If you try to feed it a data frame that includes a character vector as a variable, it produces an error.\nWith those caveats in mind, let’s use the describe() function to have a look at the clin.trial data frame. Here’s what we get:\n\ndescribe( x = clin.trial )\n\n          vars  n mean   sd median trimmed  mad min max range skew kurtosis\ndrug*        1 18 2.00 0.84   2.00    2.00 1.48 1.0 3.0   2.0 0.00    -1.66\ntherapy*     2 18 1.50 0.51   1.50    1.50 0.74 1.0 2.0   1.0 0.00    -2.11\nmood.gain    3 18 0.88 0.53   0.85    0.88 0.67 0.1 1.8   1.7 0.13    -1.44\n            se\ndrug*     0.20\ntherapy*  0.12\nmood.gain 0.13\n\n\nAs you can see, the output for the asterisked variables is pretty meaningless, and should be ignored. However, for the mood.gain variable, there’s a lot of useful information.\n\n\n\n\nIt is very commonly the case that you find yourself needing to look at descriptive statistics, broken down by some grouping variable. This is pretty easy to do in R, and there are three functions in particular that are worth knowing about: by(), describeBy() and aggregate(). Let’s start with the describeBy() function, which is part of the psych package. The describeBy() function is very similar to the describe() function, except that it has an additional argument called group which specifies a grouping variable. For instance, let’s say, I want to look at the descriptive statistics for the clin.trial data, broken down separately by therapy type. The command I would use here is:\n\ndescribeBy( x=clin.trial, group=clin.trial$therapy )\n\n\n Descriptive statistics by group \ngroup: no.therapy\n          vars n mean   sd median trimmed  mad min max range skew kurtosis   se\ndrug         1 9 2.00 0.87    2.0    2.00 1.48 1.0 3.0   2.0 0.00    -1.81 0.29\ntherapy      2 9 1.00 0.00    1.0    1.00 0.00 1.0 1.0   0.0  NaN      NaN 0.00\nmood.gain    3 9 0.72 0.59    0.5    0.72 0.44 0.1 1.7   1.6 0.51    -1.59 0.20\n------------------------------------------------------------ \ngroup: CBT\n          vars n mean   sd median trimmed  mad min max range  skew kurtosis\ndrug         1 9 2.00 0.87    2.0    2.00 1.48 1.0 3.0   2.0  0.00    -1.81\ntherapy      2 9 2.00 0.00    2.0    2.00 0.00 2.0 2.0   0.0   NaN      NaN\nmood.gain    3 9 1.04 0.45    1.1    1.04 0.44 0.3 1.8   1.5 -0.03    -1.12\n            se\ndrug      0.29\ntherapy   0.00\nmood.gain 0.15\n\n\nAs you can see, the output is essentially identical to the output that the describe() function produce, except that the output now gives you means, standard deviations etc separately for the CBT group and the no.therapy group. Notice that, as before, the output displays asterisks for factor variables, in order to draw your attention to the fact that the descriptive statistics that it has calculated won’t be very meaningful for those variables. Nevertheless, this command has given us some really useful descriptive statistics mood.gain variable, broken down as a function of therapy.\nA somewhat more general solution is offered by the by() function. There are three arguments that you need to specify when using this function: the data argument specifies the data set, the INDICES argument specifies the grouping variable, and the FUN argument specifies the name of a function that you want to apply separately to each group. To give a sense of how powerful this is, you can reproduce the describeBy() function by using a command like this:\n\nby( data=clin.trial, INDICES=clin.trial$therapy, FUN=describe )\n\nclin.trial$therapy: no.therapy\n          vars n mean   sd median trimmed  mad min max range skew kurtosis   se\ndrug*        1 9 2.00 0.87    2.0    2.00 1.48 1.0 3.0   2.0 0.00    -1.81 0.29\ntherapy*     2 9 1.00 0.00    1.0    1.00 0.00 1.0 1.0   0.0  NaN      NaN 0.00\nmood.gain    3 9 0.72 0.59    0.5    0.72 0.44 0.1 1.7   1.6 0.51    -1.59 0.20\n------------------------------------------------------------ \nclin.trial$therapy: CBT\n          vars n mean   sd median trimmed  mad min max range  skew kurtosis\ndrug*        1 9 2.00 0.87    2.0    2.00 1.48 1.0 3.0   2.0  0.00    -1.81\ntherapy*     2 9 2.00 0.00    2.0    2.00 0.00 2.0 2.0   0.0   NaN      NaN\nmood.gain    3 9 1.04 0.45    1.1    1.04 0.44 0.3 1.8   1.5 -0.03    -1.12\n            se\ndrug*     0.29\ntherapy*  0.00\nmood.gain 0.15\n\n\nThis will produce the exact same output as the command shown earlier. However, there’s nothing special about the describe() function. You could just as easily use the by() function in conjunction with the summary() function. For example:\n\nby( data=clin.trial, INDICES=clin.trial$therapy, FUN=summary )\n\nclin.trial$therapy: no.therapy\n       drug         therapy    mood.gain     \n placebo :3   no.therapy:9   Min.   :0.1000  \n anxifree:3   CBT       :0   1st Qu.:0.3000  \n joyzepam:3                  Median :0.5000  \n                             Mean   :0.7222  \n                             3rd Qu.:1.3000  \n                             Max.   :1.7000  \n------------------------------------------------------------ \nclin.trial$therapy: CBT\n       drug         therapy    mood.gain    \n placebo :3   no.therapy:0   Min.   :0.300  \n anxifree:3   CBT       :9   1st Qu.:0.800  \n joyzepam:3                  Median :1.100  \n                             Mean   :1.044  \n                             3rd Qu.:1.300  \n                             Max.   :1.800  \n\n\nAgain, this output is pretty easy to interpret. It’s the output of the summary() function, applied separately to CBT group and the no.therapy group. For the two factors (drug and therapy) it prints out a frequency table, whereas for the numeric variable (mood.gain) it prints out the range, interquartile range, mean and median.\n% What if you have multiple grouping variables? Suppose, for example, you would like to look at the average mood gain separately for all possible combinations of drug and therapy. It is actually possible to do this using the by() and describeBy() functions, but I usually find it more convenient to use the aggregate() function in this situation. There are again three arguments that you need to specify. The formula argument is used to indicate which variable you want to analyse, and which variables are used to specify the groups. For instance, if you want to look at mood.gain separately for each possible combination of drug and therapy, the formula you want is mood.gain ~ drug + therapy. The data argument is used to specify the data frame containing all the data, and the FUN argument is used to indicate what function you want to calculate for each group (e.g., the mean). So, to obtain group means, use this command:\n% {r} %  aggregate( formula = mood.gain ~ drug + therapy,  # mood.gain by drug/therapy combination %             data = clin.trial,                     # data is in the clin.trial data frame %             FUN = mean                             # print out group means %  ) %\n% or, alternatively, if you want to calculate the standard deviations for each group, you would use the following command (argument names omitted this time):\n% {r} % aggregate( mood.gain ~ drug + therapy, clin.trial, sd ) %\n\n\n\nSuppose my friend is putting together a new questionnaire intended to measure “grumpiness”. The survey has 50 questions, which you can answer in a grumpy way or not. Across a big sample (hypothetically, let’s imagine a million people or so!) the data are fairly normally distributed, with the mean grumpiness score being 17 out of 50 questions answered in a grumpy way, and the standard deviation is 5. In contrast, when I take the questionnaire, I answer 35 out of 50 questions in a grumpy way. So, how grumpy am I? One way to think about would be to say that I have grumpiness of 35/50, so you might say that I’m 70% grumpy. But that’s a bit weird, when you think about it. If my friend had phrased her questions a bit differently, people might have answered them in a different way, so the overall distribution of answers could easily move up or down depending on the precise way in which the questions were asked. So, I’m only 70% grumpy with respect to this set of survey questions. Even if it’s a very good questionnaire, this isn’t very a informative statement.\nA simpler way around this is to describe my grumpiness by comparing me to other people. Shockingly, out of my friend’s sample of 1,000,000 people, only 159 people were as grumpy as me (that’s not at all unrealistic, frankly), suggesting that I’m in the top 0.016% of people for grumpiness. This makes much more sense than trying to interpret the raw data. This idea – that we should describe my grumpiness in terms of the overall distribution of the grumpiness of humans – is the qualitative idea that standardisation attempts to get at. One way to do this is to do exactly what I just did, and describe everything in terms of percentiles. However, the problem with doing this is that “it’s lonely at the top”. Suppose that my friend had only collected a sample of 1000 people (still a pretty big sample for the purposes of testing a new questionnaire, I’d like to add), and this time gotten a mean of 16 out of 50 with a standard deviation of 5, let’s say. The problem is that almost certainly, not a single person in that sample would be as grumpy as me.\nHowever, all is not lost. A different approach is to convert my grumpiness score into a standard score, also referred to as a \\(z\\)-score. The standard score is defined as the number of standard deviations above the mean that my grumpiness score lies. To phrase it in “pseudo-maths” the standard score is calculated like this: \\[\n\\mbox{standard score} = \\frac{\\mbox{raw score} - \\mbox{mean}}{\\mbox{standard deviation}}\n\\] In actual maths, the equation for the \\(z\\)-score is \\[\nz_i = \\frac{X_i - \\bar{X}}{\\hat\\sigma}\n\\] So, going back to the grumpiness data, we can now transform Dan’s raw grumpiness into a standardised grumpiness score.13 If the mean is 17 and the standard deviation is 5 then my standardised grumpiness score would be14 \\[\nz = \\frac{35 - 17}{5} = 3.6\n\\] To interpret this value, recall the rough heuristic that I provided in Section @ref(sd), in which I noted that 99.7% of values are expected to lie within 3 standard deviations of the mean. So the fact that my grumpiness corresponds to a \\(z\\) score of 3.6 indicates that I’m very grumpy indeed. Later on, in Section @ref(normal), I’ll introduce a function called pnorm() that allows us to be a bit more precise than this. Specifically, it allows us to calculate a theoretical percentile rank for my grumpiness, as follows:\n\npnorm( 3.6 )\n\n[1] 0.9998409\n\n\nAt this stage, this command doesn’t make too much sense, but don’t worry too much about it. It’s not important for now. But the output is fairly straightforward: it suggests that I’m grumpier than 99.98% of people. Sounds about right.\nIn addition to allowing you to interpret a raw score in relation to a larger population (and thereby allowing you to make sense of variables that lie on arbitrary scales), standard scores serve a second useful function. Standard scores can be compared to one another in situations where the raw scores can’t. Suppose, for instance, my friend also had another questionnaire that measured extraversion using a 24 items questionnaire. The overall mean for this measure turns out to be 13 with standard deviation 4; and I scored a 2. As you can imagine, it doesn’t make a lot of sense to try to compare my raw score of 2 on the extraversion questionnaire to my raw score of 35 on the grumpiness questionnaire. The raw scores for the two variables are “about” fundamentally different things, so this would be like comparing apples to oranges.\nWhat about the standard scores? Well, this is a little different. If we calculate the standard scores, we get \\(z = (35-17)/5 = 3.6\\) for grumpiness and \\(z = (2-13)/4 = -2.75\\) for extraversion. These two numbers can be compared to each other.15 I’m much less extraverted than most people (\\(z = -2.75\\)) and much grumpier than most people (\\(z = 3.6\\)): but the extent of my unusualness is much more extreme for grumpiness (since 3.6 is a bigger number than 2.75). Because each standardised score is a statement about where an observation falls relative to its own population, it is possible to compare standardised scores across completely different variables.\n\n\n\nUp to this point we have focused entirely on how to construct descriptive statistics for a single variable. What we haven’t done is talked about how to describe the relationships between variables in the data. To do that, we want to talk mostly about the correlation between variables. But first, we need some data.\n\n\nAfter spending so much time looking at the AFL data, I’m starting to get bored with sports. Instead, let’s turn to a topic close to every parent’s heart: sleep. The following data set is fictitious, but based on real events. Suppose I’m curious to find out how much my infant son’s sleeping habits affect my mood. Let’s say that I can rate my grumpiness very precisely, on a scale from 0 (not at all grumpy) to 100 (grumpy as a very, very grumpy old man). And, lets also assume that I’ve been measuring my grumpiness, my sleeping patterns and my son’s sleeping patterns for quite some time now. Let’s say, for 100 days. And, being a nerd, I’ve saved the data as a file called parenthood.Rdata. If we load the data…\n\nload( \"materials/data/parenthood.Rdata\" )\nwho(TRUE)\n\n   -- Name --     -- Class --   -- Size --\n   parenthood     data.frame    100 x 4   \n    $dan.sleep    numeric       100       \n    $baby.sleep   numeric       100       \n    $dan.grump    numeric       100       \n    $day          integer       100       \n\n\n… we see that the file contains a single data frame called parenthood, which contains four variables dan.sleep, baby.sleep, dan.grump and day. If we peek at the data using head() out the data, here’s what we get:\n\nhead(parenthood,10)\n\n   dan.sleep baby.sleep dan.grump day\n1       7.59      10.18        56   1\n2       7.91      11.66        60   2\n3       5.14       7.92        82   3\n4       7.71       9.61        55   4\n5       6.68       9.75        67   5\n6       5.99       5.04        72   6\n7       8.19      10.45        53   7\n8       7.19       8.27        60   8\n9       7.40       6.06        60   9\n10      6.58       7.09        71  10\n\n\nNext, I’ll calculate some basic descriptive statistics:\n\ndescribe( parenthood )\n\n           vars   n  mean    sd median trimmed   mad   min    max range  skew\ndan.sleep     1 100  6.97  1.02   7.03    7.00  1.09  4.84   9.00  4.16 -0.29\nbaby.sleep    2 100  8.05  2.07   7.95    8.05  2.33  3.25  12.07  8.82 -0.02\ndan.grump     3 100 63.71 10.05  62.00   63.16  9.64 41.00  91.00 50.00  0.43\nday           4 100 50.50 29.01  50.50   50.50 37.06  1.00 100.00 99.00  0.00\n           kurtosis   se\ndan.sleep     -0.72 0.10\nbaby.sleep    -0.69 0.21\ndan.grump     -0.16 1.00\nday           -1.24 2.90\n\n\nFinally, to give a graphical depiction of what each of the three interesting variables looks like, Figure @ref(fig:parenthood) plots histograms.\n\n\n\n\n\nHistograms for the three interesting variables in the parenthood data set\n\n\n\n\nOne thing to note: just because R can calculate dozens of different statistics doesn’t mean you should report all of them. If I were writing this up for a report, I’d probably pick out those statistics that are of most interest to me (and to my readership), and then put them into a nice, simple table like the one in Table @ref(tab:parenthoodtab).16 Notice that when I put it into a table, I gave everything “human readable” names. This is always good practice. Notice also that I’m not getting enough sleep. This isn’t good practice, but other parents tell me that it’s standard practice.\n\n\n\nDescriptive statistics for the parenthood data.\n\n\nvariable\nmin\nmax\nmean\nmedian\nstd. dev\nIQR\n\n\n\n\nDan’s grumpiness\n41\n91\n63.71\n62\n10.05\n14\n\n\nDan’s hours slept\n4.84\n9\n6.97\n7.03\n1.02\n1.45\n\n\nDan’s son’s hours slept\n3.25\n12.07\n8.05\n7.95\n2.07\n3.21\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatterplot showing the relationship between dan.sleep and dan.grump\n\n\n\n\n\n\n\n\n\nScatterplot showing the relationship between baby.sleep and dan.grump\n\n\n\n\nWe can draw scatterplots to give us a general sense of how closely related two variables are. Ideally though, we might want to say a bit more about it than that. For instance, let’s compare the relationship between dan.sleep and dan.grump (Figure @ref(fig:scatterparent1a) with that between baby.sleep and dan.grump (Figure @ref(fig:scatterparent1b). When looking at these two plots side by side, it’s clear that the relationship is qualitatively the same in both cases: more sleep equals less grump! However, it’s also pretty obvious that the relationship between dan.sleep and dan.grump is stronger than the relationship between baby.sleep and dan.grump. The plot on the left is “neater” than the one on the right. What it feels like is that if you want to predict what my mood is, it’d help you a little bit to know how many hours my son slept, but it’d be more helpful to know how many hours I slept.\nIn contrast, let’s consider Figure @ref(fig:scatterparent1b) vs. Figure @ref(fig:scatterparent2). If we compare the scatterplot of “baby.sleep v dan.grump” to the scatterplot of “`baby.sleep v dan.sleep”, the overall strength of the relationship is the same, but the direction is different. That is, if my son sleeps more, I get more sleep (positive relationship, but if he sleeps more then I get less grumpy (negative relationship).\n\n\n\n\n\nScatterplot showing the relationship between baby.sleep and dan.sleep\n\n\n\n\n\n\n\nWe can make these ideas a bit more explicit by introducing the idea of a correlation coefficient (or, more specifically, Pearson’s correlation coefficient), which is traditionally denoted by \\(r\\). The correlation coefficient between two variables \\(X\\) and \\(Y\\) (sometimes denoted \\(r_{XY}\\)), which we’ll define more precisely in the next section, is a measure that varies from \\(-1\\) to \\(1\\). When \\(r = -1\\) it means that we have a perfect negative relationship, and when \\(r = 1\\) it means we have a perfect positive relationship. When \\(r = 0\\), there’s no relationship at all. If you look at Figure @ref(fig:corr), you can see several plots showing what different correlations look like.\n\n\n\n\n\nIllustration of the effect of varying the strength and direction of a correlation\n\n\n\n\nThe formula for the Pearson’s correlation coefficient can be written in several different ways. I think the simplest way to write down the formula is to break it into two steps. Firstly, let’s introduce the idea of a covariance. The covariance between two variables \\(X\\) and \\(Y\\) is a generalisation of the notion of the variance; it’s a mathematically simple way of describing the relationship between two variables that isn’t terribly informative to humans: \\[\n\\mbox{Cov}(X,Y) = \\frac{1}{N-1} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right) \\left( Y_i - \\bar{Y} \\right)\n\\] Because we’re multiplying (i.e., taking the “product” of) a quantity that depends on \\(X\\) by a quantity that depends on \\(Y\\) and then averaging17, you can think of the formula for the covariance as an “average cross product” between \\(X\\) and \\(Y\\). The covariance has the nice property that, if \\(X\\) and \\(Y\\) are entirely unrelated, then the covariance is exactly zero. If the relationship between them is positive (in the sense shown in Figure@reffig:corr) then the covariance is also positive; and if the relationship is negative then the covariance is also negative. In other words, the covariance captures the basic qualitative idea of correlation. Unfortunately, the raw magnitude of the covariance isn’t easy to interpret: it depends on the units in which \\(X\\) and \\(Y\\) are expressed, and worse yet, the actual units that the covariance itself is expressed in are really weird. For instance, if \\(X\\) refers to the dan.sleep variable (units: hours) and \\(Y\\) refers to the dan.grump variable (units: grumps), then the units for their covariance are “hours \\(\\times\\) grumps”. And I have no freaking idea what that would even mean.\nThe Pearson correlation coefficient \\(r\\) fixes this interpretation problem by standardising the covariance, in pretty much the exact same way that the \\(z\\)-score standardises a raw score: by dividing by the standard deviation. However, because we have two variables that contribute to the covariance, the standardisation only works if we divide by both standard deviations.18 In other words, the correlation between \\(X\\) and \\(Y\\) can be written as follows: \\[\nr_{XY}  = \\frac{\\mbox{Cov}(X,Y)}{ \\hat{\\sigma}_X \\ \\hat{\\sigma}_Y}\n\\] By doing this standardisation, not only do we keep all of the nice properties of the covariance discussed earlier, but the actual values of \\(r\\) are on a meaningful scale: \\(r= 1\\) implies a perfect positive relationship, and \\(r = -1\\) implies a perfect negative relationship. I’ll expand a little more on this point later, in Section@refsec:interpretingcorrelations. But before I do, let’s look at how to calculate correlations in R.\n\n\n\nCalculating correlations in R can be done using the cor() command. The simplest way to use the command is to specify two input arguments x and y, each one corresponding to one of the variables. The following extract illustrates the basic usage of the function:19\n\ncor( x = parenthood$dan.sleep, y = parenthood$dan.grump )\n\n[1] -0.903384\n\n\nHowever, the cor() function is a bit more powerful than this simple example suggests. For example, you can also calculate a complete “correlation matrix”, between all pairs of variables in the data frame:20\n\n# correlate all pairs of variables in \"parenthood\":\ncor( x = parenthood )  \n\n             dan.sleep  baby.sleep   dan.grump         day\ndan.sleep   1.00000000  0.62794934 -0.90338404 -0.09840768\nbaby.sleep  0.62794934  1.00000000 -0.56596373 -0.01043394\ndan.grump  -0.90338404 -0.56596373  1.00000000  0.07647926\nday        -0.09840768 -0.01043394  0.07647926  1.00000000\n\n\n\n\n\nNaturally, in real life you don’t see many correlations of 1. So how should you interpret a correlation of, say \\(r= .4\\)? The honest answer is that it really depends on what you want to use the data for, and on how strong the correlations in your field tend to be. A friend of mine in engineering once argued that any correlation less than \\(.95\\) is completely useless (I think he was exaggerating, even for engineering). On the other hand there are real cases – even in psychology – where you should really expect correlations that strong. For instance, one of the benchmark data sets used to test theories of how people judge similarities is so clean that any theory that can’t achieve a correlation of at least \\(.9\\) really isn’t deemed to be successful. However, when looking for (say) elementary correlates of intelligence (e.g., inspection time, response time), if you get a correlation above \\(.3\\) you’re doing very very well. In short, the interpretation of a correlation depends a lot on the context. That said, the rough guide in Table @ref(tab:interpretingcorrelations) is pretty typical.\n\n\n\nRough guide to interpreting correlations\n\n\nCorrelation\nStrength\nDirection\n\n\n\n\n-1.0 to -0.9\nVery strong\nNegative\n\n\n-0.9 to -0.7\nStrong\nNegative\n\n\n-0.7 to -0.4\nModerate\nNegative\n\n\n-0.4 to -0.2\nWeak\nNegative\n\n\n-0.2 to 0\nNegligible\nNegative\n\n\n0 to 0.2\nNegligible\nPositive\n\n\n0.2 to 0.4\nWeak\nPositive\n\n\n0.4 to 0.7\nModerate\nPositive\n\n\n0.7 to 0.9\nStrong\nPositive\n\n\n0.9 to 1.0\nVery strong\nPositive\n\n\n\n\n\nHowever, something that can never be stressed enough is that you should always look at the scatterplot before attaching any interpretation to the data. A correlation might not mean what you think it means. The classic illustration of this is “Anscombe’s Quartet” [@Anscombe1973], which is a collection of four data sets. Each data set has two variables, an \\(X\\) and a \\(Y\\). For all four data sets the mean value for \\(X\\) is 9 and the mean for \\(Y\\) is 7.5. The, standard deviations for all \\(X\\) variables are almost identical, as are those for the the \\(Y\\) variables. And in each case the correlation between \\(X\\) and \\(Y\\) is \\(r = 0.816\\). You can verify this yourself, since the dataset comes distributed with R. The commands would be:\n\ncor( anscombe$x1, anscombe$y1 )\n\n[1] 0.8164205\n\ncor( anscombe$x2, anscombe$y2 )\n\n[1] 0.8162365\n\n\nand so on.\nYou’d think that these four data sets would look pretty similar to one another. They do not. If we draw scatterplots of \\(X\\) against \\(Y\\) for all four variables, as shown in Figure @ref(fig:anscombe) we see that all four of these are spectacularly different to each other.\n\n\n\n\n\nAnscombe’s quartet. All four of these data sets have a Pearson correlation of \\(r = .816\\), but they are qualitatively different from one another.\n\n\n\n\nThe lesson here, which so very many people seem to forget in real life is “always graph your raw data”. This will be the focus of Chapter @ref(graphics).\n\n\n\n\n\n\n\n\nThe relationship between hours worked and grade received, for a toy data set consisting of only 10 students (each circle corresponds to one student). The dashed line through the middle shows the linear relationship between the two variables. This produces a strong Pearson correlation of \\(r = .91\\). However, the interesting thing to note here is that there’s actually a perfect monotonic relationship between the two variables: in this toy example at least, increasing the hours worked always increases the grade received, as illustrated by the solid line. This is reflected in a Spearman correlation of \\(rho = 1\\). With such a small data set, however, it’s an open question as to which version better describes the actual relationship involved.\n\n\n\n\nThe Pearson correlation coefficient is useful for a lot of things, but it does have shortcomings. One issue in particular stands out: what it actually measures is the strength of the linear relationship between two variables. In other words, what it gives you is a measure of the extent to which the data all tend to fall on a single, perfectly straight line. Often, this is a pretty good approximation to what we mean when we say “relationship”, and so the Pearson correlation is a good thing to calculation. Sometimes, it isn’t.\nOne very common situation where the Pearson correlation isn’t quite the right thing to use arises when an increase in one variable \\(X\\) really is reflected in an increase in another variable \\(Y\\), but the nature of the relationship isn’t necessarily linear. An example of this might be the relationship between effort and reward when studying for an exam. If you put in zero effort (\\(X\\)) into learning a subject, then you should expect a grade of 0% (\\(Y\\)). However, a little bit of effort will cause a massive improvement: just turning up to lectures means that you learn a fair bit, and if you just turn up to classes, and scribble a few things down so your grade might rise to 35%, all without a lot of effort. However, you just don’t get the same effect at the other end of the scale. As everyone knows, it takes a lot more effort to get a grade of 90% than it takes to get a grade of 55%. What this means is that, if I’ve got data looking at study effort and grades, there’s a pretty good chance that Pearson correlations will be misleading.\nTo illustrate, consider the data plotted in Figure @ref(fig:rankcorrpic), showing the relationship between hours worked and grade received for 10 students taking some class. The curious thing about this – highly fictitious – data set is that increasing your effort always increases your grade. It might be by a lot or it might be by a little, but increasing effort will never decrease your grade. The data are stored in effort.Rdata:\n&gt; load( \"effort.Rdata\" )\n&gt; who(TRUE)\n   -- Name --   -- Class --   -- Size --\n   effort       data.frame    10 x 2    \n    $hours      numeric       10        \n    $grade      numeric       10        \nThe raw data look like this:\n&gt; effort\n   hours grade\n1      2    13\n2     76    91\n3     40    79\n4      6    14\n5     16    21\n6     28    74\n7     27    47\n8     59    85\n9     46    84\n10    68    88\nIf we run a standard Pearson correlation, it shows a strong relationship between hours worked and grade received,\n&gt; cor( effort$hours, effort$grade )\n[1] 0.909402\nbut this doesn’t actually capture the observation that increasing hours worked always increases the grade. There’s a sense here in which we want to be able to say that the correlation is perfect but for a somewhat different notion of what a “relationship” is. What we’re looking for is something that captures the fact that there is a perfect ordinal relationship here. That is, if student 1 works more hours than student 2, then we can guarantee that student 1 will get the better grade. That’s not what a correlation of \\(r = .91\\) says at all.\nHow should we address this? Actually, it’s really easy: if we’re looking for ordinal relationships, all we have to do is treat the data as if it were ordinal scale! So, instead of measuring effort in terms of “hours worked”, lets rank all 10 of our students in order of hours worked. That is, student 1 did the least work out of anyone (2 hours) so they get the lowest rank (rank = 1). Student 4 was the next laziest, putting in only 6 hours of work in over the whole semester, so they get the next lowest rank (rank = 2). Notice that I’m using “rank =1” to mean “low rank”. Sometimes in everyday language we talk about “rank = 1” to mean “top rank” rather than “bottom rank”. So be careful: you can rank “from smallest value to largest value” (i.e., small equals rank 1) or you can rank “from largest value to smallest value” (i.e., large equals rank 1). In this case, I’m ranking from smallest to largest, because that’s the default way that R does it. But in real life, it’s really easy to forget which way you set things up, so you have to put a bit of effort into remembering!\nOkay, so let’s have a look at our students when we rank them from worst to best in terms of effort and reward:\n\n\n\n\nrank (hours worked)\nrank (grade received)\n\n\n\n\nstudent\n1\n1\n\n\nstudent\n2\n10\n\n\nstudent\n3\n6\n\n\nstudent\n4\n2\n\n\nstudent\n5\n3\n\n\nstudent\n6\n5\n\n\nstudent\n7\n4\n\n\nstudent\n8\n8\n\n\nstudent\n9\n7\n\n\nstudent\n10\n9\n\n\n\nHm. These are identical. The student who put in the most effort got the best grade, the student with the least effort got the worst grade, etc. We can get R to construct these rankings using the rank() function, like this:\n&gt; hours.rank &lt;- rank( effort$hours )   # rank students by hours worked\n&gt; grade.rank &lt;- rank( effort$grade )   # rank students by grade received\nAs the table above shows, these two rankings are identical, so if we now correlate them we get a perfect relationship:\n&gt; cor( hours.rank, grade.rank )\n[1] 1\nWhat we’ve just re-invented is Spearman’s rank order correlation, usually denoted \\(\\rho\\) to distinguish it from the Pearson correlation \\(r\\). We can calculate Spearman’s \\(\\rho\\) using R in two different ways. Firstly we could do it the way I just showed, using the rank() function to construct the rankings, and then calculate the Pearson correlation on these ranks. However, that’s way too much effort to do every time. It’s much easier to just specify the method argument of the cor() function.\n&gt; cor( effort$hours, effort$grade, method = \"spearman\")\n[1] 1\nThe default value of the method argument is \"pearson\", which is why we didn’t have to specify it earlier on when we were doing Pearson correlations.\n\n\n\nAs we’ve seen, the cor() function works pretty well, and handles many of the situations that you might be interested in. One thing that many beginners find frustrating, however, is the fact that it’s not built to handle non-numeric variables. From a statistical perspective, this is perfectly sensible: Pearson and Spearman correlations are only designed to work for numeric variables, so the cor() function spits out an error.\nHere’s what I mean. Suppose you were keeping track of how many hours you worked in any given day, and counted how many tasks you completed. If you were doing the tasks for money, you might also want to keep track of how much pay you got for each job. It would also be sensible to keep track of the weekday on which you actually did the work: most of us don’t work as much on Saturdays or Sundays. If you did this for 7 weeks, you might end up with a data set that looks like this one:\n&gt; load(\"work.Rdata\")\n\n&gt; who(TRUE)\n   -- Name --   -- Class --   -- Size --\n   work         data.frame    49 x 7    \n    $hours      numeric       49        \n    $tasks      numeric       49        \n    $pay        numeric       49        \n    $day        integer       49        \n    $weekday    factor        49        \n    $week       numeric       49        \n    $day.type   factor        49   \n    \n&gt; head(work)\n  hours tasks pay day   weekday week day.type\n1   7.2    14  41   1   Tuesday    1  weekday\n2   7.4    11  39   2 Wednesday    1  weekday\n3   6.6    14  13   3  Thursday    1  weekday\n4   6.5    22  47   4    Friday    1  weekday\n5   3.1     5   4   5  Saturday    1  weekend\n6   3.0     7  12   6    Sunday    1  weekend\nObviously, I’d like to know something about how all these variables correlate with one another. I could correlate hours with pay quite using cor(), like so:\n&gt; cor(work$hours,work$pay)\n[1] 0.7604283\nBut what if I wanted a quick and easy way to calculate all pairwise correlations between the numeric variables? I can’t just input the work data frame, because it contains two factor variables, weekday and day.type. If I try this, I get an error:\n&gt; cor(work)\nError in cor(work) : 'x' must be numeric\nIt order to get the correlations that I want using the cor() function, is create a new data frame that doesn’t contain the factor variables, and then feed that new data frame into the cor() function. It’s not actually very hard to do that, and I’ll talk about how to do it properly in Section @ref(subsetdataframe). But it would be nice to have some function that is smart enough to just ignore the factor variables. That’s where the correlate() function in the lsr package can be handy. If you feed it a data frame that contains factors, it knows to ignore them, and returns the pairwise correlations only between the numeric variables:\n&gt; correlate(work)\n\nCORRELATIONS\n============\n- correlation type:  pearson \n- correlations shown only when both variables are numeric\n\n          hours  tasks   pay    day weekday   week day.type\nhours         .  0.800 0.760 -0.049       .  0.018        .\ntasks     0.800      . 0.720 -0.072       . -0.013        .\npay       0.760  0.720     .  0.137       .  0.196        .\nday      -0.049 -0.072 0.137      .       .  0.990        .\nweekday       .      .     .      .       .      .        .\nweek      0.018 -0.013 0.196  0.990       .      .        .\nday.type      .      .     .      .       .      .        .\nThe output here shows a . whenever one of the variables is non-numeric. It also shows a . whenever a variable is correlated with itself (it’s not a meaningful thing to do). The correlate() function can also do Spearman correlations, by specifying the corr.method to use:\n&gt; correlate( work, corr.method=\"spearman\" )\n\nCORRELATIONS\n============\n- correlation type:  spearman \n- correlations shown only when both variables are numeric\n\n          hours  tasks   pay    day weekday   week day.type\nhours         .  0.805 0.745 -0.047       .  0.010        .\ntasks     0.805      . 0.730 -0.068       . -0.008        .\npay       0.745  0.730     .  0.094       .  0.154        .\nday      -0.047 -0.068 0.094      .       .  0.990        .\nweekday       .      .     .      .       .      .        .\nweek      0.010 -0.008 0.154  0.990       .      .        .\nday.type      .      .     .      .       .      .        .\nObviously, there’s no new functionality in the correlate() function, and any advanced R user would be perfectly capable of using the cor() function to get these numbers out. But if you’re not yet comfortable with extracting a subset of a data frame, the correlate() function is for you.\n\n\n\n\nThere’s one last topic that I want to discuss briefly in this chapter, and that’s the issue of missing data. Real data sets very frequently turn out to have missing values: perhaps someone forgot to fill in a particular survey question, for instance. Missing data can be the source of a lot of tricky issues, most of which I’m going to gloss over. However, at a minimum, you need to understand the basics of handling missing data in R.\n\n\nLet’s start with the simplest case, in which you’re trying to calculate descriptive statistics for a single variable which has missing data. In R, this means that there will be NA values in your data vector. Let’s create a variable like that:\n&gt; partial &lt;- c(10, 20, NA, 30)\nLet’s assume that you want to calculate the mean of this variable. By default, R assumes that you want to calculate the mean using all four elements of this vector, which is probably the safest thing for a dumb automaton to do, but it’s rarely what you actually want. Why not? Well, remember that the basic interpretation of NA is “I don’t know what this number is”. This means that 1 + NA = NA: if I add 1 to some number that I don’t know (i.e., the NA) then the answer is also a number that I don’t know. As a consequence, if you don’t explicitly tell R to ignore the NA values, and the data set does have missing values, then the output will itself be a missing value. If I try to calculate the mean of the partial vector, without doing anything about the missing value, here’s what happens:\n&gt; mean( x = partial )\n[1] NA\nTechnically correct, but deeply unhelpful.\nTo fix this, all of the descriptive statistics functions that I’ve discussed in this chapter (with the exception of cor() which is a special case I’ll discuss below) have an optional argument called na.rm, which is shorthand for “remove NA values”. By default, na.rm = FALSE, so R does nothing about the missing data problem. Let’s try setting na.rm = TRUE and see what happens:\nWhen calculating sums and means when missing data are present (i.e., when there are NA values) there’s actually an additional argument to the function that you should be aware of. This argument is called na.rm, and is a logical value indicating whether R should ignore (or “remove”) the missing data for the purposes of doing the calculations. By default, R assumes that you want to keep the missing values, so unless you say otherwise it will set na.rm = FALSE. However, R assumes that 1 + NA = NA: if I add 1 to some number that I don’t know (i.e., the NA) then the answer is also a number that I don’t know. As a consequence, if you don’t explicitly tell R to ignore the NA values, and the data set does have missing values, then the output will itself be a missing value. This is illustrated in the following extract:\n&gt; mean( x = partial, na.rm = TRUE )\n[1] 20\nNotice that the mean is 20 (i.e., 60 / 3) and not 15. When R ignores a NA value, it genuinely ignores it. In effect, the calculation above is identical to what you’d get if you asked for the mean of the three-element vector c(10, 20, 30).\nAs indicated above, this isn’t unique to the mean() function. Pretty much all of the other functions that I’ve talked about in this chapter have an na.rm argument that indicates whether it should ignore missing values. However, its behaviour is the same for all these functions, so I won’t waste everyone’s time by demonstrating it separately for each one.\n\n\n\nI mentioned earlier that the cor() function is a special case. It doesn’t have an na.rm argument, because the story becomes a lot more complicated when more than one variable is involved. What it does have is an argument called use which does roughly the same thing, but you need to think little more carefully about what you want this time. To illustrate the issues, let’s open up a data set that has missing values, parenthood2.Rdata. This file contains the same data as the original parenthood data, but with some values deleted. It contains a single data frame, parenthood2:\n&gt; load( \"parenthood2.Rdata\" )\n&gt; print( parenthood2 )\n  dan.sleep baby.sleep dan.grump day\n1      7.59         NA        56   1\n2      7.91      11.66        60   2\n3      5.14       7.92        82   3\n4      7.71       9.61        55   4\n5      6.68       9.75        NA   5\n6      5.99       5.04        72   6\nBLAH BLAH BLAH\nIf I calculate my descriptive statistics using the describe() function\n&gt; describe( parenthood2 )\n           var   n  mean    sd median trimmed   mad   min    max    BLAH\ndan.sleep    1  91  6.98  1.02   7.03    7.02  1.13  4.84   9.00    BLAH\nbaby.sleep   2  89  8.11  2.05   8.20    8.13  2.28  3.25  12.07    BLAH\ndan.grump    3  92 63.15  9.85  61.00   62.66 10.38 41.00  89.00    BLAH\nday          4 100 50.50 29.01  50.50   50.50 37.06  1.00 100.00    BLAH\nwe can see from the n column that there are 9 missing values for dan.sleep, 11 missing values for baby.sleep and 8 missing values for dan.grump.21 Suppose what I would like is a correlation matrix. And let’s also suppose that I don’t bother to tell R how to handle those missing values. Here’s what happens:\n&gt; cor( parenthood2 )\n           dan.sleep baby.sleep dan.grump day\ndan.sleep          1         NA        NA  NA\nbaby.sleep        NA          1        NA  NA\ndan.grump         NA         NA         1  NA\nday               NA         NA        NA   1\nAnnoying, but it kind of makes sense. If I don’t know what some of the values of dan.sleep and baby.sleep actually are, then I can’t possibly know what the correlation between these two variables is either, since the formula for the correlation coefficient makes use of every single observation in the data set. Once again, it makes sense: it’s just not particularly helpful.\nTo make R behave more sensibly in this situation, you need to specify the use argument to the cor() function. There are several different values that you can specify for this, but the two that we care most about in practice tend to be \"complete.obs\" and \"pairwise.complete.obs\". If we specify use = \"complete.obs\", R will completely ignore all cases (i.e., all rows in our parenthood2 data frame) that have any missing values at all. So, for instance, if you look back at the extract earlier when I used the head() function, notice that observation 1 (i.e., day 1) of the parenthood2 data set is missing the value for baby.sleep, but is otherwise complete? Well, if you choose use = \"complete.obs\" R will ignore that row completely: that is, even when it’s trying to calculate the correlation between dan.sleep and dan.grump, observation 1 will be ignored, because the value of baby.sleep is missing for that observation. Here’s what we get:\n&gt; cor(parenthood2, use = \"complete.obs\")\n             dan.sleep baby.sleep   dan.grump         day\ndan.sleep   1.00000000  0.6394985 -0.89951468  0.06132891\nbaby.sleep  0.63949845  1.0000000 -0.58656066  0.14555814\ndan.grump  -0.89951468 -0.5865607  1.00000000 -0.06816586\nday         0.06132891  0.1455581 -0.06816586  1.00000000\nThe other possibility that we care about, and the one that tends to get used more often in practice, is to set use = \"pairwise.complete.obs\". When we do that, R only looks at the variables that it’s trying to correlate when determining what to drop. So, for instance, since the only missing value for observation 1 of parenthood2 is for baby.sleep R will only drop observation 1 when baby.sleep is one of the variables involved: and so R keeps observation 1 when trying to correlate dan.sleep and dan.grump. When we do it this way, here’s what we get:\n&gt; cor(parenthood2, use = \"pairwise.complete.obs\") \n             dan.sleep  baby.sleep    dan.grump          day\ndan.sleep   1.00000000  0.61472303 -0.903442442 -0.076796665\nbaby.sleep  0.61472303  1.00000000 -0.567802669  0.058309485\ndan.grump  -0.90344244 -0.56780267  1.000000000  0.005833399\nday        -0.07679667  0.05830949  0.005833399  1.000000000\nSimilar, but not quite the same. It’s also worth noting that the correlate() function (in the lsr package) automatically uses the “pairwise complete” method:\n&gt; correlate(parenthood2)\n\nCORRELATIONS\n============\n- correlation type:  pearson \n- correlations shown only when both variables are numeric\n\n           dan.sleep baby.sleep dan.grump    day\ndan.sleep          .      0.615    -0.903 -0.077\nbaby.sleep     0.615          .    -0.568  0.058\ndan.grump     -0.903     -0.568         .  0.006\nday           -0.077      0.058     0.006      .\nThe two approaches have different strengths and weaknesses. The “pairwise complete” approach has the advantage that it keeps more observations, so you’re making use of more of your data and (as we’ll discuss in tedious detail in Chapter @ref(estimation) and it improves the reliability of your estimated correlation. On the other hand, it means that every correlation in your correlation matrix is being computed from a slightly different set of observations, which can be awkward when you want to compare the different correlations that you’ve got.\nSo which method should you use? It depends a lot on why you think your values are missing, and probably depends a little on how paranoid you are. For instance, if you think that the missing values were “chosen” completely randomly22 then you’ll probably want to use the pairwise method. If you think that missing data are a cue to thinking that the whole observation might be rubbish (e.g., someone just selecting arbitrary responses in your questionnaire), but that there’s no pattern to which observations are “rubbish” then it’s probably safer to keep only those observations that are complete. If you think there’s something systematic going on, in that some observations are more likely to be missing than others, then you have a much trickier problem to solve, and one that is beyond the scope of this book.\n\n\n\n\nCalculating some basic descriptive statistics is one of the very first things you do when analysing real data, and descriptive statistics are much simpler to understand than inferential statistics, so like every other statistics textbook I’ve started with descriptives. In this chapter, we talked about the following topics:\n\nMeasures of central tendency. Broadly speaking, central tendency measures tell you where the data are. There’s three measures that are typically reported in the literature: the mean, median and mode. (Section @ref(centraltendency))\nMeasures of variability. In contrast, measures of variability tell you about how “spread out” the data are. The key measures are: range, standard deviation, interquartile reange (Section @ref(var))\nGetting summaries of variables in R. Since this book focuses on doing data analysis in R, we spent a bit of time talking about how descriptive statistics are computed in R. (Section @ref(summary) and @ref(groupdescriptives))\nStandard scores. The \\(z\\)-score is a slightly unusual beast. It’s not quite a descriptive statistic, and not quite an inference. We talked about it in Section @ref(zscore). Make sure you understand that section: it’ll come up again later.\nCorrelations. Want to know how strong the relationship is between two variables? Calculate a correlation. (Section @ref(correl))\nMissing data. Dealing with missing data is one of those frustrating things that data analysts really wish the didn’t have to think about. In real life it can be hard to do well. For the purpose of this book, we only touched on the basics in Section @ref(missing)\n\nIn the next section we’ll move on to a discussion of how to draw pictures! Everyone loves a pretty picture, right? But before we do, I want to end on an important point. A traditional first course in statistics spends only a small proportion of the class on descriptive statistics, maybe one or two lectures at most. The vast majority of the lecturer’s time is spent on inferential statistics, because that’s where all the hard stuff is. That makes sense, but it hides the practical everyday importance of choosing good descriptives. With that in mind…\n\n\n\n\nThe death of one man is a tragedy. The death of millions is a statistic.\n– Josef Stalin, Potsdam 1945\n\n\n950,000 – 1,200,000\n– Estimate of Soviet repression deaths, 1937-1938 [@Ellman2002]\n\nStalin’s infamous quote about the statistical character death of millions is worth giving some thought. The clear intent of his statement is that the death of an individual touches us personally and its force cannot be denied, but that the deaths of a multitude are incomprehensible, and as a consequence mere statistics, more easily ignored. I’d argue that Stalin was half right. A statistic is an abstraction, a description of events beyond our personal experience, and so hard to visualise. Few if any of us can imagine what the deaths of millions is “really” like, but we can imagine one death, and this gives the lone death its feeling of immediate tragedy, a feeling that is missing from Ellman’s cold statistical description.\nYet it is not so simple: without numbers, without counts, without a description of what happened, we have no chance of understanding what really happened, no opportunity event to try to summon the missing feeling. And in truth, as I write this, sitting in comfort on a Saturday morning, half a world and a whole lifetime away from the Gulags, when I put the Ellman estimate next to the Stalin quote a dull dread settles in my stomach and a chill settles over me. The Stalinist repression is something truly beyond my experience, but with a combination of statistical data and those recorded personal histories that have come down to us, it is not entirely beyond my comprehension. Because what Ellman’s numbers tell us is this: over a two year period, Stalinist repression wiped out the equivalent of every man, woman and child currently alive in the city where I live. Each one of those deaths had it’s own story, was it’s own tragedy, and only some of those are known to us now. Even so, with a few carefully chosen statistics, the scale of the atrocity starts to come into focus.\nThus it is no small thing to say that the first task of the statistician and the scientist is to summarise the data, to find some collection of numbers that can convey to an audience a sense of what has happened. This is the job of descriptive statistics, but it’s not a job that can be told solely using the numbers. You are a data analyst, not a statistical software package. Part of your job is to take these statistics and turn them into a description. When you analyse data, it is not sufficient to list off a collection of numbers. Always remember that what you’re really trying to do is communicate with a human audience. The numbers are important, but they need to be put together into a meaningful story that your audience can interpret. That means you need to think about framing. You need to think about context. And you need to think about the individual events that your statistics are summarising."
  },
  {
    "objectID": "materials/descriptives.html#centraltendency",
    "href": "materials/descriptives.html#centraltendency",
    "title": "Descriptive statistics",
    "section": "",
    "text": "Drawing pictures of the data, as I did in Figure @ref(fig:histogram1) is an excellent way to convey the “gist” of what the data is trying to tell you, it’s often extremely useful to try to condense the data into a few simple “summary” statistics. In most situations, the first thing that you’ll want to calculate is a measure of central tendency. That is, you’d like to know something about the “average” or “middle” of your data lies. The two most commonly used measures are the mean, median and mode; occasionally people will also report a trimmed mean. I’ll explain each of these in turn, and then discuss when each of them is useful.\n\n\nThe mean of a set of observations is just a normal, old-fashioned average: add all of the values up, and then divide by the total number of values. The first five AFL margins were 56, 31, 56, 8 and 32, so the mean of these observations is just: \\[\n\\frac{56 + 31 + 56 + 8 + 32}{5} = \\frac{183}{5} = 36.60\n\\] Of course, this definition of the mean isn’t news to anyone: averages (i.e., means) are used so often in everyday life that this is pretty familiar stuff. However, since the concept of a mean is something that everyone already understands, I’ll use this as an excuse to start introducing some of the mathematical notation that statisticians use to describe this calculation, and talk about how the calculations would be done in R.\nThe first piece of notation to introduce is \\(N\\), which we’ll use to refer to the number of observations that we’re averaging (in this case \\(N = 5\\)). Next, we need to attach a label to the observations themselves. It’s traditional to use \\(X\\) for this, and to use subscripts to indicate which observation we’re actually talking about. That is, we’ll use \\(X_1\\) to refer to the first observation, \\(X_2\\) to refer to the second observation, and so on, all the way up to \\(X_N\\) for the last one. Or, to say the same thing in a slightly more abstract way, we use \\(X_i\\) to refer to the \\(i\\)-th observation. Just to make sure we’re clear on the notation, the following table lists the 5 observations in the afl.margins variable, along with the mathematical symbol used to refer to it, and the actual value that the observation corresponds to:\n\n\n\n\n\nthe observation\nits symbol\nthe observed value\n\n\n\n\nwinning margin, game 1\n\\(X_1\\)\n56 points\n\n\nwinning margin, game 2\n\\(X_2\\)\n31 points\n\n\nwinning margin, game 3\n\\(X_3\\)\n56 points\n\n\nwinning margin, game 4\n\\(X_4\\)\n8 points\n\n\nwinning margin, game 5\n\\(X_5\\)\n32 points\n\n\n\n\n\nOkay, now let’s try to write a formula for the mean. By tradition, we use \\(\\bar{X}\\) as the notation for the mean. So the calculation for the mean could be expressed using the following formula: \\[\n\\bar{X} = \\frac{X_1 + X_2 + ... + X_{N-1} + X_N}{N}\n\\] This formula is entirely correct, but it’s terribly long, so we make use of the summation symbol \\(\\scriptstyle\\sum\\) to shorten it.2 If I want to add up the first five observations, I could write out the sum the long way, \\(X_1 + X_2 + X_3 + X_4 +X_5\\) or I could use the summation symbol to shorten it to this: \\[\n\\sum_{i=1}^5 X_i\n\\] Taken literally, this could be read as “the sum, taken over all \\(i\\) values from 1 to 5, of the value \\(X_i\\)”. But basically, what it means is “add up the first five observations”. In any case, we can use this notation to write out the formula for the mean, which looks like this: \\[\n\\bar{X} = \\frac{1}{N} \\sum_{i=1}^N X_i\n\\]\nIn all honesty, I can’t imagine that all this mathematical notation helps clarify the concept of the mean at all. In fact, it’s really just a fancy way of writing out the same thing I said in words: add all the values up, and then divide by the total number of items. However, that’s not really the reason I went into all that detail. My goal was to try to make sure that everyone reading this book is clear on the notation that we’ll be using throughout the book: \\(\\bar{X}\\) for the mean, \\(\\scriptstyle\\sum\\) for the idea of summation, \\(X_i\\) for the \\(i\\)th observation, and \\(N\\) for the total number of observations. We’re going to be re-using these symbols a fair bit, so it’s important that you understand them well enough to be able to “read” the equations, and to be able to see that it’s just saying “add up lots of things and then divide by another thing”.\n\n\n\nOkay that’s the maths, how do we get the magic computing box to do the work for us? If you really wanted to, you could do this calculation directly in R. For the first 5 AFL scores, do this just by typing it in as if R were a calculator…\n\n(56 + 31 + 56 + 8 + 32) / 5\n\n[1] 36.6\n\n\n… in which case R outputs the answer 36.6, just as if it were a calculator. However, that’s not the only way to do the calculations, and when the number of observations starts to become large, it’s easily the most tedious. Besides, in almost every real world scenario, you’ve already got the actual numbers stored in a variable of some kind, just like we have with the afl.margins variable. Under those circumstances, what you want is a function that will just add up all the values stored in a numeric vector. That’s what the sum() function does. If we want to add up all 176 winning margins in the data set, we can do so using the following command:3\n\nsum( afl.margins )\n\n[1] 6213\n\n\nIf we only want the sum of the first five observations, then we can use square brackets to pull out only the first five elements of the vector. So the command would now be:\n\nsum( afl.margins[1:5] )\n\n[1] 183\n\n\nTo calculate the mean, we now tell R to divide the output of this summation by five, so the command that we need to type now becomes the following:\n\nsum( afl.margins[1:5] ) / 5\n\n[1] 36.6\n\n\nAlthough it’s pretty easy to calculate the mean using the sum() function, we can do it in an even easier way, since R also provides us with the mean() function. To calculate the mean for all 176 games, we would use the following command:\n\nmean( x = afl.margins )\n\n[1] 35.30114\n\n\nHowever, since x is the first argument to the function, I could have omitted the argument name. In any case, just to show you that there’s nothing funny going on, here’s what we would do to calculate the mean for the first five observations:\n\nmean( afl.margins[1:5] )\n\n[1] 36.6\n\n\nAs you can see, this gives exactly the same answers as the previous calculations.\n\n\n\nThe second measure of central tendency that people use a lot is the median, and it’s even easier to describe than the mean. The median of a set of observations is just the middle value. As before let’s imagine we were interested only in the first 5 AFL winning margins: 56, 31, 56, 8 and 32. To figure out the median, we sort these numbers into ascending order: \\[\n8, 31, \\mathbf{32}, 56, 56\n\\] From inspection, it’s obvious that the median value of these 5 observations is 32, since that’s the middle one in the sorted list (I’ve put it in bold to make it even more obvious). Easy stuff. But what should we do if we were interested in the first 6 games rather than the first 5? Since the sixth game in the season had a winning margin of 14 points, our sorted list is now \\[\n8, 14, \\mathbf{31}, \\mathbf{32}, 56, 56\n\\] and there are two middle numbers, 31 and 32. The median is defined as the average of those two numbers, which is of course 31.5. As before, it’s very tedious to do this by hand when you’ve got lots of numbers. To illustrate this, here’s what happens when you use R to sort all 176 winning margins. First, I’ll use the sort() function (discussed in Chapter @ref(datahandling)) to display the winning margins in increasing numerical order:\n\nsort( x = afl.margins )\n\n  [1]   0   0   1   1   1   1   2   2   3   3   3   3   3   3   3   3   4   4\n [19]   5   6   7   7   8   8   8   8   8   9   9   9   9   9   9  10  10  10\n [37]  10  10  11  11  11  12  12  12  13  14  14  15  16  16  16  16  18  19\n [55]  19  19  19  19  20  20  20  21  21  22  22  22  23  23  23  24  24  25\n [73]  25  26  26  26  26  27  27  28  28  29  29  29  29  29  29  30  31  32\n [91]  32  33  35  35  35  35  36  36  36  36  36  36  37  38  38  38  38  38\n[109]  39  39  40  41  42  43  43  44  44  44  44  44  47  47  47  48  48  48\n[127]  49  49  50  50  50  50  52  52  53  53  54  54  55  55  55  56  56  56\n[145]  57  60  61  61  63  64  65  65  66  67  68  70  71  71  72  73  75  75\n[163]  76  81  82  82  83  84  89  94  95  98 101 104 108 116\n\n\nThe middle values are 30 and 31, so the median winning margin for 2010 was 30.5 points. In real life, of course, no-one actually calculates the median by sorting the data and then looking for the middle value. In real life, we use the median command:\n\nmedian( x = afl.margins )\n\n[1] 30.5\n\n\nwhich outputs the median value of 30.5.\n\n\n\n\n\n\n\n\nAn illustration of the difference between how the mean and the median should be interpreted. The mean is basically the “centre of gravity” of the data set: if you imagine that the histogram of the data is a solid object, then the point on which you could balance it (as if on a see-saw) is the mean. In contrast, the median is the middle observation. Half of the observations are smaller, and half of the observations are larger.\n\n\n\n\nKnowing how to calculate means and medians is only a part of the story. You also need to understand what each one is saying about the data, and what that implies for when you should use each one. This is illustrated in Figure @ref(fig:meanmedian) the mean is kind of like the “centre of gravity” of the data set, whereas the median is the “middle value” in the data. What this implies, as far as which one you should use, depends a little on what type of data you’ve got and what you’re trying to achieve. As a rough guide:\n\nIf your data are nominal scale, you probably shouldn’t be using either the mean or the median. Both the mean and the median rely on the idea that the numbers assigned to values are meaningful. If the numbering scheme is arbitrary, then it’s probably best to use the mode (Section @ref(mode)) instead.\nIf your data are ordinal scale, you’re more likely to want to use the median than the mean. The median only makes use of the order information in your data (i.e., which numbers are bigger), but doesn’t depend on the precise numbers involved. That’s exactly the situation that applies when your data are ordinal scale. The mean, on the other hand, makes use of the precise numeric values assigned to the observations, so it’s not really appropriate for ordinal data.\nFor interval and ratio scale data, either one is generally acceptable. Which one you pick depends a bit on what you’re trying to achieve. The mean has the advantage that it uses all the information in the data (which is useful when you don’t have a lot of data), but it’s very sensitive to extreme values, as we’ll see in Section @ref(trimmedmean).\n\nLet’s expand on that last part a little. One consequence is that there’s systematic differences between the mean and the median when the histogram is asymmetric (skewed; see Section @ref(skewandkurtosis)). This is illustrated in Figure @ref(fig:meanmedian) notice that the median (right hand side) is located closer to the “body” of the histogram, whereas the mean (left hand side) gets dragged towards the “tail” (where the extreme values are). To give a concrete example, suppose Bob (income $50,000), Kate (income $60,000) and Jane (income $65,000) are sitting at a table: the average income at the table is $58,333 and the median income is $60,000. Then Bill sits down with them (income $100,000,000). The average income has now jumped to $25,043,750 but the median rises only to $62,500. If you’re interested in looking at the overall income at the table, the mean might be the right answer; but if you’re interested in what counts as a typical income at the table, the median would be a better choice here.\n\n\n\nTo try to get a sense of why you need to pay attention to the differences between the mean and the median, let’s consider a real life example. Since I tend to mock journalists for their poor scientific and statistical knowledge, I should give credit where credit is due. This is from an excellent article on the ABC news website4 24 September, 2010:\n\nSenior Commonwealth Bank executives have travelled the world in the past couple of weeks with a presentation showing how Australian house prices, and the key price to income ratios, compare favourably with similar countries. “Housing affordability has actually been going sideways for the last five to six years,” said Craig James, the chief economist of the bank’s trading arm, CommSec.\n\nThis probably comes as a huge surprise to anyone with a mortgage, or who wants a mortgage, or pays rent, or isn’t completely oblivious to what’s been going on in the Australian housing market over the last several years. Back to the article:\n\nCBA has waged its war against what it believes are housing doomsayers with graphs, numbers and international comparisons. In its presentation, the bank rejects arguments that Australia’s housing is relatively expensive compared to incomes. It says Australia’s house price to household income ratio of 5.6 in the major cities, and 4.3 nationwide, is comparable to many other developed nations. It says San Francisco and New York have ratios of 7, Auckland’s is 6.7, and Vancouver comes in at 9.3.\n\nMore excellent news! Except, the article goes on to make the observation that…\n\nMany analysts say that has led the bank to use misleading figures and comparisons. If you go to page four of CBA’s presentation and read the source information at the bottom of the graph and table, you would notice there is an additional source on the international comparison – Demographia. However, if the Commonwealth Bank had also used Demographia’s analysis of Australia’s house price to income ratio, it would have come up with a figure closer to 9 rather than 5.6 or 4.3\n\nThat’s, um, a rather serious discrepancy. One group of people say 9, another says 4-5. Should we just split the difference, and say the truth lies somewhere in between? Absolutely not: this is a situation where there is a right answer and a wrong answer. Demographia are correct, and the Commonwealth Bank is incorrect. As the article points out\n\n[An] obvious problem with the Commonwealth Bank’s domestic price to income figures is they compare average incomes with median house prices (unlike the Demographia figures that compare median incomes to median prices). The median is the mid-point, effectively cutting out the highs and lows, and that means the average is generally higher when it comes to incomes and asset prices, because it includes the earnings of Australia’s wealthiest people. To put it another way: the Commonwealth Bank’s figures count Ralph Norris’ multi-million dollar pay packet on the income side, but not his (no doubt) very expensive house in the property price figures, thus understating the house price to income ratio for middle-income Australians.\n\nCouldn’t have put it better myself. The way that Demographia calculated the ratio is the right thing to do. The way that the Bank did it is incorrect. As for why an extremely quantitatively sophisticated organisation such as a major bank made such an elementary mistake, well… I can’t say for sure, since I have no special insight into their thinking, but the article itself does happen to mention the following facts, which may or may not be relevant:\n\n[As] Australia’s largest home lender, the Commonwealth Bank has one of the biggest vested interests in house prices rising. It effectively owns a massive swathe of Australian housing as security for its home loans as well as many small business loans.\n\nMy, my.\n\n\n\nOne of the fundamental rules of applied statistics is that the data are messy. Real life is never simple, and so the data sets that you obtain are never as straightforward as the statistical theory says.5 This can have awkward consequences. To illustrate, consider this rather strange looking data set: \\[\n-100,2,3,4,5,6,7,8,9,10\n\\] If you were to observe this in a real life data set, you’d probably suspect that something funny was going on with the \\(-100\\) value. It’s probably an outlier, a value that doesn’t really belong with the others. You might consider removing it from the data set entirely, and in this particular case I’d probably agree with that course of action. In real life, however, you don’t always get such cut-and-dried examples. For instance, you might get this instead: \\[\n-15,2,3,4,5,6,7,8,9,12\n\\] The \\(-15\\) looks a bit suspicious, but not anywhere near as much as that \\(-100\\) did. In this case, it’s a little trickier. It might be a legitimate observation, it might not.\nWhen faced with a situation where some of the most extreme-valued observations might not be quite trustworthy, the mean is not necessarily a good measure of central tendency. It is highly sensitive to one or two extreme values, and is thus not considered to be a robust measure. One remedy that we’ve seen is to use the median. A more general solution is to use a “trimmed mean”. To calculate a trimmed mean, what you do is “discard” the most extreme examples on both ends (i.e., the largest and the smallest), and then take the mean of everything else. The goal is to preserve the best characteristics of the mean and the median: just like a median, you aren’t highly influenced by extreme outliers, but like the mean, you “use” more than one of the observations. Generally, we describe a trimmed mean in terms of the percentage of observation on either side that are discarded. So, for instance, a 10% trimmed mean discards the largest 10% of the observations and the smallest 10% of the observations, and then takes the mean of the remaining 80% of the observations. Not surprisingly, the 0% trimmed mean is just the regular mean, and the 50% trimmed mean is the median. In that sense, trimmed means provide a whole family of central tendency measures that span the range from the mean to the median.\nFor our toy example above, we have 10 observations, and so a 10% trimmed mean is calculated by ignoring the largest value (i.e., 12) and the smallest value (i.e., -15) and taking the mean of the remaining values. First, let’s enter the data\n\ndataset &lt;- c( -15,2,3,4,5,6,7,8,9,12 )\n\nNext, let’s calculate means and medians:\n\nmean( x = dataset )\n\n[1] 4.1\n\nmedian( x = dataset )\n\n[1] 5.5\n\n\nThat’s a fairly substantial difference, but I’m tempted to think that the mean is being influenced a bit too much by the extreme values at either end of the data set, especially the \\(-15\\) one. So let’s just try trimming the mean a bit. If I take a 10% trimmed mean, we’ll drop the extreme values on either side, and take the mean of the rest:\n\nmean( x = dataset, trim = .1)\n\n[1] 5.5\n\n\nwhich in this case gives exactly the same answer as the median. Note that, to get a 10% trimmed mean you write trim = .1, not trim = 10. In any case, let’s finish up by calculating the 5% trimmed mean for the afl.margins data,\n\nmean( x = afl.margins, trim = .05)  \n\n[1] 33.75\n\n\n\n\n\nThe mode of a sample is very simple: it is the value that occurs most frequently. To illustrate the mode using the AFL data, let’s examine a different aspect to the data set. Who has played in the most finals? The afl.finalists variable is a factor that contains the name of every team that played in any AFL final from 1987-2010, so let’s have a look at it. To do this we will use the head() command. head() is useful when you’re working with a data.frame with a lot of rows since you can use it to tell you how many rows to return. There have been a lot of finals in this period so printing afl.finalists using print(afl.finalists) will just fill us the screen. The command below tells R we just want the first 25 rows of the data.frame.\n\nhead(afl.finalists, 25)\n\n [1] Hawthorn    Melbourne   Carlton     Melbourne   Hawthorn    Carlton    \n [7] Melbourne   Carlton     Hawthorn    Melbourne   Melbourne   Hawthorn   \n[13] Melbourne   Essendon    Hawthorn    Geelong     Geelong     Hawthorn   \n[19] Collingwood Melbourne   Collingwood West Coast  Collingwood Essendon   \n[25] Collingwood\n17 Levels: Adelaide Brisbane Carlton Collingwood Essendon Fitzroy ... Western Bulldogs\n\n\nThere are actually 400 entries (aren’t you glad we didn’t print them all?). We could read through all 400, and count the number of occasions on which each team name appears in our list of finalists, thereby producing a frequency table. However, that would be mindless and boring: exactly the sort of task that computers are great at. So let’s use the table() function (discussed in more detail in Section @ref(freqtables)) to do this task for us:\n\ntable( afl.finalists )\n\nafl.finalists\n        Adelaide         Brisbane          Carlton      Collingwood \n              26               25               26               28 \n        Essendon          Fitzroy        Fremantle          Geelong \n              32                0                6               39 \n        Hawthorn        Melbourne  North Melbourne    Port Adelaide \n              27               28               28               17 \n        Richmond         St Kilda           Sydney       West Coast \n               6               24               26               38 \nWestern Bulldogs \n              24 \n\n\nNow that we have our frequency table, we can just look at it and see that, over the 24 years for which we have data, Geelong has played in more finals than any other team. Thus, the mode of the finalists data is \"Geelong\". The core packages in R don’t have a function for calculating the mode6. However, I’ve included a function in the lsr package that does this. The function is called modeOf(), and here’s how you use it:\n\nmodeOf( x = afl.finalists )\n\n[1] \"Geelong\"\n\n\nThere’s also a function called maxFreq() that tells you what the modal frequency is. If we apply this function to our finalists data, we obtain the following:\n\nmaxFreq( x = afl.finalists )\n\n[1] 39\n\n\nTaken together, we observe that Geelong (39 finals) played in more finals than any other team during the 1987-2010 period.\nOne last point to make with respect to the mode. While it’s generally true that the mode is most often calculated when you have nominal scale data (because means and medians are useless for those sorts of variables), there are some situations in which you really do want to know the mode of an ordinal, interval or ratio scale variable. For instance, let’s go back to thinking about our afl.margins variable. This variable is clearly ratio scale (if it’s not clear to you, it may help to re-read Section @ref(scales)), and so in most situations the mean or the median is the measure of central tendency that you want. But consider this scenario… a friend of yours is offering a bet. They pick a football game at random, and (without knowing who is playing) you have to guess the exact margin. If you guess correctly, you win $50. If you don’t, you lose $1. There are no consolation prizes for “almost” getting the right answer. You have to guess exactly the right margin7 For this bet, the mean and the median are completely useless to you. It is the mode that you should bet on. So, we calculate this modal value\n\nmodeOf( x = afl.margins )\n\n[1] 3\n\nmaxFreq( x = afl.margins )\n\n[1] 8\n\n\nSo the 2010 data suggest you should bet on a 3 point margin, and since this was observed in 8 of the 176 game (4.5% of games) the odds are firmly in your favour."
  },
  {
    "objectID": "materials/descriptives.html#var",
    "href": "materials/descriptives.html#var",
    "title": "Descriptive statistics",
    "section": "",
    "text": "The statistics that we’ve discussed so far all relate to central tendency. That is, they all talk about which values are “in the middle” or “popular” in the data. However, central tendency is not the only type of summary statistic that we want to calculate. The second thing that we really want is a measure of the variability of the data. That is, how “spread out” are the data? How “far” away from the mean or median do the observed values tend to be? For now, let’s assume that the data are interval or ratio scale, so we’ll continue to use the afl.margins data. We’ll use this data to discuss several different measures of spread, each with different strengths and weaknesses.\n\n\nThe range of a variable is very simple: it’s the biggest value minus the smallest value. For the AFL winning margins data, the maximum value is 116, and the minimum value is 0. We can calculate these values in R using the max() and min() functions:\n\nmax( afl.margins )\n\n[1] 116\n\nmin( afl.margins )\n\n[1] 0\n\n\nwhere I’ve omitted the output because it’s not interesting. The other possibility is to use the range() function; which outputs both the minimum value and the maximum value in a vector, like this:\n\nrange( afl.margins )\n\n[1]   0 116\n\n\nAlthough the range is the simplest way to quantify the notion of “variability”, it’s one of the worst. Recall from our discussion of the mean that we want our summary measure to be robust. If the data set has one or two extremely bad values in it, we’d like our statistics not to be unduly influenced by these cases. If we look once again at our toy example of a data set containing very extreme outliers… \\[\n-100,2,3,4,5,6,7,8,9,10\n\\] … it is clear that the range is not robust, since this has a range of 110, but if the outlier were removed we would have a range of only 8.\n\n\n\nThe interquartile range (IQR) is like the range, but instead of calculating the difference between the biggest and smallest value, it calculates the difference between the 25th quantile and the 75th quantile. Probably you already know what a quantile is (they’re more commonly called percentiles), but if not: the 10th percentile of a data set is the smallest number \\(x\\) such that 10% of the data is less than \\(x\\). In fact, we’ve already come across the idea: the median of a data set is its 50th quantile / percentile! R actually provides you with a way of calculating quantiles, using the (surprise, surprise) quantile() function. Let’s use it to calculate the median AFL winning margin:\n\nquantile( x = afl.margins, probs = .5)\n\n 50% \n30.5 \n\n\nAnd not surprisingly, this agrees with the answer that we saw earlier with the median() function. Now, we can actually input lots of quantiles at once, by specifying a vector for the probs argument. So lets do that, and get the 25th and 75th percentile:\n\nquantile( x = afl.margins, probs = c(.25,.75) )\n\n  25%   75% \n12.75 50.50 \n\n\nAnd, by noting that \\(50.5 - 12.75 = 37.75\\), we can see that the interquartile range for the 2010 AFL winning margins data is 37.75. Of course, that seems like too much work to do all that typing, so R has a built in function called IQR() that we can use:\n\nIQR( x = afl.margins )\n\n[1] 37.75\n\n\nWhile it’s obvious how to interpret the range, it’s a little less obvious how to interpret the IQR. The simplest way to think about it is like this: the interquartile range is the range spanned by the “middle half” of the data. That is, one quarter of the data falls below the 25th percentile, one quarter of the data is above the 75th percentile, leaving the “middle half” of the data lying in between the two. And the IQR is the range covered by that middle half.\n\n\n\nThe two measures we’ve looked at so far, the range and the interquartile range, both rely on the idea that we can measure the spread of the data by looking at the quantiles of the data. However, this isn’t the only way to think about the problem. A different approach is to select a meaningful reference point (usually the mean or the median) and then report the “typical” deviations from that reference point. What do we mean by “typical” deviation? Usually, the mean or median value of these deviations! In practice, this leads to two different measures, the “mean absolute deviation (from the mean)” and the “median absolute deviation (from the median)”. From what I’ve read, the measure based on the median seems to be used in statistics, and does seem to be the better of the two, but to be honest I don’t think I’ve seen it used much in psychology. The measure based on the mean does occasionally show up in psychology though. In this section I’ll talk about the first one, and I’ll come back to talk about the second one later.\nSince the previous paragraph might sound a little abstract, let’s go through the mean absolute deviation from the mean a little more slowly. One useful thing about this measure is that the name actually tells you exactly how to calculate it. Let’s think about our AFL winning margins data, and once again we’ll start by pretending that there’s only 5 games in total, with winning margins of 56, 31, 56, 8 and 32. Since our calculations rely on an examination of the deviation from some reference point (in this case the mean), the first thing we need to calculate is the mean, \\(\\bar{X}\\). For these five observations, our mean is \\(\\bar{X} = 36.6\\). The next step is to convert each of our observations \\(X_i\\) into a deviation score. We do this by calculating the difference between the observation \\(X_i\\) and the mean \\(\\bar{X}\\). That is, the deviation score is defined to be \\(X_i - \\bar{X}\\). For the first observation in our sample, this is equal to \\(56 - 36.6 = 19.4\\). Okay, that’s simple enough. The next step in the process is to convert these deviations to absolute deviations. As we discussed earlier when talking about the abs() function in R (Section @ref(usingfunctions)), we do this by converting any negative values to positive ones. Mathematically, we would denote the absolute value of \\(-3\\) as \\(|-3|\\), and so we say that \\(|-3| = 3\\). We use the absolute value function here because we don’t really care whether the value is higher than the mean or lower than the mean, we’re just interested in how close it is to the mean. To help make this process as obvious as possible, the table below shows these calculations for all five observations:\n\n\n\n\n\nthe observation\nits symbol\nthe observed value\n\n\n\n\nwinning margin, game 1\n\\(X_1\\)\n56 points\n\n\nwinning margin, game 2\n\\(X_2\\)\n31 points\n\n\nwinning margin, game 3\n\\(X_3\\)\n56 points\n\n\nwinning margin, game 4\n\\(X_4\\)\n8 points\n\n\nwinning margin, game 5\n\\(X_5\\)\n32 points\n\n\n\n\n\nNow that we have calculated the absolute deviation score for every observation in the data set, all that we have to do to calculate the mean of these scores. Let’s do that: \\[\n\\frac{19.4 + 5.6 + 19.4 + 28.6 + 4.6}{5} = 15.52\n\\] And we’re done. The mean absolute deviation for these five scores is 15.52.\nHowever, while our calculations for this little example are at an end, we do have a couple of things left to talk about. Firstly, we should really try to write down a proper mathematical formula. But in order do to this I need some mathematical notation to refer to the mean absolute deviation. Irritatingly, “mean absolute deviation” and “median absolute deviation” have the same acronym (MAD), which leads to a certain amount of ambiguity, and since R tends to use MAD to refer to the median absolute deviation, I’d better come up with something different for the mean absolute deviation. Sigh. What I’ll do is use AAD instead, short for average absolute deviation. Now that we have some unambiguous notation, here’s the formula that describes what we just calculated: \\[\n\\mbox{}(X) = \\frac{1}{N} \\sum_{i = 1}^N |X_i - \\bar{X}|\n\\]\nThe last thing we need to talk about is how to calculate AAD in R. One possibility would be to do everything using low level commands, laboriously following the same steps that I used when describing the calculations above. However, that’s pretty tedious. You’d end up with a series of commands that might look like this:\n\nX &lt;- c(56, 31,56,8,32)   # enter the data\nX.bar &lt;- mean( X )       # step 1. the mean of the data\nAD &lt;- abs( X - X.bar )   # step 2. the absolute deviations from the mean\nAAD &lt;- mean( AD )        # step 3. the mean absolute deviations\nprint( AAD )             # print the results\n\n[1] 15.52\n\n\nEach of those commands is pretty simple, but there’s just too many of them. And because I find that to be too much typing, the lsr package has a very simple function called aad() that does the calculations for you. If we apply the aad() function to our data, we get this:\n\nlibrary(lsr)\naad( X )\n\n[1] 15.52\n\n\nNo suprises there.\n\n\n\nAlthough the mean absolute deviation measure has its uses, it’s not the best measure of variability to use. From a purely mathematical perspective, there are some solid reasons to prefer squared deviations rather than absolute deviations. If we do that, we obtain a measure is called the variance, which has a lot of really nice statistical properties that I’m going to ignore,8 and one massive psychological flaw that I’m going to make a big deal out of in a moment. The variance of a data set \\(X\\) is sometimes written as \\(\\mbox{Var}(X)\\), but it’s more commonly denoted \\(s^2\\) (the reason for this will become clearer shortly). The formula that we use to calculate the variance of a set of observations is as follows: \\[\n\\mbox{Var}(X) = \\frac{1}{N} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2\n\\] \\[\\mbox{Var}(X) = \\frac{\\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2}{N}\\] As you can see, it’s basically the same formula that we used to calculate the mean absolute deviation, except that instead of using “absolute deviations” we use “squared deviations”. It is for this reason that the variance is sometimes referred to as the “mean square deviation”.\nNow that we’ve got the basic idea, let’s have a look at a concrete example. Once again, let’s use the first five AFL games as our data. If we follow the same approach that we took last time, we end up with the following table:\n\n\n\nBasic arithmetic operations in R. These five operators are used very frequently throughout the text, so it’s important to be familiar with them at the outset.\n\n\n\n\n\n\n\n\n\nNotation [English]\n\\(i\\) [which game]\n\\(X_i\\) [value]\n\\(X_i - \\bar{X}\\) [deviation from mean]\n\\((X_i - \\bar{X})^2\\) [absolute deviation]\n\n\n\n\n\n1\n56\n19.4\n376.36\n\n\n\n2\n31\n-5.6\n31.36\n\n\n\n3\n56\n19.4\n376.36\n\n\n\n4\n8\n-28.6\n817.96\n\n\n\n5\n32\n-4.6\n21.16\n\n\n\n\n\nThat last column contains all of our squared deviations, so all we have to do is average them. If we do that by typing all the numbers into R by hand…\n\n( 376.36 + 31.36 + 376.36 + 817.96 + 21.16 ) / 5\n\n[1] 324.64\n\n\n… we end up with a variance of 324.64. Exciting, isn’t it? For the moment, let’s ignore the burning question that you’re all probably thinking (i.e., what the heck does a variance of 324.64 actually mean?) and instead talk a bit more about how to do the calculations in R, because this will reveal something very weird.\nAs always, we want to avoid having to type in a whole lot of numbers ourselves. And as it happens, we have the vector X lying around, which we created in the previous section. With this in mind, we can calculate the variance of X by using the following command,\n\nmean( (X - mean(X) )^2)\n\n[1] 324.64\n\n\nand as usual we get the same answer as the one that we got when we did everything by hand. However, I still think that this is too much typing. Fortunately, R has a built in function called var() which does calculate variances. So we could also do this…\n\nvar(X)\n\n[1] 405.8\n\n\nand you get the same… no, wait… you get a completely different answer. That’s just weird. Is R broken? Is this a typo? Is Dan an idiot?\nAs it happens, the answer is no.9 It’s not a typo, and R is not making a mistake. To get a feel for what’s happening, let’s stop using the tiny data set containing only 5 data points, and switch to the full set of 176 games that we’ve got stored in our afl.margins vector. First, let’s calculate the variance by using the formula that I described above:\n\nmean( (afl.margins - mean(afl.margins) )^2)\n\n[1] 675.9718\n\n\nNow let’s use the var() function:\n\nvar( afl.margins )\n\n[1] 679.8345\n\n\nHm. These two numbers are very similar this time. That seems like too much of a coincidence to be a mistake. And of course it isn’t a mistake. In fact, it’s very simple to explain what R is doing here, but slightly trickier to explain why R is doing it. So let’s start with the “what”. What R is doing is evaluating a slightly different formula to the one I showed you above. Instead of averaging the squared deviations, which requires you to divide by the number of data points \\(N\\), R has chosen to divide by \\(N-1\\). In other words, the formula that R is using is this one\n\\[\n\\frac{1}{N-1} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2\n\\] It’s easy enough to verify that this is what’s happening, as the following command illustrates:\n\nsum( (X-mean(X))^2 ) / 4\n\n[1] 405.8\n\n\nThis is the same answer that R gave us originally when we calculated var(X) originally. So that’s the what. The real question is why R is dividing by \\(N-1\\) and not by \\(N\\). After all, the variance is supposed to be the mean squared deviation, right? So shouldn’t we be dividing by \\(N\\), the actual number of observations in the sample? Well, yes, we should. However, as we’ll discuss in Chapter @ref(estimation), there’s a subtle distinction between “describing a sample” and “making guesses about the population from which the sample came”. Up to this point, it’s been a distinction without a difference. Regardless of whether you’re describing a sample or drawing inferences about the population, the mean is calculated exactly the same way. Not so for the variance, or the standard deviation, or for many other measures besides. What I outlined to you initially (i.e., take the actual average, and thus divide by \\(N\\)) assumes that you literally intend to calculate the variance of the sample. Most of the time, however, you’re not terribly interested in the sample in and of itself. Rather, the sample exists to tell you something about the world. If so, you’re actually starting to move away from calculating a “sample statistic”, and towards the idea of estimating a “population parameter”. However, I’m getting ahead of myself. For now, let’s just take it on faith that R knows what it’s doing, and we’ll revisit the question later on when we talk about estimation in Chapter @ref(estimation).\nOkay, one last thing. This section so far has read a bit like a mystery novel. I’ve shown you how to calculate the variance, described the weird “\\(N-1\\)” thing that R does and hinted at the reason why it’s there, but I haven’t mentioned the single most important thing… how do you interpret the variance? Descriptive statistics are supposed to describe things, after all, and right now the variance is really just a gibberish number. Unfortunately, the reason why I haven’t given you the human-friendly interpretation of the variance is that there really isn’t one. This is the most serious problem with the variance. Although it has some elegant mathematical properties that suggest that it really is a fundamental quantity for expressing variation, it’s completely useless if you want to communicate with an actual human… variances are completely uninterpretable in terms of the original variable! All the numbers have been squared, and they don’t mean anything anymore. This is a huge issue. For instance, according to the table I presented earlier, the margin in game 1 was “376.36 points-squared higher than the average margin”. This is exactly as stupid as it sounds; and so when we calculate a variance of 324.64, we’re in the same situation. I’ve watched a lot of footy games, and never has anyone referred to “points squared”. It’s not a real unit of measurement, and since the variance is expressed in terms of this gibberish unit, it is totally meaningless to a human.\n\n\n\nOkay, suppose that you like the idea of using the variance because of those nice mathematical properties that I haven’t talked about, but – since you’re a human and not a robot – you’d like to have a measure that is expressed in the same units as the data itself (i.e., points, not points-squared). What should you do? The solution to the problem is obvious: take the square root of the variance, known as the standard deviation, also called the “root mean squared deviation”, or RMSD. This solves out problem fairly neatly: while nobody has a clue what “a variance of 324.68 points-squared” really means, it’s much easier to understand “a standard deviation of 18.01 points”, since it’s expressed in the original units. It is traditional to refer to the standard deviation of a sample of data as \\(s\\), though “sd” and “std dev.” are also used at times. Because the standard deviation is equal to the square root of the variance, you probably won’t be surprised to see that the formula is: \\[\ns = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2 }\n\\] and the R function that we use to calculate it is sd(). However, as you might have guessed from our discussion of the variance, what R actually calculates is slightly different to the formula given above. Just like the we saw with the variance, what R calculates is a version that divides by \\(N-1\\) rather than \\(N\\). For reasons that will make sense when we return to this topic in Chapter@refch:estimation I’ll refer to this new quantity as \\(\\hat\\sigma\\) (read as: “sigma hat”), and the formula for this is \\[\n\\hat\\sigma = \\sqrt{ \\frac{1}{N-1} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2 }\n\\] With that in mind, calculating standard deviations in R is simple:\n\nsd( afl.margins ) \n\n[1] 26.07364\n\n\nInterpreting standard deviations is slightly more complex. Because the standard deviation is derived from the variance, and the variance is a quantity that has little to no meaning that makes sense to us humans, the standard deviation doesn’t have a simple interpretation. As a consequence, most of us just rely on a simple rule of thumb: in general, you should expect 68% of the data to fall within 1 standard deviation of the mean, 95% of the data to fall within 2 standard deviation of the mean, and 99.7% of the data to fall within 3 standard deviations of the mean. This rule tends to work pretty well most of the time, but it’s not exact: it’s actually calculated based on an assumption that the histogram is symmetric and “bell shaped”.10 As you can tell from looking at the AFL winning margins histogram in Figure @ref(fig:histogram1), this isn’t exactly true of our data! Even so, the rule is approximately correct. As it turns out, 65.3% of the AFL margins data fall within one standard deviation of the mean. This is shown visually in Figure @ref(fig:aflsd).\n\n\n\n\n\nAn illustration of the standard deviation, applied to the AFL winning margins data. The shaded bars in the histogram show how much of the data fall within one standard deviation of the mean. In this case, 65.3% of the data set lies within this range, which is pretty consistent with the “approximately 68% rule” discussed in the main text.\n\n\n\n\n\n\n\nThe last measure of variability that I want to talk about is the median absolute deviation (MAD). The basic idea behind MAD is very simple, and is pretty much identical to the idea behind the mean absolute deviation (Section @ref(aad)). The difference is that you use the median everywhere. If we were to frame this idea as a pair of R commands, they would look like this:\n\n# mean absolute deviation from the mean:\nmean( abs(afl.margins - mean(afl.margins)) )\n\n[1] 21.10124\n\n# *median* absolute deviation from the *median*:\nmedian( abs(afl.margins - median(afl.margins)) )\n\n[1] 19.5\n\n\nThis has a straightforward interpretation: every observation in the data set lies some distance away from the typical value (the median). So the MAD is an attempt to describe a typical deviation from a typical value in the data set. It wouldn’t be unreasonable to interpret the MAD value of 19.5 for our AFL data by saying something like this:\n\nThe median winning margin in 2010 was 30.5, indicating that a typical game involved a winning margin of about 30 points. However, there was a fair amount of variation from game to game: the MAD value was 19.5, indicating that a typical winning margin would differ from this median value by about 19-20 points.\n\nAs you’d expect, R has a built in function for calculating MAD, and you will be shocked no doubt to hear that it’s called mad(). However, it’s a little bit more complicated than the functions that we’ve been using previously. If you want to use it to calculate MAD in the exact same way that I have described it above, the command that you need to use specifies two arguments: the data set itself x, and a constant that I’ll explain in a moment. For our purposes, the constant is 1, so our command becomes\n\nmad( x = afl.margins, constant = 1 )\n\n[1] 19.5\n\n\nApart from the weirdness of having to type that constant = 1 part, this is pretty straightforward.\nOkay, so what exactly is this constant = 1 argument? I won’t go into all the details here, but here’s the gist. Although the “raw” MAD value that I’ve described above is completely interpretable on its own terms, that’s not actually how it’s used in a lot of real world contexts. Instead, what happens a lot is that the researcher actually wants to calculate the standard deviation. However, in the same way that the mean is very sensitive to extreme values, the standard deviation is vulnerable to the exact same issue. So, in much the same way that people sometimes use the median as a “robust” way of calculating “something that is like the mean”, it’s not uncommon to use MAD as a method for calculating “something that is like the standard deviation”. Unfortunately, the raw MAD value doesn’t do this. Our raw MAD value is 19.5, and our standard deviation was 26.07. However, what some clever person has shown is that, under certain assumptions11, you can multiply the raw MAD value by 1.4826 and obtain a number that is directly comparable to the standard deviation. As a consequence, the default value of constant is 1.4826, and so when you use the mad() command without manually setting a value, here’s what you get:\n\nmad( afl.margins )\n\n[1] 28.9107\n\n\nI should point out, though, that if you want to use this “corrected” MAD value as a robust version of the standard deviation, you really are relying on the assumption that the data are (or at least, are “supposed to be” in some sense) symmetric and basically shaped like a bell curve. That’s really not true for our afl.margins data, so in this case I wouldn’t try to use the MAD value this way.\n\n\n\nWe’ve discussed quite a few measures of spread (range, IQR, MAD, variance and standard deviation), and hinted at their strengths and weaknesses. Here’s a quick summary:\n\nRange. Gives you the full spread of the data. It’s very vulnerable to outliers, and as a consequence it isn’t often used unless you have good reasons to care about the extremes in the data.\nInterquartile range. Tells you where the “middle half” of the data sits. It’s pretty robust, and complements the median nicely. This is used a lot.\nMean absolute deviation. Tells you how far “on average” the observations are from the mean. It’s very interpretable, but has a few minor issues (not discussed here) that make it less attractive to statisticians than the standard deviation. Used sometimes, but not often.\nVariance. Tells you the average squared deviation from the mean. It’s mathematically elegant, and is probably the “right” way to describe variation around the mean, but it’s completely uninterpretable because it doesn’t use the same units as the data. Almost never used except as a mathematical tool; but it’s buried “under the hood” of a very large number of statistical tools.\nStandard deviation. This is the square root of the variance. It’s fairly elegant mathematically, and it’s expressed in the same units as the data so it can be interpreted pretty well. In situations where the mean is the measure of central tendency, this is the default. This is by far the most popular measure of variation.\nMedian absolute deviation. The typical (i.e., median) deviation from the median value. In the raw form it’s simple and interpretable; in the corrected form it’s a robust way to estimate the standard deviation, for some kinds of data sets. Not used very often, but it does get reported sometimes.\n\nIn short, the IQR and the standard deviation are easily the two most common measures used to report the variability of the data; but there are situations in which the others are used. I’ve described all of them in this book because there’s a fair chance you’ll run into most of these somewhere."
  },
  {
    "objectID": "materials/descriptives.html#skewandkurtosis",
    "href": "materials/descriptives.html#skewandkurtosis",
    "title": "Descriptive statistics",
    "section": "",
    "text": "There are two more descriptive statistics that you will sometimes see reported in the psychological literature, known as skew and kurtosis. In practice, neither one is used anywhere near as frequently as the measures of central tendency and variability that we’ve been talking about. Skew is pretty important, so you do see it mentioned a fair bit; but I’ve actually never seen kurtosis reported in a scientific article to date.\n\n\n[1] -0.9239273\n\n\n[1] -0.005588142\n\n\n\n\n\nAn illustration of skewness. On the left we have a negatively skewed data set (skewness \\(= -.93\\)), in the middle we have a data set with no skew (technically, skewness \\(= -.006\\)), and on the right we have a positively skewed data set (skewness \\(= .93\\)).\n\n\n\n\n[1] 0.9254561\n\n\nSince it’s the more interesting of the two, let’s start by talking about the skewness. Skewness is basically a measure of asymmetry, and the easiest way to explain it is by drawing some pictures. As Figure @ref(fig:skewness) illustrates, if the data tend to have a lot of extreme small values (i.e., the lower tail is “longer” than the upper tail) and not so many extremely large values (left panel), then we say that the data are negatively skewed. On the other hand, if there are more extremely large values than extremely small ones (right panel) we say that the data are positively skewed. That’s the qualitative idea behind skewness. The actual formula for the skewness of a data set is as follows \\[\n\\mbox{skewness}(X) = \\frac{1}{N \\hat{\\sigma}^3} \\sum_{i=1}^N (X_i - \\bar{X})^3\n\\] where \\(N\\) is the number of observations, \\(\\bar{X}\\) is the sample mean, and \\(\\hat{\\sigma}\\) is the standard deviation (the “divide by \\(N-1\\)” version, that is). Perhaps more helpfully, it might be useful to point out that the psych package contains a skew() function that you can use to calculate skewness. So if we wanted to use this function to calculate the skewness of the afl.margins data, we’d first need to load the package\n\nlibrary( psych )\n\nwhich now makes it possible to use the following command:\n\nskew( x = afl.margins )\n\n[1] 0.7671555\n\n\nNot surprisingly, it turns out that the AFL winning margins data is fairly skewed.\nThe final measure that is sometimes referred to, though very rarely in practice, is the kurtosis of a data set. Put simply, kurtosis is a measure of the “pointiness” of a data set, as illustrated in Figure @ref(fig:kurtosis).\n\n\n[1] -0.9521099\n\n\n[1] 0.01635112\n\n\n\n\n\nAn illustration of kurtosis. On the left, we have a “platykurtic” data set (kurtosis = \\(-.95\\)), meaning that the data set is “too flat”. In the middle we have a “mesokurtic” data set (kurtosis is almost exactly 0), which means that the pointiness of the data is just about right. Finally, on the right, we have a “leptokurtic” data set (kurtosis \\(= 2.12\\)) indicating that the data set is “too pointy”. Note that kurtosis is measured with respect to a normal curve (black line)\n\n\n\n\n[1] 2.065825\n\n\nBy convention, we say that the “normal curve” (black lines) has zero kurtosis, so the pointiness of a data set is assessed relative to this curve. In this Figure, the data on the left are not pointy enough, so the kurtosis is negative and we call the data platykurtic. The data on the right are too pointy, so the kurtosis is positive and we say that the data is leptokurtic. But the data in the middle are just pointy enough, so we say that it is mesokurtic and has kurtosis zero. This is summarised in the table below:\n\n\n\n\n\ninformal term\ntechnical name\nkurtosis value\n\n\n\n\ntoo flat\nplatykurtic\nnegative\n\n\njust pointy enough\nmesokurtic\nzero\n\n\ntoo pointy\nleptokurtic\npositive\n\n\n\n\n\nThe equation for kurtosis is pretty similar in spirit to the formulas we’ve seen already for the variance and the skewness; except that where the variance involved squared deviations and the skewness involved cubed deviations, the kurtosis involves raising the deviations to the fourth power:12 \\[\n\\mbox{kurtosis}(X) = \\frac{1}{N \\hat\\sigma^4} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^4  - 3\n\\] I know, it’s not terribly interesting to me either. More to the point, the psych package has a function called kurtosi() that you can use to calculate the kurtosis of your data. For instance, if we were to do this for the AFL margins,\n\nkurtosi( x = afl.margins )\n\n[1] 0.02962633\n\n\nwe discover that the AFL winning margins data are just pointy enough."
  },
  {
    "objectID": "materials/descriptives.html#summary",
    "href": "materials/descriptives.html#summary",
    "title": "Descriptive statistics",
    "section": "",
    "text": "Up to this point in the chapter I’ve explained several different summary statistics that are commonly used when analysing data, along with specific functions that you can use in R to calculate each one. However, it’s kind of annoying to have to separately calculate means, medians, standard deviations, skews etc. Wouldn’t it be nice if R had some helpful functions that would do all these tedious calculations at once? Something like summary() or describe(), perhaps? Why yes, yes it would. So much so that both of these functions exist. The summary() function is in the base package, so it comes with every installation of R. The describe() function is part of the psych package, which we loaded earlier in the chapter.\n\n\nThe summary() function is an easy thing to use, but a tricky thing to understand in full, since it’s a generic function (see Section @ref(generics). The basic idea behind the summary() function is that it prints out some useful information about whatever object (i.e., variable, as far as we’re concerned) you specify as the object argument. As a consequence, the behaviour of the summary() function differs quite dramatically depending on the class of the object that you give it. Let’s start by giving it a numeric object:\n\nsummary( object = afl.margins )  \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00   12.75   30.50   35.30   50.50  116.00 \n\n\nFor numeric variables, we get a whole bunch of useful descriptive statistics. It gives us the minimum and maximum values (i.e., the range), the first and third quartiles (25th and 75th percentiles; i.e., the IQR), the mean and the median. In other words, it gives us a pretty good collection of descriptive statistics related to the central tendency and the spread of the data.\nOkay, what about if we feed it a logical vector instead? Let’s say I want to know something about how many “blowouts” there were in the 2010 AFL season. I operationalise the concept of a blowout (see Chapter @ref(studydesign)) as a game in which the winning margin exceeds 50 points. Let’s create a logical variable blowouts in which the \\(i\\)-th element is TRUE if that game was a blowout according to my definition,\n\nblowouts &lt;-  afl.margins &gt; 50\nblowouts\n\n  [1]  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n [13] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE\n [37]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE\n [49] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE\n [61]  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE\n[109]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n[121] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE\n[133] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE\n[145]  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE\n[157]  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n[169] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nSo that’s what the blowouts variable looks like. Now let’s ask R for a summary()\n\nsummary( object = blowouts )\n\n   Mode   FALSE    TRUE \nlogical     132      44 \n\n\nIn this context, the summary() function gives us a count of the number of TRUE values, the number of FALSE values, and the number of missing values (i.e., the NAs). Pretty reasonable behaviour.\nNext, let’s try to give it a factor. If you recall, I’ve defined the afl.finalists vector as a factor, so let’s use that:\n\nsummary( object = afl.finalists )\n\n        Adelaide         Brisbane          Carlton      Collingwood \n              26               25               26               28 \n        Essendon          Fitzroy        Fremantle          Geelong \n              32                0                6               39 \n        Hawthorn        Melbourne  North Melbourne    Port Adelaide \n              27               28               28               17 \n        Richmond         St Kilda           Sydney       West Coast \n               6               24               26               38 \nWestern Bulldogs \n              24 \n\n\nFor factors, we get a frequency table, just like we got when we used the table() function. Interestingly, however, if we convert this to a character vector using the as.character() function (see Section @ref(coercion), we don’t get the same results:\n\nf2 &lt;- as.character( afl.finalists )\nsummary( object = f2 )\n\n   Length     Class      Mode \n      400 character character \n\n\nThis is one of those situations I was referring to in Section @ref(factors), in which it is helpful to declare your nominal scale variable as a factor rather than a character vector. Because I’ve defined afl.finalists as a factor, R knows that it should treat it as a nominal scale variable, and so it gives you a much more detailed (and helpful) summary than it would have if I’d left it as a character vector.\n\n\n\nOkay what about data frames? When you pass a data frame to the summary() function, it produces a slightly condensed summary of each variable inside the data frame. To give you a sense of how this can be useful, let’s try this for a new data set, one that you’ve never seen before. The data is stored in the clinicaltrial.Rdata file, and we’ll use it a lot in Chapter @ref(anova) (you can find a complete description of the data at the start of that chapter). Let’s load it, and see what we’ve got:\n\nload( \"materials/data/clinicaltrial.Rdata\" )\nwho(TRUE)\n\n   -- Name --    -- Class --   -- Size --\n   clin.trial    data.frame    18 x 3    \n    $drug        factor        18        \n    $therapy     factor        18        \n    $mood.gain   numeric       18        \n\n\nThere’s a single data frame called clin.trial which contains three variables, drug, therapy and mood.gain. Presumably then, this data is from a clinical trial of some kind, in which people were administered different drugs; and the researchers looked to see what the drugs did to their mood. Let’s see if the summary() function sheds a little more light on this situation:\n\nsummary( clin.trial )\n\n       drug         therapy    mood.gain     \n placebo :6   no.therapy:9   Min.   :0.1000  \n anxifree:6   CBT       :9   1st Qu.:0.4250  \n joyzepam:6                  Median :0.8500  \n                             Mean   :0.8833  \n                             3rd Qu.:1.3000  \n                             Max.   :1.8000  \n\n\nEvidently there were three drugs: a placebo, something called “anxifree” and something called “joyzepam”; and there were 6 people administered each drug. There were 9 people treated using cognitive behavioural therapy (CBT) and 9 people who received no psychological treatment. And we can see from looking at the summary of the mood.gain variable that most people did show a mood gain (mean \\(=.88\\)), though without knowing what the scale is here it’s hard to say much more than that. Still, that’s not too bad. Overall, I feel that I learned something from that.\n\n\n\nThe describe() function (in the psych package) is a little different, and it’s really only intended to be useful when your data are interval or ratio scale. Unlike the summary() function, it calculates the same descriptive statistics for any type of variable you give it. By default, these are:\n\nvar. This is just an index: 1 for the first variable, 2 for the second variable, and so on.\nn. This is the sample size: more precisely, it’s the number of non-missing values.\nmean. This is the sample mean (Section @ref(mean)).\nsd. This is the (bias corrected) standard deviation (Section @ref(sd)).\nmedian. The median (Section @ref(median)).\ntrimmed. This is trimmed mean. By default it’s the 10% trimmed mean (Section @ref(trimmedmean)).\nmad. The median absolute deviation (Section @ref(mad)).\nmin. The minimum value.\nmax. The maximum value.\nrange. The range spanned by the data (Section @ref(range)).\nskew. The skewness (Section @ref(skewandkurtosis)).\nkurtosis. The kurtosis (Section @ref(skewandkurtosis)).\nse. The standard error of the mean (Chapter @ref(estimation)).\n\nNotice that these descriptive statistics generally only make sense for data that are interval or ratio scale (usually encoded as numeric vectors). For nominal or ordinal variables (usually encoded as factors), most of these descriptive statistics are not all that useful. What the describe() function does is convert factors and logical variables to numeric vectors in order to do the calculations. These variables are marked with * and most of the time, the descriptive statistics for those variables won’t make much sense. If you try to feed it a data frame that includes a character vector as a variable, it produces an error.\nWith those caveats in mind, let’s use the describe() function to have a look at the clin.trial data frame. Here’s what we get:\n\ndescribe( x = clin.trial )\n\n          vars  n mean   sd median trimmed  mad min max range skew kurtosis\ndrug*        1 18 2.00 0.84   2.00    2.00 1.48 1.0 3.0   2.0 0.00    -1.66\ntherapy*     2 18 1.50 0.51   1.50    1.50 0.74 1.0 2.0   1.0 0.00    -2.11\nmood.gain    3 18 0.88 0.53   0.85    0.88 0.67 0.1 1.8   1.7 0.13    -1.44\n            se\ndrug*     0.20\ntherapy*  0.12\nmood.gain 0.13\n\n\nAs you can see, the output for the asterisked variables is pretty meaningless, and should be ignored. However, for the mood.gain variable, there’s a lot of useful information."
  },
  {
    "objectID": "materials/descriptives.html#groupdescriptives",
    "href": "materials/descriptives.html#groupdescriptives",
    "title": "Descriptive statistics",
    "section": "",
    "text": "It is very commonly the case that you find yourself needing to look at descriptive statistics, broken down by some grouping variable. This is pretty easy to do in R, and there are three functions in particular that are worth knowing about: by(), describeBy() and aggregate(). Let’s start with the describeBy() function, which is part of the psych package. The describeBy() function is very similar to the describe() function, except that it has an additional argument called group which specifies a grouping variable. For instance, let’s say, I want to look at the descriptive statistics for the clin.trial data, broken down separately by therapy type. The command I would use here is:\n\ndescribeBy( x=clin.trial, group=clin.trial$therapy )\n\n\n Descriptive statistics by group \ngroup: no.therapy\n          vars n mean   sd median trimmed  mad min max range skew kurtosis   se\ndrug         1 9 2.00 0.87    2.0    2.00 1.48 1.0 3.0   2.0 0.00    -1.81 0.29\ntherapy      2 9 1.00 0.00    1.0    1.00 0.00 1.0 1.0   0.0  NaN      NaN 0.00\nmood.gain    3 9 0.72 0.59    0.5    0.72 0.44 0.1 1.7   1.6 0.51    -1.59 0.20\n------------------------------------------------------------ \ngroup: CBT\n          vars n mean   sd median trimmed  mad min max range  skew kurtosis\ndrug         1 9 2.00 0.87    2.0    2.00 1.48 1.0 3.0   2.0  0.00    -1.81\ntherapy      2 9 2.00 0.00    2.0    2.00 0.00 2.0 2.0   0.0   NaN      NaN\nmood.gain    3 9 1.04 0.45    1.1    1.04 0.44 0.3 1.8   1.5 -0.03    -1.12\n            se\ndrug      0.29\ntherapy   0.00\nmood.gain 0.15\n\n\nAs you can see, the output is essentially identical to the output that the describe() function produce, except that the output now gives you means, standard deviations etc separately for the CBT group and the no.therapy group. Notice that, as before, the output displays asterisks for factor variables, in order to draw your attention to the fact that the descriptive statistics that it has calculated won’t be very meaningful for those variables. Nevertheless, this command has given us some really useful descriptive statistics mood.gain variable, broken down as a function of therapy.\nA somewhat more general solution is offered by the by() function. There are three arguments that you need to specify when using this function: the data argument specifies the data set, the INDICES argument specifies the grouping variable, and the FUN argument specifies the name of a function that you want to apply separately to each group. To give a sense of how powerful this is, you can reproduce the describeBy() function by using a command like this:\n\nby( data=clin.trial, INDICES=clin.trial$therapy, FUN=describe )\n\nclin.trial$therapy: no.therapy\n          vars n mean   sd median trimmed  mad min max range skew kurtosis   se\ndrug*        1 9 2.00 0.87    2.0    2.00 1.48 1.0 3.0   2.0 0.00    -1.81 0.29\ntherapy*     2 9 1.00 0.00    1.0    1.00 0.00 1.0 1.0   0.0  NaN      NaN 0.00\nmood.gain    3 9 0.72 0.59    0.5    0.72 0.44 0.1 1.7   1.6 0.51    -1.59 0.20\n------------------------------------------------------------ \nclin.trial$therapy: CBT\n          vars n mean   sd median trimmed  mad min max range  skew kurtosis\ndrug*        1 9 2.00 0.87    2.0    2.00 1.48 1.0 3.0   2.0  0.00    -1.81\ntherapy*     2 9 2.00 0.00    2.0    2.00 0.00 2.0 2.0   0.0   NaN      NaN\nmood.gain    3 9 1.04 0.45    1.1    1.04 0.44 0.3 1.8   1.5 -0.03    -1.12\n            se\ndrug*     0.29\ntherapy*  0.00\nmood.gain 0.15\n\n\nThis will produce the exact same output as the command shown earlier. However, there’s nothing special about the describe() function. You could just as easily use the by() function in conjunction with the summary() function. For example:\n\nby( data=clin.trial, INDICES=clin.trial$therapy, FUN=summary )\n\nclin.trial$therapy: no.therapy\n       drug         therapy    mood.gain     \n placebo :3   no.therapy:9   Min.   :0.1000  \n anxifree:3   CBT       :0   1st Qu.:0.3000  \n joyzepam:3                  Median :0.5000  \n                             Mean   :0.7222  \n                             3rd Qu.:1.3000  \n                             Max.   :1.7000  \n------------------------------------------------------------ \nclin.trial$therapy: CBT\n       drug         therapy    mood.gain    \n placebo :3   no.therapy:0   Min.   :0.300  \n anxifree:3   CBT       :9   1st Qu.:0.800  \n joyzepam:3                  Median :1.100  \n                             Mean   :1.044  \n                             3rd Qu.:1.300  \n                             Max.   :1.800  \n\n\nAgain, this output is pretty easy to interpret. It’s the output of the summary() function, applied separately to CBT group and the no.therapy group. For the two factors (drug and therapy) it prints out a frequency table, whereas for the numeric variable (mood.gain) it prints out the range, interquartile range, mean and median.\n% What if you have multiple grouping variables? Suppose, for example, you would like to look at the average mood gain separately for all possible combinations of drug and therapy. It is actually possible to do this using the by() and describeBy() functions, but I usually find it more convenient to use the aggregate() function in this situation. There are again three arguments that you need to specify. The formula argument is used to indicate which variable you want to analyse, and which variables are used to specify the groups. For instance, if you want to look at mood.gain separately for each possible combination of drug and therapy, the formula you want is mood.gain ~ drug + therapy. The data argument is used to specify the data frame containing all the data, and the FUN argument is used to indicate what function you want to calculate for each group (e.g., the mean). So, to obtain group means, use this command:\n% {r} %  aggregate( formula = mood.gain ~ drug + therapy,  # mood.gain by drug/therapy combination %             data = clin.trial,                     # data is in the clin.trial data frame %             FUN = mean                             # print out group means %  ) %\n% or, alternatively, if you want to calculate the standard deviations for each group, you would use the following command (argument names omitted this time):\n% {r} % aggregate( mood.gain ~ drug + therapy, clin.trial, sd ) %"
  },
  {
    "objectID": "materials/descriptives.html#zscore",
    "href": "materials/descriptives.html#zscore",
    "title": "Descriptive statistics",
    "section": "",
    "text": "Suppose my friend is putting together a new questionnaire intended to measure “grumpiness”. The survey has 50 questions, which you can answer in a grumpy way or not. Across a big sample (hypothetically, let’s imagine a million people or so!) the data are fairly normally distributed, with the mean grumpiness score being 17 out of 50 questions answered in a grumpy way, and the standard deviation is 5. In contrast, when I take the questionnaire, I answer 35 out of 50 questions in a grumpy way. So, how grumpy am I? One way to think about would be to say that I have grumpiness of 35/50, so you might say that I’m 70% grumpy. But that’s a bit weird, when you think about it. If my friend had phrased her questions a bit differently, people might have answered them in a different way, so the overall distribution of answers could easily move up or down depending on the precise way in which the questions were asked. So, I’m only 70% grumpy with respect to this set of survey questions. Even if it’s a very good questionnaire, this isn’t very a informative statement.\nA simpler way around this is to describe my grumpiness by comparing me to other people. Shockingly, out of my friend’s sample of 1,000,000 people, only 159 people were as grumpy as me (that’s not at all unrealistic, frankly), suggesting that I’m in the top 0.016% of people for grumpiness. This makes much more sense than trying to interpret the raw data. This idea – that we should describe my grumpiness in terms of the overall distribution of the grumpiness of humans – is the qualitative idea that standardisation attempts to get at. One way to do this is to do exactly what I just did, and describe everything in terms of percentiles. However, the problem with doing this is that “it’s lonely at the top”. Suppose that my friend had only collected a sample of 1000 people (still a pretty big sample for the purposes of testing a new questionnaire, I’d like to add), and this time gotten a mean of 16 out of 50 with a standard deviation of 5, let’s say. The problem is that almost certainly, not a single person in that sample would be as grumpy as me.\nHowever, all is not lost. A different approach is to convert my grumpiness score into a standard score, also referred to as a \\(z\\)-score. The standard score is defined as the number of standard deviations above the mean that my grumpiness score lies. To phrase it in “pseudo-maths” the standard score is calculated like this: \\[\n\\mbox{standard score} = \\frac{\\mbox{raw score} - \\mbox{mean}}{\\mbox{standard deviation}}\n\\] In actual maths, the equation for the \\(z\\)-score is \\[\nz_i = \\frac{X_i - \\bar{X}}{\\hat\\sigma}\n\\] So, going back to the grumpiness data, we can now transform Dan’s raw grumpiness into a standardised grumpiness score.13 If the mean is 17 and the standard deviation is 5 then my standardised grumpiness score would be14 \\[\nz = \\frac{35 - 17}{5} = 3.6\n\\] To interpret this value, recall the rough heuristic that I provided in Section @ref(sd), in which I noted that 99.7% of values are expected to lie within 3 standard deviations of the mean. So the fact that my grumpiness corresponds to a \\(z\\) score of 3.6 indicates that I’m very grumpy indeed. Later on, in Section @ref(normal), I’ll introduce a function called pnorm() that allows us to be a bit more precise than this. Specifically, it allows us to calculate a theoretical percentile rank for my grumpiness, as follows:\n\npnorm( 3.6 )\n\n[1] 0.9998409\n\n\nAt this stage, this command doesn’t make too much sense, but don’t worry too much about it. It’s not important for now. But the output is fairly straightforward: it suggests that I’m grumpier than 99.98% of people. Sounds about right.\nIn addition to allowing you to interpret a raw score in relation to a larger population (and thereby allowing you to make sense of variables that lie on arbitrary scales), standard scores serve a second useful function. Standard scores can be compared to one another in situations where the raw scores can’t. Suppose, for instance, my friend also had another questionnaire that measured extraversion using a 24 items questionnaire. The overall mean for this measure turns out to be 13 with standard deviation 4; and I scored a 2. As you can imagine, it doesn’t make a lot of sense to try to compare my raw score of 2 on the extraversion questionnaire to my raw score of 35 on the grumpiness questionnaire. The raw scores for the two variables are “about” fundamentally different things, so this would be like comparing apples to oranges.\nWhat about the standard scores? Well, this is a little different. If we calculate the standard scores, we get \\(z = (35-17)/5 = 3.6\\) for grumpiness and \\(z = (2-13)/4 = -2.75\\) for extraversion. These two numbers can be compared to each other.15 I’m much less extraverted than most people (\\(z = -2.75\\)) and much grumpier than most people (\\(z = 3.6\\)): but the extent of my unusualness is much more extreme for grumpiness (since 3.6 is a bigger number than 2.75). Because each standardised score is a statement about where an observation falls relative to its own population, it is possible to compare standardised scores across completely different variables."
  },
  {
    "objectID": "materials/descriptives.html#correl",
    "href": "materials/descriptives.html#correl",
    "title": "Descriptive statistics",
    "section": "",
    "text": "Up to this point we have focused entirely on how to construct descriptive statistics for a single variable. What we haven’t done is talked about how to describe the relationships between variables in the data. To do that, we want to talk mostly about the correlation between variables. But first, we need some data.\n\n\nAfter spending so much time looking at the AFL data, I’m starting to get bored with sports. Instead, let’s turn to a topic close to every parent’s heart: sleep. The following data set is fictitious, but based on real events. Suppose I’m curious to find out how much my infant son’s sleeping habits affect my mood. Let’s say that I can rate my grumpiness very precisely, on a scale from 0 (not at all grumpy) to 100 (grumpy as a very, very grumpy old man). And, lets also assume that I’ve been measuring my grumpiness, my sleeping patterns and my son’s sleeping patterns for quite some time now. Let’s say, for 100 days. And, being a nerd, I’ve saved the data as a file called parenthood.Rdata. If we load the data…\n\nload( \"materials/data/parenthood.Rdata\" )\nwho(TRUE)\n\n   -- Name --     -- Class --   -- Size --\n   parenthood     data.frame    100 x 4   \n    $dan.sleep    numeric       100       \n    $baby.sleep   numeric       100       \n    $dan.grump    numeric       100       \n    $day          integer       100       \n\n\n… we see that the file contains a single data frame called parenthood, which contains four variables dan.sleep, baby.sleep, dan.grump and day. If we peek at the data using head() out the data, here’s what we get:\n\nhead(parenthood,10)\n\n   dan.sleep baby.sleep dan.grump day\n1       7.59      10.18        56   1\n2       7.91      11.66        60   2\n3       5.14       7.92        82   3\n4       7.71       9.61        55   4\n5       6.68       9.75        67   5\n6       5.99       5.04        72   6\n7       8.19      10.45        53   7\n8       7.19       8.27        60   8\n9       7.40       6.06        60   9\n10      6.58       7.09        71  10\n\n\nNext, I’ll calculate some basic descriptive statistics:\n\ndescribe( parenthood )\n\n           vars   n  mean    sd median trimmed   mad   min    max range  skew\ndan.sleep     1 100  6.97  1.02   7.03    7.00  1.09  4.84   9.00  4.16 -0.29\nbaby.sleep    2 100  8.05  2.07   7.95    8.05  2.33  3.25  12.07  8.82 -0.02\ndan.grump     3 100 63.71 10.05  62.00   63.16  9.64 41.00  91.00 50.00  0.43\nday           4 100 50.50 29.01  50.50   50.50 37.06  1.00 100.00 99.00  0.00\n           kurtosis   se\ndan.sleep     -0.72 0.10\nbaby.sleep    -0.69 0.21\ndan.grump     -0.16 1.00\nday           -1.24 2.90\n\n\nFinally, to give a graphical depiction of what each of the three interesting variables looks like, Figure @ref(fig:parenthood) plots histograms.\n\n\n\n\n\nHistograms for the three interesting variables in the parenthood data set\n\n\n\n\nOne thing to note: just because R can calculate dozens of different statistics doesn’t mean you should report all of them. If I were writing this up for a report, I’d probably pick out those statistics that are of most interest to me (and to my readership), and then put them into a nice, simple table like the one in Table @ref(tab:parenthoodtab).16 Notice that when I put it into a table, I gave everything “human readable” names. This is always good practice. Notice also that I’m not getting enough sleep. This isn’t good practice, but other parents tell me that it’s standard practice.\n\n\n\nDescriptive statistics for the parenthood data.\n\n\nvariable\nmin\nmax\nmean\nmedian\nstd. dev\nIQR\n\n\n\n\nDan’s grumpiness\n41\n91\n63.71\n62\n10.05\n14\n\n\nDan’s hours slept\n4.84\n9\n6.97\n7.03\n1.02\n1.45\n\n\nDan’s son’s hours slept\n3.25\n12.07\n8.05\n7.95\n2.07\n3.21\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatterplot showing the relationship between dan.sleep and dan.grump\n\n\n\n\n\n\n\n\n\nScatterplot showing the relationship between baby.sleep and dan.grump\n\n\n\n\nWe can draw scatterplots to give us a general sense of how closely related two variables are. Ideally though, we might want to say a bit more about it than that. For instance, let’s compare the relationship between dan.sleep and dan.grump (Figure @ref(fig:scatterparent1a) with that between baby.sleep and dan.grump (Figure @ref(fig:scatterparent1b). When looking at these two plots side by side, it’s clear that the relationship is qualitatively the same in both cases: more sleep equals less grump! However, it’s also pretty obvious that the relationship between dan.sleep and dan.grump is stronger than the relationship between baby.sleep and dan.grump. The plot on the left is “neater” than the one on the right. What it feels like is that if you want to predict what my mood is, it’d help you a little bit to know how many hours my son slept, but it’d be more helpful to know how many hours I slept.\nIn contrast, let’s consider Figure @ref(fig:scatterparent1b) vs. Figure @ref(fig:scatterparent2). If we compare the scatterplot of “baby.sleep v dan.grump” to the scatterplot of “`baby.sleep v dan.sleep”, the overall strength of the relationship is the same, but the direction is different. That is, if my son sleeps more, I get more sleep (positive relationship, but if he sleeps more then I get less grumpy (negative relationship).\n\n\n\n\n\nScatterplot showing the relationship between baby.sleep and dan.sleep\n\n\n\n\n\n\n\nWe can make these ideas a bit more explicit by introducing the idea of a correlation coefficient (or, more specifically, Pearson’s correlation coefficient), which is traditionally denoted by \\(r\\). The correlation coefficient between two variables \\(X\\) and \\(Y\\) (sometimes denoted \\(r_{XY}\\)), which we’ll define more precisely in the next section, is a measure that varies from \\(-1\\) to \\(1\\). When \\(r = -1\\) it means that we have a perfect negative relationship, and when \\(r = 1\\) it means we have a perfect positive relationship. When \\(r = 0\\), there’s no relationship at all. If you look at Figure @ref(fig:corr), you can see several plots showing what different correlations look like.\n\n\n\n\n\nIllustration of the effect of varying the strength and direction of a correlation\n\n\n\n\nThe formula for the Pearson’s correlation coefficient can be written in several different ways. I think the simplest way to write down the formula is to break it into two steps. Firstly, let’s introduce the idea of a covariance. The covariance between two variables \\(X\\) and \\(Y\\) is a generalisation of the notion of the variance; it’s a mathematically simple way of describing the relationship between two variables that isn’t terribly informative to humans: \\[\n\\mbox{Cov}(X,Y) = \\frac{1}{N-1} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right) \\left( Y_i - \\bar{Y} \\right)\n\\] Because we’re multiplying (i.e., taking the “product” of) a quantity that depends on \\(X\\) by a quantity that depends on \\(Y\\) and then averaging17, you can think of the formula for the covariance as an “average cross product” between \\(X\\) and \\(Y\\). The covariance has the nice property that, if \\(X\\) and \\(Y\\) are entirely unrelated, then the covariance is exactly zero. If the relationship between them is positive (in the sense shown in Figure@reffig:corr) then the covariance is also positive; and if the relationship is negative then the covariance is also negative. In other words, the covariance captures the basic qualitative idea of correlation. Unfortunately, the raw magnitude of the covariance isn’t easy to interpret: it depends on the units in which \\(X\\) and \\(Y\\) are expressed, and worse yet, the actual units that the covariance itself is expressed in are really weird. For instance, if \\(X\\) refers to the dan.sleep variable (units: hours) and \\(Y\\) refers to the dan.grump variable (units: grumps), then the units for their covariance are “hours \\(\\times\\) grumps”. And I have no freaking idea what that would even mean.\nThe Pearson correlation coefficient \\(r\\) fixes this interpretation problem by standardising the covariance, in pretty much the exact same way that the \\(z\\)-score standardises a raw score: by dividing by the standard deviation. However, because we have two variables that contribute to the covariance, the standardisation only works if we divide by both standard deviations.18 In other words, the correlation between \\(X\\) and \\(Y\\) can be written as follows: \\[\nr_{XY}  = \\frac{\\mbox{Cov}(X,Y)}{ \\hat{\\sigma}_X \\ \\hat{\\sigma}_Y}\n\\] By doing this standardisation, not only do we keep all of the nice properties of the covariance discussed earlier, but the actual values of \\(r\\) are on a meaningful scale: \\(r= 1\\) implies a perfect positive relationship, and \\(r = -1\\) implies a perfect negative relationship. I’ll expand a little more on this point later, in Section@refsec:interpretingcorrelations. But before I do, let’s look at how to calculate correlations in R.\n\n\n\nCalculating correlations in R can be done using the cor() command. The simplest way to use the command is to specify two input arguments x and y, each one corresponding to one of the variables. The following extract illustrates the basic usage of the function:19\n\ncor( x = parenthood$dan.sleep, y = parenthood$dan.grump )\n\n[1] -0.903384\n\n\nHowever, the cor() function is a bit more powerful than this simple example suggests. For example, you can also calculate a complete “correlation matrix”, between all pairs of variables in the data frame:20\n\n# correlate all pairs of variables in \"parenthood\":\ncor( x = parenthood )  \n\n             dan.sleep  baby.sleep   dan.grump         day\ndan.sleep   1.00000000  0.62794934 -0.90338404 -0.09840768\nbaby.sleep  0.62794934  1.00000000 -0.56596373 -0.01043394\ndan.grump  -0.90338404 -0.56596373  1.00000000  0.07647926\nday        -0.09840768 -0.01043394  0.07647926  1.00000000\n\n\n\n\n\nNaturally, in real life you don’t see many correlations of 1. So how should you interpret a correlation of, say \\(r= .4\\)? The honest answer is that it really depends on what you want to use the data for, and on how strong the correlations in your field tend to be. A friend of mine in engineering once argued that any correlation less than \\(.95\\) is completely useless (I think he was exaggerating, even for engineering). On the other hand there are real cases – even in psychology – where you should really expect correlations that strong. For instance, one of the benchmark data sets used to test theories of how people judge similarities is so clean that any theory that can’t achieve a correlation of at least \\(.9\\) really isn’t deemed to be successful. However, when looking for (say) elementary correlates of intelligence (e.g., inspection time, response time), if you get a correlation above \\(.3\\) you’re doing very very well. In short, the interpretation of a correlation depends a lot on the context. That said, the rough guide in Table @ref(tab:interpretingcorrelations) is pretty typical.\n\n\n\nRough guide to interpreting correlations\n\n\nCorrelation\nStrength\nDirection\n\n\n\n\n-1.0 to -0.9\nVery strong\nNegative\n\n\n-0.9 to -0.7\nStrong\nNegative\n\n\n-0.7 to -0.4\nModerate\nNegative\n\n\n-0.4 to -0.2\nWeak\nNegative\n\n\n-0.2 to 0\nNegligible\nNegative\n\n\n0 to 0.2\nNegligible\nPositive\n\n\n0.2 to 0.4\nWeak\nPositive\n\n\n0.4 to 0.7\nModerate\nPositive\n\n\n0.7 to 0.9\nStrong\nPositive\n\n\n0.9 to 1.0\nVery strong\nPositive\n\n\n\n\n\nHowever, something that can never be stressed enough is that you should always look at the scatterplot before attaching any interpretation to the data. A correlation might not mean what you think it means. The classic illustration of this is “Anscombe’s Quartet” [@Anscombe1973], which is a collection of four data sets. Each data set has two variables, an \\(X\\) and a \\(Y\\). For all four data sets the mean value for \\(X\\) is 9 and the mean for \\(Y\\) is 7.5. The, standard deviations for all \\(X\\) variables are almost identical, as are those for the the \\(Y\\) variables. And in each case the correlation between \\(X\\) and \\(Y\\) is \\(r = 0.816\\). You can verify this yourself, since the dataset comes distributed with R. The commands would be:\n\ncor( anscombe$x1, anscombe$y1 )\n\n[1] 0.8164205\n\ncor( anscombe$x2, anscombe$y2 )\n\n[1] 0.8162365\n\n\nand so on.\nYou’d think that these four data sets would look pretty similar to one another. They do not. If we draw scatterplots of \\(X\\) against \\(Y\\) for all four variables, as shown in Figure @ref(fig:anscombe) we see that all four of these are spectacularly different to each other.\n\n\n\n\n\nAnscombe’s quartet. All four of these data sets have a Pearson correlation of \\(r = .816\\), but they are qualitatively different from one another.\n\n\n\n\nThe lesson here, which so very many people seem to forget in real life is “always graph your raw data”. This will be the focus of Chapter @ref(graphics).\n\n\n\n\n\n\n\n\nThe relationship between hours worked and grade received, for a toy data set consisting of only 10 students (each circle corresponds to one student). The dashed line through the middle shows the linear relationship between the two variables. This produces a strong Pearson correlation of \\(r = .91\\). However, the interesting thing to note here is that there’s actually a perfect monotonic relationship between the two variables: in this toy example at least, increasing the hours worked always increases the grade received, as illustrated by the solid line. This is reflected in a Spearman correlation of \\(rho = 1\\). With such a small data set, however, it’s an open question as to which version better describes the actual relationship involved.\n\n\n\n\nThe Pearson correlation coefficient is useful for a lot of things, but it does have shortcomings. One issue in particular stands out: what it actually measures is the strength of the linear relationship between two variables. In other words, what it gives you is a measure of the extent to which the data all tend to fall on a single, perfectly straight line. Often, this is a pretty good approximation to what we mean when we say “relationship”, and so the Pearson correlation is a good thing to calculation. Sometimes, it isn’t.\nOne very common situation where the Pearson correlation isn’t quite the right thing to use arises when an increase in one variable \\(X\\) really is reflected in an increase in another variable \\(Y\\), but the nature of the relationship isn’t necessarily linear. An example of this might be the relationship between effort and reward when studying for an exam. If you put in zero effort (\\(X\\)) into learning a subject, then you should expect a grade of 0% (\\(Y\\)). However, a little bit of effort will cause a massive improvement: just turning up to lectures means that you learn a fair bit, and if you just turn up to classes, and scribble a few things down so your grade might rise to 35%, all without a lot of effort. However, you just don’t get the same effect at the other end of the scale. As everyone knows, it takes a lot more effort to get a grade of 90% than it takes to get a grade of 55%. What this means is that, if I’ve got data looking at study effort and grades, there’s a pretty good chance that Pearson correlations will be misleading.\nTo illustrate, consider the data plotted in Figure @ref(fig:rankcorrpic), showing the relationship between hours worked and grade received for 10 students taking some class. The curious thing about this – highly fictitious – data set is that increasing your effort always increases your grade. It might be by a lot or it might be by a little, but increasing effort will never decrease your grade. The data are stored in effort.Rdata:\n&gt; load( \"effort.Rdata\" )\n&gt; who(TRUE)\n   -- Name --   -- Class --   -- Size --\n   effort       data.frame    10 x 2    \n    $hours      numeric       10        \n    $grade      numeric       10        \nThe raw data look like this:\n&gt; effort\n   hours grade\n1      2    13\n2     76    91\n3     40    79\n4      6    14\n5     16    21\n6     28    74\n7     27    47\n8     59    85\n9     46    84\n10    68    88\nIf we run a standard Pearson correlation, it shows a strong relationship between hours worked and grade received,\n&gt; cor( effort$hours, effort$grade )\n[1] 0.909402\nbut this doesn’t actually capture the observation that increasing hours worked always increases the grade. There’s a sense here in which we want to be able to say that the correlation is perfect but for a somewhat different notion of what a “relationship” is. What we’re looking for is something that captures the fact that there is a perfect ordinal relationship here. That is, if student 1 works more hours than student 2, then we can guarantee that student 1 will get the better grade. That’s not what a correlation of \\(r = .91\\) says at all.\nHow should we address this? Actually, it’s really easy: if we’re looking for ordinal relationships, all we have to do is treat the data as if it were ordinal scale! So, instead of measuring effort in terms of “hours worked”, lets rank all 10 of our students in order of hours worked. That is, student 1 did the least work out of anyone (2 hours) so they get the lowest rank (rank = 1). Student 4 was the next laziest, putting in only 6 hours of work in over the whole semester, so they get the next lowest rank (rank = 2). Notice that I’m using “rank =1” to mean “low rank”. Sometimes in everyday language we talk about “rank = 1” to mean “top rank” rather than “bottom rank”. So be careful: you can rank “from smallest value to largest value” (i.e., small equals rank 1) or you can rank “from largest value to smallest value” (i.e., large equals rank 1). In this case, I’m ranking from smallest to largest, because that’s the default way that R does it. But in real life, it’s really easy to forget which way you set things up, so you have to put a bit of effort into remembering!\nOkay, so let’s have a look at our students when we rank them from worst to best in terms of effort and reward:\n\n\n\n\nrank (hours worked)\nrank (grade received)\n\n\n\n\nstudent\n1\n1\n\n\nstudent\n2\n10\n\n\nstudent\n3\n6\n\n\nstudent\n4\n2\n\n\nstudent\n5\n3\n\n\nstudent\n6\n5\n\n\nstudent\n7\n4\n\n\nstudent\n8\n8\n\n\nstudent\n9\n7\n\n\nstudent\n10\n9\n\n\n\nHm. These are identical. The student who put in the most effort got the best grade, the student with the least effort got the worst grade, etc. We can get R to construct these rankings using the rank() function, like this:\n&gt; hours.rank &lt;- rank( effort$hours )   # rank students by hours worked\n&gt; grade.rank &lt;- rank( effort$grade )   # rank students by grade received\nAs the table above shows, these two rankings are identical, so if we now correlate them we get a perfect relationship:\n&gt; cor( hours.rank, grade.rank )\n[1] 1\nWhat we’ve just re-invented is Spearman’s rank order correlation, usually denoted \\(\\rho\\) to distinguish it from the Pearson correlation \\(r\\). We can calculate Spearman’s \\(\\rho\\) using R in two different ways. Firstly we could do it the way I just showed, using the rank() function to construct the rankings, and then calculate the Pearson correlation on these ranks. However, that’s way too much effort to do every time. It’s much easier to just specify the method argument of the cor() function.\n&gt; cor( effort$hours, effort$grade, method = \"spearman\")\n[1] 1\nThe default value of the method argument is \"pearson\", which is why we didn’t have to specify it earlier on when we were doing Pearson correlations.\n\n\n\nAs we’ve seen, the cor() function works pretty well, and handles many of the situations that you might be interested in. One thing that many beginners find frustrating, however, is the fact that it’s not built to handle non-numeric variables. From a statistical perspective, this is perfectly sensible: Pearson and Spearman correlations are only designed to work for numeric variables, so the cor() function spits out an error.\nHere’s what I mean. Suppose you were keeping track of how many hours you worked in any given day, and counted how many tasks you completed. If you were doing the tasks for money, you might also want to keep track of how much pay you got for each job. It would also be sensible to keep track of the weekday on which you actually did the work: most of us don’t work as much on Saturdays or Sundays. If you did this for 7 weeks, you might end up with a data set that looks like this one:\n&gt; load(\"work.Rdata\")\n\n&gt; who(TRUE)\n   -- Name --   -- Class --   -- Size --\n   work         data.frame    49 x 7    \n    $hours      numeric       49        \n    $tasks      numeric       49        \n    $pay        numeric       49        \n    $day        integer       49        \n    $weekday    factor        49        \n    $week       numeric       49        \n    $day.type   factor        49   \n    \n&gt; head(work)\n  hours tasks pay day   weekday week day.type\n1   7.2    14  41   1   Tuesday    1  weekday\n2   7.4    11  39   2 Wednesday    1  weekday\n3   6.6    14  13   3  Thursday    1  weekday\n4   6.5    22  47   4    Friday    1  weekday\n5   3.1     5   4   5  Saturday    1  weekend\n6   3.0     7  12   6    Sunday    1  weekend\nObviously, I’d like to know something about how all these variables correlate with one another. I could correlate hours with pay quite using cor(), like so:\n&gt; cor(work$hours,work$pay)\n[1] 0.7604283\nBut what if I wanted a quick and easy way to calculate all pairwise correlations between the numeric variables? I can’t just input the work data frame, because it contains two factor variables, weekday and day.type. If I try this, I get an error:\n&gt; cor(work)\nError in cor(work) : 'x' must be numeric\nIt order to get the correlations that I want using the cor() function, is create a new data frame that doesn’t contain the factor variables, and then feed that new data frame into the cor() function. It’s not actually very hard to do that, and I’ll talk about how to do it properly in Section @ref(subsetdataframe). But it would be nice to have some function that is smart enough to just ignore the factor variables. That’s where the correlate() function in the lsr package can be handy. If you feed it a data frame that contains factors, it knows to ignore them, and returns the pairwise correlations only between the numeric variables:\n&gt; correlate(work)\n\nCORRELATIONS\n============\n- correlation type:  pearson \n- correlations shown only when both variables are numeric\n\n          hours  tasks   pay    day weekday   week day.type\nhours         .  0.800 0.760 -0.049       .  0.018        .\ntasks     0.800      . 0.720 -0.072       . -0.013        .\npay       0.760  0.720     .  0.137       .  0.196        .\nday      -0.049 -0.072 0.137      .       .  0.990        .\nweekday       .      .     .      .       .      .        .\nweek      0.018 -0.013 0.196  0.990       .      .        .\nday.type      .      .     .      .       .      .        .\nThe output here shows a . whenever one of the variables is non-numeric. It also shows a . whenever a variable is correlated with itself (it’s not a meaningful thing to do). The correlate() function can also do Spearman correlations, by specifying the corr.method to use:\n&gt; correlate( work, corr.method=\"spearman\" )\n\nCORRELATIONS\n============\n- correlation type:  spearman \n- correlations shown only when both variables are numeric\n\n          hours  tasks   pay    day weekday   week day.type\nhours         .  0.805 0.745 -0.047       .  0.010        .\ntasks     0.805      . 0.730 -0.068       . -0.008        .\npay       0.745  0.730     .  0.094       .  0.154        .\nday      -0.047 -0.068 0.094      .       .  0.990        .\nweekday       .      .     .      .       .      .        .\nweek      0.010 -0.008 0.154  0.990       .      .        .\nday.type      .      .     .      .       .      .        .\nObviously, there’s no new functionality in the correlate() function, and any advanced R user would be perfectly capable of using the cor() function to get these numbers out. But if you’re not yet comfortable with extracting a subset of a data frame, the correlate() function is for you."
  },
  {
    "objectID": "materials/descriptives.html#missing",
    "href": "materials/descriptives.html#missing",
    "title": "Descriptive statistics",
    "section": "",
    "text": "There’s one last topic that I want to discuss briefly in this chapter, and that’s the issue of missing data. Real data sets very frequently turn out to have missing values: perhaps someone forgot to fill in a particular survey question, for instance. Missing data can be the source of a lot of tricky issues, most of which I’m going to gloss over. However, at a minimum, you need to understand the basics of handling missing data in R.\n\n\nLet’s start with the simplest case, in which you’re trying to calculate descriptive statistics for a single variable which has missing data. In R, this means that there will be NA values in your data vector. Let’s create a variable like that:\n&gt; partial &lt;- c(10, 20, NA, 30)\nLet’s assume that you want to calculate the mean of this variable. By default, R assumes that you want to calculate the mean using all four elements of this vector, which is probably the safest thing for a dumb automaton to do, but it’s rarely what you actually want. Why not? Well, remember that the basic interpretation of NA is “I don’t know what this number is”. This means that 1 + NA = NA: if I add 1 to some number that I don’t know (i.e., the NA) then the answer is also a number that I don’t know. As a consequence, if you don’t explicitly tell R to ignore the NA values, and the data set does have missing values, then the output will itself be a missing value. If I try to calculate the mean of the partial vector, without doing anything about the missing value, here’s what happens:\n&gt; mean( x = partial )\n[1] NA\nTechnically correct, but deeply unhelpful.\nTo fix this, all of the descriptive statistics functions that I’ve discussed in this chapter (with the exception of cor() which is a special case I’ll discuss below) have an optional argument called na.rm, which is shorthand for “remove NA values”. By default, na.rm = FALSE, so R does nothing about the missing data problem. Let’s try setting na.rm = TRUE and see what happens:\nWhen calculating sums and means when missing data are present (i.e., when there are NA values) there’s actually an additional argument to the function that you should be aware of. This argument is called na.rm, and is a logical value indicating whether R should ignore (or “remove”) the missing data for the purposes of doing the calculations. By default, R assumes that you want to keep the missing values, so unless you say otherwise it will set na.rm = FALSE. However, R assumes that 1 + NA = NA: if I add 1 to some number that I don’t know (i.e., the NA) then the answer is also a number that I don’t know. As a consequence, if you don’t explicitly tell R to ignore the NA values, and the data set does have missing values, then the output will itself be a missing value. This is illustrated in the following extract:\n&gt; mean( x = partial, na.rm = TRUE )\n[1] 20\nNotice that the mean is 20 (i.e., 60 / 3) and not 15. When R ignores a NA value, it genuinely ignores it. In effect, the calculation above is identical to what you’d get if you asked for the mean of the three-element vector c(10, 20, 30).\nAs indicated above, this isn’t unique to the mean() function. Pretty much all of the other functions that I’ve talked about in this chapter have an na.rm argument that indicates whether it should ignore missing values. However, its behaviour is the same for all these functions, so I won’t waste everyone’s time by demonstrating it separately for each one.\n\n\n\nI mentioned earlier that the cor() function is a special case. It doesn’t have an na.rm argument, because the story becomes a lot more complicated when more than one variable is involved. What it does have is an argument called use which does roughly the same thing, but you need to think little more carefully about what you want this time. To illustrate the issues, let’s open up a data set that has missing values, parenthood2.Rdata. This file contains the same data as the original parenthood data, but with some values deleted. It contains a single data frame, parenthood2:\n&gt; load( \"parenthood2.Rdata\" )\n&gt; print( parenthood2 )\n  dan.sleep baby.sleep dan.grump day\n1      7.59         NA        56   1\n2      7.91      11.66        60   2\n3      5.14       7.92        82   3\n4      7.71       9.61        55   4\n5      6.68       9.75        NA   5\n6      5.99       5.04        72   6\nBLAH BLAH BLAH\nIf I calculate my descriptive statistics using the describe() function\n&gt; describe( parenthood2 )\n           var   n  mean    sd median trimmed   mad   min    max    BLAH\ndan.sleep    1  91  6.98  1.02   7.03    7.02  1.13  4.84   9.00    BLAH\nbaby.sleep   2  89  8.11  2.05   8.20    8.13  2.28  3.25  12.07    BLAH\ndan.grump    3  92 63.15  9.85  61.00   62.66 10.38 41.00  89.00    BLAH\nday          4 100 50.50 29.01  50.50   50.50 37.06  1.00 100.00    BLAH\nwe can see from the n column that there are 9 missing values for dan.sleep, 11 missing values for baby.sleep and 8 missing values for dan.grump.21 Suppose what I would like is a correlation matrix. And let’s also suppose that I don’t bother to tell R how to handle those missing values. Here’s what happens:\n&gt; cor( parenthood2 )\n           dan.sleep baby.sleep dan.grump day\ndan.sleep          1         NA        NA  NA\nbaby.sleep        NA          1        NA  NA\ndan.grump         NA         NA         1  NA\nday               NA         NA        NA   1\nAnnoying, but it kind of makes sense. If I don’t know what some of the values of dan.sleep and baby.sleep actually are, then I can’t possibly know what the correlation between these two variables is either, since the formula for the correlation coefficient makes use of every single observation in the data set. Once again, it makes sense: it’s just not particularly helpful.\nTo make R behave more sensibly in this situation, you need to specify the use argument to the cor() function. There are several different values that you can specify for this, but the two that we care most about in practice tend to be \"complete.obs\" and \"pairwise.complete.obs\". If we specify use = \"complete.obs\", R will completely ignore all cases (i.e., all rows in our parenthood2 data frame) that have any missing values at all. So, for instance, if you look back at the extract earlier when I used the head() function, notice that observation 1 (i.e., day 1) of the parenthood2 data set is missing the value for baby.sleep, but is otherwise complete? Well, if you choose use = \"complete.obs\" R will ignore that row completely: that is, even when it’s trying to calculate the correlation between dan.sleep and dan.grump, observation 1 will be ignored, because the value of baby.sleep is missing for that observation. Here’s what we get:\n&gt; cor(parenthood2, use = \"complete.obs\")\n             dan.sleep baby.sleep   dan.grump         day\ndan.sleep   1.00000000  0.6394985 -0.89951468  0.06132891\nbaby.sleep  0.63949845  1.0000000 -0.58656066  0.14555814\ndan.grump  -0.89951468 -0.5865607  1.00000000 -0.06816586\nday         0.06132891  0.1455581 -0.06816586  1.00000000\nThe other possibility that we care about, and the one that tends to get used more often in practice, is to set use = \"pairwise.complete.obs\". When we do that, R only looks at the variables that it’s trying to correlate when determining what to drop. So, for instance, since the only missing value for observation 1 of parenthood2 is for baby.sleep R will only drop observation 1 when baby.sleep is one of the variables involved: and so R keeps observation 1 when trying to correlate dan.sleep and dan.grump. When we do it this way, here’s what we get:\n&gt; cor(parenthood2, use = \"pairwise.complete.obs\") \n             dan.sleep  baby.sleep    dan.grump          day\ndan.sleep   1.00000000  0.61472303 -0.903442442 -0.076796665\nbaby.sleep  0.61472303  1.00000000 -0.567802669  0.058309485\ndan.grump  -0.90344244 -0.56780267  1.000000000  0.005833399\nday        -0.07679667  0.05830949  0.005833399  1.000000000\nSimilar, but not quite the same. It’s also worth noting that the correlate() function (in the lsr package) automatically uses the “pairwise complete” method:\n&gt; correlate(parenthood2)\n\nCORRELATIONS\n============\n- correlation type:  pearson \n- correlations shown only when both variables are numeric\n\n           dan.sleep baby.sleep dan.grump    day\ndan.sleep          .      0.615    -0.903 -0.077\nbaby.sleep     0.615          .    -0.568  0.058\ndan.grump     -0.903     -0.568         .  0.006\nday           -0.077      0.058     0.006      .\nThe two approaches have different strengths and weaknesses. The “pairwise complete” approach has the advantage that it keeps more observations, so you’re making use of more of your data and (as we’ll discuss in tedious detail in Chapter @ref(estimation) and it improves the reliability of your estimated correlation. On the other hand, it means that every correlation in your correlation matrix is being computed from a slightly different set of observations, which can be awkward when you want to compare the different correlations that you’ve got.\nSo which method should you use? It depends a lot on why you think your values are missing, and probably depends a little on how paranoid you are. For instance, if you think that the missing values were “chosen” completely randomly22 then you’ll probably want to use the pairwise method. If you think that missing data are a cue to thinking that the whole observation might be rubbish (e.g., someone just selecting arbitrary responses in your questionnaire), but that there’s no pattern to which observations are “rubbish” then it’s probably safer to keep only those observations that are complete. If you think there’s something systematic going on, in that some observations are more likely to be missing than others, then you have a much trickier problem to solve, and one that is beyond the scope of this book."
  },
  {
    "objectID": "materials/descriptives.html#summary-1",
    "href": "materials/descriptives.html#summary-1",
    "title": "Descriptive statistics",
    "section": "",
    "text": "Calculating some basic descriptive statistics is one of the very first things you do when analysing real data, and descriptive statistics are much simpler to understand than inferential statistics, so like every other statistics textbook I’ve started with descriptives. In this chapter, we talked about the following topics:\n\nMeasures of central tendency. Broadly speaking, central tendency measures tell you where the data are. There’s three measures that are typically reported in the literature: the mean, median and mode. (Section @ref(centraltendency))\nMeasures of variability. In contrast, measures of variability tell you about how “spread out” the data are. The key measures are: range, standard deviation, interquartile reange (Section @ref(var))\nGetting summaries of variables in R. Since this book focuses on doing data analysis in R, we spent a bit of time talking about how descriptive statistics are computed in R. (Section @ref(summary) and @ref(groupdescriptives))\nStandard scores. The \\(z\\)-score is a slightly unusual beast. It’s not quite a descriptive statistic, and not quite an inference. We talked about it in Section @ref(zscore). Make sure you understand that section: it’ll come up again later.\nCorrelations. Want to know how strong the relationship is between two variables? Calculate a correlation. (Section @ref(correl))\nMissing data. Dealing with missing data is one of those frustrating things that data analysts really wish the didn’t have to think about. In real life it can be hard to do well. For the purpose of this book, we only touched on the basics in Section @ref(missing)\n\nIn the next section we’ll move on to a discussion of how to draw pictures! Everyone loves a pretty picture, right? But before we do, I want to end on an important point. A traditional first course in statistics spends only a small proportion of the class on descriptive statistics, maybe one or two lectures at most. The vast majority of the lecturer’s time is spent on inferential statistics, because that’s where all the hard stuff is. That makes sense, but it hides the practical everyday importance of choosing good descriptives. With that in mind…"
  },
  {
    "objectID": "materials/descriptives.html#epilogue-good-descriptive-statistics-are-descriptive",
    "href": "materials/descriptives.html#epilogue-good-descriptive-statistics-are-descriptive",
    "title": "Descriptive statistics",
    "section": "",
    "text": "The death of one man is a tragedy. The death of millions is a statistic.\n– Josef Stalin, Potsdam 1945\n\n\n950,000 – 1,200,000\n– Estimate of Soviet repression deaths, 1937-1938 [@Ellman2002]\n\nStalin’s infamous quote about the statistical character death of millions is worth giving some thought. The clear intent of his statement is that the death of an individual touches us personally and its force cannot be denied, but that the deaths of a multitude are incomprehensible, and as a consequence mere statistics, more easily ignored. I’d argue that Stalin was half right. A statistic is an abstraction, a description of events beyond our personal experience, and so hard to visualise. Few if any of us can imagine what the deaths of millions is “really” like, but we can imagine one death, and this gives the lone death its feeling of immediate tragedy, a feeling that is missing from Ellman’s cold statistical description.\nYet it is not so simple: without numbers, without counts, without a description of what happened, we have no chance of understanding what really happened, no opportunity event to try to summon the missing feeling. And in truth, as I write this, sitting in comfort on a Saturday morning, half a world and a whole lifetime away from the Gulags, when I put the Ellman estimate next to the Stalin quote a dull dread settles in my stomach and a chill settles over me. The Stalinist repression is something truly beyond my experience, but with a combination of statistical data and those recorded personal histories that have come down to us, it is not entirely beyond my comprehension. Because what Ellman’s numbers tell us is this: over a two year period, Stalinist repression wiped out the equivalent of every man, woman and child currently alive in the city where I live. Each one of those deaths had it’s own story, was it’s own tragedy, and only some of those are known to us now. Even so, with a few carefully chosen statistics, the scale of the atrocity starts to come into focus.\nThus it is no small thing to say that the first task of the statistician and the scientist is to summarise the data, to find some collection of numbers that can convey to an audience a sense of what has happened. This is the job of descriptive statistics, but it’s not a job that can be told solely using the numbers. You are a data analyst, not a statistical software package. Part of your job is to take these statistics and turn them into a description. When you analyse data, it is not sufficient to list off a collection of numbers. Always remember that what you’re really trying to do is communicate with a human audience. The numbers are important, but they need to be put together into a meaningful story that your audience can interpret. That means you need to think about framing. You need to think about context. And you need to think about the individual events that your statistics are summarising."
  },
  {
    "objectID": "materials/descriptives.html#footnotes",
    "href": "materials/descriptives.html#footnotes",
    "title": "Descriptive statistics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote for non-Australians: the AFL is an Australian rules football competition. You don’t need to know anything about Australian rules in order to follow this section.↩︎\nThe choice to use \\(\\Sigma\\) to denote summation isn’t arbitrary: it’s the Greek upper case letter sigma, which is the analogue of the letter S in that alphabet. Similarly, there’s an equivalent symbol used to denote the multiplication of lots of numbers: because multiplications are also called “products”, we use the \\(\\Pi\\) symbol for this; the Greek upper case pi, which is the analogue of the letter P.↩︎\nNote that, just as we saw with the combine function c() and the remove function rm(), the sum() function has unnamed arguments. I’ll talk about unnamed arguments later in Section @ref(dotsargument), but for now let’s just ignore this detail.↩︎\nwww.abc.net.au/news/stories/2010/09/24/3021480.htm↩︎\nOr at least, the basic statistical theory – these days there is a whole subfield of statistics called robust statistics that tries to grapple with the messiness of real data and develop theory that can cope with it.↩︎\nAs we saw earlier, it does have a function called mode(), but it does something completely different.↩︎\nThis is called a “0-1 loss function”, meaning that you either win (1) or you lose (0), with no middle ground.↩︎\nWell, I will very briefly mention the one that I think is coolest, for a very particular definition of “cool”, that is. Variances are additive. Here’s what that means: suppose I have two variables \\(X\\) and \\(Y\\), whose variances are \\(\\mbox{Var}](X)\\) and \\(\\mbox{Var}(Y)\\) respectively. Now imagine I want to define a new variable \\(Z\\) that is the sum of the two, \\(Z = X+Y\\). As it turns out, the variance of \\(Z\\) is equal to \\(\\mbox{Var}(X) + \\mbox{Var}(Y)\\). This is a very useful property, but it’s not true of the other measures that I talk about in this section.↩︎\nWith the possible exception of the third question.↩︎\nStrictly, the assumption is that the data are normally distributed, which is an important concept that we’ll discuss more in Chapter @ref(probability), and will turn up over and over again later in the book.↩︎\nThe assumption again being that the data are normally-distributed!↩︎\nThe “\\(-3\\)” part is something that statisticians tack on to ensure that the normal curve has kurtosis zero. It looks a bit stupid, just sticking a “-3” at the end of the formula, but there are good mathematical reasons for doing this.↩︎\nI haven’t discussed how to compute \\(z\\)-scores, explicitly, but you can probably guess. For a variable X, the simplest way is to use a command like (X - mean(X)) / sd(X). There’s also a fancier function called scale() that you can use, but it relies on somewhat more complicated R concepts that I haven’t explained yet.↩︎\nTechnically, because I’m calculating means and standard deviations from a sample of data, but want to talk about my grumpiness relative to a population, what I’m actually doing is estimating a \\(z\\) score. However, since we haven’t talked about estimation yet (see Chapter @ref(estimation)) I think it’s best to ignore this subtlety, especially as it makes very little difference to our calculations.↩︎\nThough some caution is usually warranted. It’s not always the case that one standard deviation on variable A corresponds to the same “kind” of thing as one standard deviation on variable B. Use common sense when trying to determine whether or not the \\(z\\) scores of two variables can be meaningfully compared.↩︎\nActually, even that table is more than I’d bother with. In practice most people pick one measure of central tendency, and one measure of variability only.↩︎\nJust like we saw with the variance and the standard deviation, in practice we divide by \\(N-1\\) rather than \\(N\\).↩︎\nThis is an oversimplification, but it’ll do for our purposes.↩︎\nIf you are reading this after having already completed Chapter @ref(hypothesistesting) you might be wondering about hypothesis tests for correlations. R has a function called cor.test() that runs a hypothesis test for a single correlation, and the psych package contains a version called corr.test() that can run tests for every correlation in a correlation matrix; hypothesis tests for correlations are discussed in more detail in Section @ref(corrhyp).↩︎\nAn alternative usage of cor() is to correlate one set of variables with another subset of variables. If X and Y are both data frames with the same number of rows, then cor(x = X, y = Y) will produce a correlation matrix that correlates all variables in X with all variables in Y.↩︎\nIt’s worth noting that, even though we have missing data for each of these variables, the output doesn’t contain any NA values. This is because, while describe() also has an na.rm argument, the default value for this function is na.rm = TRUE.↩︎\nThe technical term here is “missing completely at random” (often written MCAR for short). Makes sense, I suppose, but it does sound ungrammatical to me.↩︎"
  }
]