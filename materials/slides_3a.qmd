---
title: "Intro to Data Visualisation II"
author: "Dr. Gordon Wright"
engine: knitr

format: 
  revealjs: 
    slide-number: c/t
    controls-layout: bottom-right
    embed-resources: true
    scrollable: true
    code-overflow: wrap
    code-line-numbers: true
    code-copy: true
    theme: [simple, slides.scss]
    reference-location: document
    footer: "Data Visualisation II"
  html:
    code-copy: true
webr:
  packages: ['ggplot2', 'dplyr', 'tidyr'] # Install R packages on document open
# autoload-packages: false       # Disable automatic loading of packages
show-startup-message: false    # Disable displaying status of webR initialization
filters:
- webr
---

## Refresher/Primer on Data Types

In statistics, we work with various types of data. Understanding these
types is crucial for:

-   Choosing appropriate statistical methods
-   Interpreting results correctly
-   Making informed decisions based on data

Let's explore the four main levels of measurement...

::: notes
Emphasize the importance of understanding data types for statistical
analysis.
:::

## Nominal Data

-   **Definition**: Categories or groups without intrinsic order
-   **Characteristics**:
    -   Cannot be ordered
    -   No numerical value
    -   Only shows distinct groups
-   **Examples from our dataset**:
    -   MBTI (Myers-Briggs Type Indicator)
    -   Coin (Heads or Tails)
    -   DogCatBoth (Preference for pets)

::: notes
Explain that nominal data is qualitative and used for labeling variables
without any quantitative value.
:::

## Ordinal Data

-   **Definition**: Categories with a meaningful order, but differences
    aren't measurable
-   **Characteristics**:
    -   Can be ordered
    -   Intervals between ranks aren't necessarily equal
-   **Examples from our dataset**:
    -   1-7 rating scales (Likert scale responses)
    -   TIPI scores (Big Five - OCEAN)

::: notes
Highlight that ordinal data has an order, but the distance between
categories is not uniform or meaningful.
:::

## Interval Data

-   **Definition**: Numerical data with consistent intervals, but no
    true zero point
-   **Characteristics**:
    -   Ordered with equal intervals between values
    -   Can be added or subtracted
    -   No true zero point
-   **Examples**:
    -   Temperature in Celsius or Fahrenheit
    -   Calendar years

::: notes
Explain that interval data allows for degree of difference, but not
ratio comparisons.
:::

## Ratio Data

-   **Definition**: Numerical data with equal intervals and a true zero
    point
-   **Characteristics**:
    -   Ordered with equal intervals
    -   Has a true zero point (absence of the variable is possible)
    -   Can be added, subtracted, multiplied, and divided
-   **Examples from our dataset**:
    -   LoginCount
    -   CompTime (assuming it's recorded in minutes)
    -   EyeContact (assuming it's a continuous measure)

::: notes
Emphasize that ratio data allows for all arithmetic operations and
comparisons.
:::

## Summary of Data Types

| Data Type | Can Be Ordered? | Equal Intervals? | True Zero Point? | Example |
|----|----|----|----|----|
| Nominal | No | No | No | Gender, MBTI |
| Ordinal | Yes | No | No | Scale measures, TIPI scores |
| Interval | Yes | Yes | No | Temperature (°C) |
| Ratio | Yes | Yes | Yes | LoginCount, CompTime |

## Practical Application

Let's look at some variables from our new, exciting dataset:

| Column Name | Description | Data Type |
|----|----|----|
| PokeNumber | Unique identifier for each Pokémon | Categorical |
| PokeName | Name of the Pokémon | Text |
| PokeImage | URL to the Pokémon's image | Text |
| LoginCount | Number of VLE logins (to date) | Ratio |
| Q_Scale | The Scale-Based Question you submitted for DS01 | Text |
| Q_choice | The Choice-Based Question you submitted for DS01 | Text |
| Q_option | The Open Question you submitted for DS01 | Text |
| CompTime | DS02 Survey Completion Time | Ratio |
| A-Level | Whether the participant has A-Levels | Categorical |
| Coin | Result of a coin flip | Categorical |
| EyeContact | Liking for Eye-Contact | Ordinal |
| Psy4me | Is Psychology for me Question | Ordinal |
| DrWho | Rating of Doctor Who | Ordinal |
| Maths | Self-Assessed Maths competence | Ordinal |
| InsectApocalypse | Preferred insect in case of an insect apocalypse | Categorical |
| DogCatBoth | Preference for dogs, cats, or both | Categorical |
| LarkorOwl | Preferred sleep/wake pattern | Categorical |
| E_TIPI | Extraversion score from TIPI | Ordinal |
| A_TIPI | Agreeableness score from TIPI | Ordinal |
| C_TIPI | Conscientiousness score from TIPI | Ordinal |
| ES_TIPI | Emotional Stability score from TIPI | Ordinal |
| O_TIPI | Openness score from TIPI | Ordinal |
| MBTI | Myers-Briggs Type Indicator result | Categorical |
| Row | Lecture Hall Row Number F-B | Ordinal |
| Column | Lecture Hall Column L-R | Ordinal |

## Conclusion and Next Steps

-   Understanding data types is crucial for proper statistical analysis
-   In our dataset, we have a mix of nominal, ordinal, and ratio data
-   Next, we'll explore how to summarize and visualize these different
    types of data

## Introduction to Measures of Central Tendency

Measures of central tendency help us understand the typical or central
value in a dataset. The three main measures are:

1.  Mean
2.  Median
3.  Mode

Let's explore each of these using our dataset...

::: notes
Explain that these measures help summarize data and provide a "typical"
value.
:::

## Mean

-   **Definition**: The average of all values in a dataset
-   **Formula**: $\bar{x} = \frac{\sum{X}}{N}$
-   **Best for**: Interval and ratio data
-   **Example**: Let's calculate the mean LoginCount

## Let's see all the values available, first

This is called a Stem and Leaf Plot and we will calculate the mean below
it.

```{r}
# Read the data
data <- read.csv("materials/data/Y1W3_data.csv")

# Create and print stem and leaf plot without showing NULL
invisible(stem(data$LoginCount, scale = 1))

# Calculate and print the mean
mean_login <- mean(data$LoginCount, na.rm = TRUE)
cat("\nMean LoginCount:", round(mean_login, 2))
```

## Median

-   **Definition**: The middle value when data is ordered
-   **Best for**: Ordinal, interval, and ratio data
-   **Example**: Let's find the median LoginCount

```{r}
data <- read.csv("materials/data/Y1W3_data.csv")
median_login <- median(data$LoginCount, na.rm = TRUE)
cat("Median LoginCount:", median_login)
```

::: notes
Highlight that the median is less affected by outliers than the mean.
:::

## Mode

-   **Definition**: The most frequently occurring value
-   **Best for**: Any type of data, especially nominal
-   **Example**: Let's find the mode of DogCatBoth

```{r}
# Read the data
data <- read.csv("materials/data/Y1W3_data.csv")

# Get frequency table
pet_counts <- table(data$DogCatBoth)

# Sort the table in descending order
pet_counts_sorted <- sort(pet_counts, decreasing = TRUE)

# Print the counts
cat("Counts for DogCatBoth:\n")
print(pet_counts_sorted)

# Calculate and print the mode
mode_pets <- names(pet_counts_sorted)[1]
cat("\nMode of DogCatBoth:", mode_pets)

# Calculate the total number of responses
total_responses <- sum(pet_counts)

# Calculate and print the percentages
cat("\n\nPercentages for DogCatBoth:\n")
percentages <- (pet_counts_sorted / total_responses) * 100
for (pet in names(percentages)) {
  cat(sprintf("%s: %.2f%%\n", pet, percentages[pet]))
}
```

::: notes
Explain that mode is the only measure of central tendency for nominal
data.
:::

## Comparing Measures of Central Tendency

Let's compare these measures for LoginCount:

```{r}
cat("Mean LoginCount:", round(mean_login, 2), "\n")
cat("Median LoginCount:", median_login, "\n")
cat("Mode LoginCount:", as.numeric(names(sort(table(data$LoginCount), decreasing = TRUE)[1])))
```

::: notes
Discuss what these differences might tell us about the distribution of
LoginCount.
:::

## Introduction to Measures of Variance

Measures of variance help us understand the spread or dispersion of
data. Key measures include:

1.  Range
2.  Interquartile Range (IQR)
3.  Variance
4.  Standard Deviation

::: notes
Explain that these measures complement central tendency by showing how
spread out the data is.
:::

## Range and Interquartile Range (IQR)

-   **Range**: Difference between the maximum and minimum values
-   **IQR**: Range of the middle 50% of the data

Let's calculate these for LoginCount:

```{r}
login_range <- range(data$LoginCount, na.rm = TRUE)
login_iqr <- IQR(data$LoginCount, na.rm = TRUE)

cat("Range of LoginCount:", login_range[2] - login_range[1], "\n")
cat("IQR of LoginCount:", login_iqr)
```

::: notes
Explain that range is sensitive to outliers, while IQR is more robust.
:::

## Variance and Standard Deviation

-   **Variance**: Average squared deviation from the mean
-   **Standard Deviation**: Square root of the variance

Let's calculate these for LoginCount:

```{r}
login_var <- var(data$LoginCount, na.rm = TRUE)
login_sd <- sd(data$LoginCount, na.rm = TRUE)

cat("Variance of LoginCount:", round(login_var, 2), "\n")
cat("Standard Deviation of LoginCount:", round(login_sd, 2))
```

::: notes
Explain that standard deviation is in the same units as the original
data, making it easier to interpret.
:::

## Interpreting Standard Deviation

-   In a normal distribution:
    -   About 68% of data falls within 1 SD of the mean
    -   About 95% falls within 2 SD
    -   About 99.7% falls within 3 SD

Let's visualize this for LoginCount:

```{r}
library(ggplot2)
library(dplyr)

# Calculate mean and standard deviation
mean_login <- mean(data$LoginCount, na.rm = TRUE)
login_sd <- sd(data$LoginCount, na.rm = TRUE)

# Create the plot
ggplot(data, aes(x = LoginCount)) +
  geom_histogram(aes(y = ..density..), binwidth = 5, fill = "skyblue", color = "black", alpha = 0.7) +
  geom_density(color = "darkblue", size = 1) +
  geom_vline(xintercept = mean_login, color = "red", linetype = "dashed", size = 1) +
  geom_vline(xintercept = mean_login + c(-1, 1) * login_sd, color = "darkgreen", linetype = "dashed", size = 1) +
  geom_vline(xintercept = mean_login + c(-2, 2) * login_sd, color = "blue", linetype = "dashed", size = 1) +
  annotate("text", x = mean_login, y = 0.04, label = "Mean", color = "red", angle = 90, vjust = -0.5) +
  annotate("text", x = mean_login + login_sd, y = 0.04, label = "68%", color = "darkgreen", angle = 90, vjust = -0.5) +
  annotate("text", x = mean_login + 2*login_sd, y = 0.04, label = "95%", color = "blue", angle = 90, vjust = -0.5) +
  labs(title = "Distribution of LoginCount with Standard Deviations",
       subtitle = paste("Mean =", round(mean_login, 2), "SD =", round(login_sd, 2)),
       x = "LoginCount", 
       y = "Density") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))

# Calculate percentages within each SD range
within_1sd <- mean(data$LoginCount >= (mean_login - login_sd) & data$LoginCount <= (mean_login + login_sd), na.rm = TRUE) * 100
within_2sd <- mean(data$LoginCount >= (mean_login - 2*login_sd) & data$LoginCount <= (mean_login + 2*login_sd), na.rm = TRUE) * 100
within_3sd <- mean(data$LoginCount >= (mean_login - 3*login_sd) & data$LoginCount <= (mean_login + 3*login_sd), na.rm = TRUE) * 100

cat("Percentage within 1 SD:", round(within_1sd, 2), "%\n")
cat("Percentage within 2 SD:", round(within_2sd, 2), "%\n")
cat("Percentage within 3 SD:", round(within_3sd, 2), "%\n")
```

::: notes
Explain how to interpret this visualization and what it tells us about
the spread of LoginCount.
:::

## Application to Different Data Types

Let's apply these concepts to different types of data in our dataset:

```{r}
# Read the original data
data <- read.csv("materials/data/Y1W3_data.csv")

# Create dummy data for Q_Scale
set.seed(123)  # for reproducibility
n <- nrow(data)
dummy_q_scale <- sample(1:5, n, replace = TRUE, prob = c(0.1, 0.2, 0.4, 0.2, 0.1))

# Add dummy Q_Scale to the data
data$Q_Scale <- dummy_q_scale

# Analysis
# For ratio data (LoginCount)
cat("LoginCount (Ratio):\n")
cat("Mean:", round(mean(data$LoginCount, na.rm = TRUE), 2), "\n")
cat("SD:", round(sd(data$LoginCount, na.rm = TRUE), 2), "\n\n")

# For ordinal data (Q_Scale)
cat("Q_Scale (Ordinal):\n")
cat("Mean:", round(mean(data$Q_Scale, na.rm = TRUE), 2), "\n")
cat("Median:", median(data$Q_Scale, na.rm = TRUE), "\n")
cat("IQR:", IQR(data$Q_Scale, na.rm = TRUE), "\n")
cat("Frequency Table:\n")
print(table(data$Q_Scale))
cat("\n")

# For nominal data (DogCatBoth)
cat("DogCatBoth (Nominal):\n")
print(table(data$DogCatBoth))

```

::: notes
Discuss how the choice of measure depends on the type of data, and how
to interpret these results.
:::

## Mi-Fin

-   Measures of central tendency and variance provide crucial summaries
    of our data
-   The choice of measure depends on the type of data and the shape of
    its distribution
-   These measures form the basis for more advanced statistical analyses
-   In the second half, we'll explore how to visualize these concepts
    and dive deeper into our dataset and preview the labs!

::: notes
Summarize the key points about central tendency and variance, and
preview the next part on data visualization.
:::

## Data Exploration

Data exploration involves:

-   Visualizing data distributions
-   Identifying patterns and relationships
-   Detecting outliers and anomalies

We'll use various chart types to explore our dataset.

::: notes
Explain the importance of data exploration in understanding the dataset
before applying more advanced statistical techniques.
:::

## Histogram: Login Count Distribution

```{r}
#| warning: false


library(ggplot2)
library(plotly)
library(dplyr)

# Read the data
data <- read.csv("materials/data/Y1W3_data.csv")

# Create the ggplot object
p <- ggplot(data, aes(x = LoginCount)) +
  geom_histogram(binwidth = 5, fill = "skyblue", color = "black", aes(text = paste("Login Count:", ..x.., "<br>Frequency:", ..count..))) +
  labs(title = "Distribution of Login Counts - What do you think?",
       x = "Login Count",
       y = "Frequency") +
  theme_minimal()

# Convert to plotly
interactive_plot <- ggplotly(p, tooltip = "text")

# Configure the layout for better interactivity
interactive_plot <- interactive_plot %>% 
  layout(hoverlabel = list(bgcolor = "white"),
         dragmode = "zoom")

# If you're knitting to an HTML document or using in RStudio, you can display the plot with:
# interactive_plot

# For use in a reveal.js slideshow, you'll need to save this as an HTML file:
htmlwidgets::saveWidget(interactive_plot, "login_count_distribution.html")

# Print a message about where the file is saved
cat("Interactive plot saved as 'login_count_distribution.html' in your working directory.")
```

::: notes
Discuss the shape of the distribution, any skewness, and what it tells
us about student engagement.
:::

## Bar Chart: Pet Preferences

```{r}
ggplot(data, aes(x = DogCatBoth)) +
  geom_bar(fill = "lightgreen") +
  labs(title = "Preference for Dogs, Cats, or Both",
       x = "Preference",
       y = "Count") +
  theme_minimal()
```

::: notes
Interpret the bar chart, discussing the most common preference and
potential implications for student engagement strategies.
:::

## Scatter Plot: Eye Contact vs Login Count

```{r}
ggplot(data, aes(x = LoginCount, y = EyeContact)) +
  geom_point(alpha = 0.7) +
  labs(title = "Eye Contact Score vs Login Count",
       x = "Login Count",
       y = "Eye Contact Score") +
  theme_minimal()
```

::: notes
Discuss any visible patterns or correlations, and what they might imply
about the relationship between online engagement and social comfort.
:::

## Box Plot: Completion Time by Conscientiousness

```{r}
library(ggplot2)
library(dplyr)

# Read the data
data <- read.csv("materials/data/Y1W3_data.csv")

# Function to convert time string to minutes
time_to_minutes <- function(time_str) {
  if (is.na(time_str) || time_str == "") return(NA)
  parts <- strsplit(time_str, ":")[[1]]
  return(as.numeric(parts[1]) + as.numeric(parts[2]) / 60)
}

# Convert CompTime to minutes
data$CompTimeMinutes <- sapply(data$CompTime, time_to_minutes)

# Remove NA values
data <- data %>% filter(!is.na(C_TIPI) & !is.na(CompTimeMinutes))

# Perform median split on C_TIPI
median_c <- median(data$C_TIPI, na.rm = TRUE)
data$C_Group <- factor(ifelse(data$C_TIPI > median_c, "High", "Low"), levels = c("Low", "High"))

# Verify the levels of C_Group
print(table(data$C_Group))

# Create the plot
p <- ggplot(data, aes(x = C_Group, y = CompTimeMinutes)) +
  geom_boxplot(fill = "skyblue", color = "darkblue") +
  labs(title = "Completion Time by Conscientiousness Level",
       x = "Conscientiousness Level",
       y = "Completion Time (hours)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

# Display the plot
print(p)

# Calculate and display summary statistics
summary_stats <- data %>%
  group_by(C_Group) %>%
  summarise(
    N = n(),
    Mean = mean(CompTimeMinutes, na.rm = TRUE),
    Median = median(CompTimeMinutes, na.rm = TRUE),
    SD = sd(CompTimeMinutes, na.rm = TRUE)
  )



# # Additional diagnostic information
# cat("\nRange of C_TIPI scores:", range(data$C_TIPI, na.rm = TRUE), "\n")
# cat("Median C_TIPI score:", median_c, "\n")
# cat("Range of CompTimeMinutes:", range(data$CompTimeMinutes, na.rm = TRUE), "\n")
```

## Outliers removed

```{r}
library(ggplot2)
library(dplyr)

# Read the data
data <- read.csv("materials/data/Y1W3_data.csv")

# Function to convert time string to minutes
time_to_minutes <- function(time_str) {
  if (is.na(time_str) || time_str == "") return(NA)
  parts <- strsplit(time_str, ":")[[1]]
  return(as.numeric(parts[1]) + as.numeric(parts[2]) / 60)
}

# Convert CompTime to minutes
data$CompTimeMinutes <- sapply(data$CompTime, time_to_minutes)

# Remove NA values and filter to 1 minute or less
data <- data %>% 
  filter(!is.na(C_TIPI) & !is.na(CompTimeMinutes) & CompTimeMinutes <= 1)

# Perform median split on C_TIPI
median_c <- median(data$C_TIPI, na.rm = TRUE)
data$C_Group <- factor(ifelse(data$C_TIPI > median_c, "High", "Low"), levels = c("Low", "High"))

# Remove outliers (values beyond 1.5 * IQR)
Q1 <- quantile(data$CompTimeMinutes, 0.25)
Q3 <- quantile(data$CompTimeMinutes, 0.75)
IQR <- Q3 - Q1
data <- data %>% filter(CompTimeMinutes >= (Q1 - 1.5 * IQR) & CompTimeMinutes <= (Q3 + 1.5 * IQR))

# Verify the levels of C_Group
print(table(data$C_Group))

# Create the plot
p <- ggplot(data, aes(x = C_Group, y = CompTimeMinutes)) +
  geom_boxplot(fill = "skyblue", color = "darkblue") +
  labs(title = "Completion Time by Conscientiousness Level",
       subtitle = "Filtered to ≤ 1 hour and outliers removed",
       x = "Conscientiousness Level",
       y = "Completion Time (hours)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))

# Display the plot
print(p)

# Calculate and display summary statistics
summary_stats <- data %>%
  group_by(C_Group) %>%
  summarise(
    N = n(),
    Mean = mean(CompTimeMinutes, na.rm = TRUE),
    Median = median(CompTimeMinutes, na.rm = TRUE),
    SD = sd(CompTimeMinutes, na.rm = TRUE)
  )


# # Additional diagnostic information
# cat("\nRange of C_TIPI scores:", range(data$C_TIPI, na.rm = TRUE), "\n")
# cat("Median C_TIPI score:", median_c, "\n")
# cat("Range of CompTimeMinutes:", range(data$CompTimeMinutes, na.rm = TRUE), "\n")
```

::: notes
Interpret the box plot, discussing differences in completion time
between high and low conscientiousness groups.
:::

::: notes
Explain how to interpret the radar chart and discuss the overall
personality profile of the class based on TIPI scores.
:::

## MBTI Distribution

```{r}
mbti_data <- data %>%
  filter(!is.na(MBTI)) %>%
  count(MBTI) %>%
  arrange(desc(n))

ggplot(mbti_data, aes(x = reorder(MBTI, -n), y = n)) +
  geom_bar(stat = "identity", fill = "coral") +
  labs(title = "Distribution of MBTI Types",
       x = "MBTI Type",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Violin Plot: EyeContact by MBTI Extraversion/Introversion

```{r}
library(ggplot2)
library(dplyr)

# Read the data (if not already in the environment)
data <- read.csv("materials/data/Y1W3_data.csv")

# Extract E/I and filter for only E and I
data$IE <- substr(data$MBTI, 1, 1)
data_filtered <- data %>% 
  filter(IE %in% c("E", "I"))

# Create the plot
ggplot(data_filtered, aes(x = IE, y = EyeContact, fill = IE)) +
  geom_violin(trim = FALSE) +
  geom_boxplot(width = 0.1, fill = "white") +
  labs(title = "EyeContact Score by Extraversion/Introversion",
       x = "MBTI Extraversion/Introversion",
       y = "EyeContact Score") +
  scale_fill_manual(values = c("E" = "#FFA500", "I" = "#4682B4")) +  # Orange for E, Steel Blue for I
  theme_minimal() +
  theme(legend.position = "none")  # Remove legend as it's redundant

# Print summary statistics
summary_stats <- data_filtered %>%
  group_by(IE) %>%
  summarise(
    n = n(),
    mean = mean(EyeContact, na.rm = TRUE),
    median = median(EyeContact, na.rm = TRUE),
    sd = sd(EyeContact, na.rm = TRUE)
  )
# print(summary_stats)

# # Perform t-test
# t_test_result <- t.test(EyeContact ~ IE, data = data_filtered)
# print(t_test_result)
```

::: notes
Interpret the violin plot, discussing differences in eye contact scores
between extraverts and introverts.
:::

## Stacked Bar Chart: MBTI Types by Pet Preference

```{r}
mbti_pet <- data %>%
  filter(!is.na(MBTI) & !is.na(DogCatBoth)) %>%
  count(MBTI, DogCatBoth) %>%
  group_by(MBTI) %>%
  mutate(prop = n / sum(n))

ggplot(mbti_pet, aes(x = MBTI, y = prop, fill = DogCatBoth)) +
  geom_bar(stat = "identity") +
  labs(title = "MBTI Types by Pet Preference",
       x = "MBTI Type",
       y = "Proportion",
       fill = "Pet Preference") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Anscombe's Quartet

```{r}
#| echo: false
library(ggplot2)
library(data.table)

ac <- fread("materials/data/anscombe.txt")
ac <- as.data.frame(ac)

ac_long <- data.frame(
  x = c(ac[,1], ac[,3], ac[,5], ac[,7]),
  y = c(ac[,2], ac[,4], ac[,6], ac[,8]),
  quartet = as.factor(rep(1:4, each=11))
)

ggplot(ac_long, aes(x=x, y=y, color=quartet)) +
  geom_point() +
  theme_classic() +
  facet_wrap(~quartet)
```

## Datasuarus

```{r}
#| message: false
#| warning: false


# install.packages("datasauRus")
library(datasauRus)
library(ggplot2)
library(dplyr)

# Load the datasaurus_dozen dataset
data(datasaurus_dozen)

# Create the plot
ggplot(datasaurus_dozen, aes(x = x, y = y, colour = dataset)) +
  geom_point() +
  theme_void() +
  theme(legend.position = "none") +
  facet_wrap(~dataset, ncol = 3) +
  labs(title = "The Datasaurus Dozen")

# Calculate summary statistics for each dataset
summary_stats <- datasaurus_dozen %>%
  group_by(dataset) %>%
  summarise(
    mean_x = mean(x),
    mean_y = mean(y),
    sd_x = sd(x),
    sd_y = sd(y),
    correlation = cor(x, y)
  )


```

## Summary DatasauRus Stats

```{r}
# Print the summary statistics
print(summary_stats)
```

## **Plato’s Triad: The True, The Good, and The Beautiful**

Plato believed in the intrinsic connection between **truth**,
**goodness**, and **beauty** — a concept that has influenced Western
thought for centuries.

For Plato, these three qualities are inseparable in the realm of
**Forms**.

The **truth** is inherently **good**, and what is **good** is also
inherently **beautiful**.

Thus, if something is **false**, it cannot be truly **beautiful** or
**good**.

## R Big Picture

[Quarto Publishing System](https://quarto.org/)

## What do you currently use?

**How do you write your essays or lab reports?**

-   Microsoft Word?

-   Google Docs?

-   Markdown?

::: fragment
**How do you currently play with numbers?**

-   Excel?

-   SPSS?

-   R?

-   Python?
:::

## What is Quarto?

Quarto is an open-source scientific and technical publishing system
[that allows you to combine text, images, code, plots, and tables in a
fully-reproducible document.]{.fragment}

<br>

[Quarto has support for multiple languages including R, Python, Julia,
and Observable.]{.fragment}

<br>

[It also works for a range of output formats such as PDFs, HTML
documents, websites, presentations,...]{.fragment}

## Why use Quarto? Why use R?

-   More journals require code to be submitted (for transparency and
    reproducibility). Keeping the code with the paper makes this easier.

-   Copying and pasting is tedious (and a great source of accidental
    errors).

-   If you fix an error in code or data, the results and figures in the
    paper update automatically.

-   Easy to share publicly.

-   Open source so anyone can use it.

## What about R Markdown?

R Markdown isn't going anywhere but...

-   Quarto has better multi-language support

-   More user-friendly

-   Better control of the output layouts

## Rendering a document

Within RStudio IDE: click **Render** (or Ctrl+Shift+K)

. . .

Content

-   Text, links, images

-   Code, tables, plots

-   Equations, references

## Output types

::: incremental
-   Documents: HTML, PDF, MS Word, Markdown

-   Presentations: Revealjs, PowerPoint, Beamer

-   Websites

-   Books

-   ...
:::

## Introduction

```{r}
library(tidyverse)
library(plotly)
library(knitr)

# Load and prepare the data
data <- read_csv("materials/data/Y1W3_data.csv", na = c("", "NA", "#N/A"))

student_data <- data %>%
  select(PokeNumber, PokeName, PokeImage, LoginCount, Q_Scale, CompTime, Image) %>%
  mutate(
    HasLoginCount = !is.na(LoginCount) & LoginCount != 0,
    SubmittedQuestions = !is.na(CompTime) & CompTime != "0" & CompTime != "",
    SubmittedImage = !is.na(Image) & Image != ""
  )

# Calculate engagement statistics
total_students <- nrow(student_data)
active_students <- sum(student_data$HasLoginCount)
questions_submitted <- sum(student_data$SubmittedQuestions)
image_submitted <- sum(student_data$SubmittedImage)

# Create a data frame for the engagement stages
engagement_data <- data.frame(
  Stage = c("Total Enrolled", "Active (Non-zero LoginCount)", "Submitted Questions", "Submitted Image"),
  Count = c(total_students, active_students, questions_submitted, image_submitted)
)

# Calculate percentages
engagement_data$Percentage <- engagement_data$Count / total_students * 100

# Reorder the stages to be in descending order
engagement_data <- engagement_data %>%
  arrange(desc(Count))

# Display the engagement data
kable(engagement_data, caption = "Student Engagement Summary", digits = 2)
```

## Visual

```{r}
# Create a visual representation of the engagement
p <- plot_ly(engagement_data, x = ~reorder(Stage, -Count), y = ~Count, type = 'bar', 
             text = ~paste(Count, " (", round(Percentage, 1), "%)"), 
             textposition = 'auto',
             marker = list(color = c('#1f77b4', '#2ca02c', '#d62728', '#9467bd'))) %>%
  layout(title = "Student Engagement Stages",
         xaxis = list(title = "Stage"),
         yaxis = list(title = "Number of Students"),
         showlegend = FALSE)

# Display the plot
p

```

## Visualisation of Your Data

This analysis covers levels of measurement, measures of central
tendency, variance, and various plot types using the student data
collected.

## 1. Levels of Measurement

1.  **Nominal**:
    -   MBTI (Myers-Briggs Type Indicator)
    -   Coin (Heads or Tails)
    -   DogCatBoth (Preference for pets)
    -   InsectApocalypse preference
2.  **Ordinal**:
    -   EyeContact (Likert or scale responses)
    -   Ten Item Personality Inventory scores (OCEAN)
3.  **Interval/Ratio**:
    -   Count of VLE Logins so far \[LoginCount\]
    -   Survey Completion Time \[CompTime - minutes\]

## 2. Central Tendency and Variance

Let's calculate these for LoginCount and the Extraversion (TIPI) score:

```{r central-tendency-variance}
numeric_data <- data %>%
  select(LoginCount, E_TIPI) %>%
  na.omit()

summary_stats <- numeric_data %>%
  summarise(across(everything(), list(
    Mean = ~mean(., na.rm = TRUE),
    Median = ~median(., na.rm = TRUE),
    SD = ~sd(., na.rm = TRUE),
    Variance = ~var(., na.rm = TRUE)
  )))

kable(t(summary_stats), digits = 2, caption = "Summary Statistics")
```

## Various Plot Types

### 1. Histogram: LoginCount

```{r histogram}
ggplot(data, aes(x = LoginCount)) +
  geom_histogram(binwidth = 5, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Login Counts",
       x = "Login Count",
       y = "Frequency") +
  theme_minimal()
```

## 2. Bar Chart: DogCatBoth Preferences

```{r bar-chart}
# Libraries
library(ggplot2)
library(dplyr)

# Assuming `data` is your dataset
ggplot(data %>% filter(!is.na(DogCatBoth)), aes(x = DogCatBoth)) +
  geom_bar(fill = "lightgreen") +
  geom_text(stat = "count", aes(label = ..count..), vjust = -0.5) +  # Adds count figures on top of bars
  labs(title = "Preference for Dogs, Cats, or Both",
       x = "Preference",
       y = "Count") +
  theme_minimal()


```

## 3a. Insect Apocalypse

```{r}
#| warning: false
  
library(tidyverse)
library(ggplot2)

data <- read_csv("materials/data/Y1W3_data.csv")

# Prepare the data
big_five_data <- data %>%
  select(InsectApocalypse, E_TIPI, A_TIPI, C_TIPI, ES_TIPI, O_TIPI) %>%
  pivot_longer(cols = c(E_TIPI, A_TIPI, C_TIPI, ES_TIPI, O_TIPI), 
               names_to = "Trait", 
               values_to = "Score") %>%
  mutate(Trait = factor(Trait, 
                        levels = c("E_TIPI", "A_TIPI", "C_TIPI", "ES_TIPI", "O_TIPI"),
                        labels = c("Extraversion", "Agreeableness", "Conscientiousness", 
                                   "Emotional Stability", "Openness"))) %>%
  group_by(InsectApocalypse, Trait) %>%
  summarise(Mean_Score = mean(Score, na.rm = TRUE), .groups = 'drop')

# Create the plot
ggplot(big_five_data, aes(x = Trait, y = Mean_Score, fill = InsectApocalypse)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Big Five Personality Traits Across InsectApocalypse Groups",
       x = NULL, 
       y = "Mean TIPI Score",
       fill = "Insect Preference") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom") +
  scale_fill_brewer(palette = "Set2")
```

## 3b. Insect Apocalypse

```{r}
#| warning: false
#| fig.width: 12
#| fig.height: 6

library(tidyverse)
library(ggplot2)

# Prepare the data
big_five_data <- data %>%
  select(InsectApocalypse, E_TIPI, A_TIPI, C_TIPI, ES_TIPI, O_TIPI) %>%
  pivot_longer(cols = c(E_TIPI, A_TIPI, C_TIPI, ES_TIPI, O_TIPI), 
               names_to = "Trait", 
               values_to = "Score") %>%
  mutate(Trait = factor(Trait, 
                        levels = c("E_TIPI", "A_TIPI", "C_TIPI", "ES_TIPI", "O_TIPI"),
                        labels = c("Extraversion", "Agreeableness", "Conscientiousness", 
                                   "Emotional Stability", "Openness"))) %>%
  group_by(InsectApocalypse, Trait) %>%
  summarise(
    Mean_Score = mean(Score, na.rm = TRUE),
    SE = sd(Score, na.rm = TRUE) / sqrt(n()),
    .groups = 'drop'
  )

# Create the plot
ggplot(big_five_data, aes(x = Trait, y = Mean_Score, fill = InsectApocalypse)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
  geom_errorbar(aes(ymin = Mean_Score - SE, ymax = Mean_Score + SE),
                position = position_dodge(width = 0.9),
                width = 0.25) +
  labs(title = "Big Five Personality Traits Across InsectApocalypse Groups",
       x = NULL, 
       y = "Mean TIPI Score",
       fill = "Insect Preference") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom") +
  scale_fill_brewer(palette = "Set2")
```

## 4a. Scatter Plot: EyeContact vs LoginCount

```{r scatter-plot}
ggplot(data, aes(x = LoginCount, y = EyeContact)) +
  geom_point(alpha = 0.7) +
  labs(title = "Eye Contact Score vs Login Count",
       x = "Login Count",
       y = "Eye Contact Score") +
  theme_minimal()
```

## 4b. Scatter Plot: EyeContact vs LoginCount

```{r}
# Libraries
library(ggplot2)

# Assuming `data` is your dataset
ggplot(data, aes(x = LoginCount, y = EyeContact)) +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "lm", color = "blue", se = FALSE) +  # Adds a trend line
  labs(title = "Eye Contact Score vs Login Count",
       x = "Login Count",
       y = "Eye Contact Score") +
  theme_minimal()

```

## 5a. Box Plot: CompTime by Conscientiousness Level

```{r boxplot}
# Create high/low conscientiousness groups
data$C_Group <- ifelse(data$C_TIPI > median(data$C_TIPI, na.rm = TRUE), "High", "Low")

# Ensure CompTime is numeric and remove NAs for plotting
ggplot(data %>% filter(!is.na(CompTime)), aes(x = C_Group, y = as.numeric(CompTime))) +
  geom_boxplot() +
  labs(title = "Completion Time by Conscientiousness Level",
       x = "Conscientiousness Level",
       y = "Completion Time (minutes)") +
  theme_minimal()

```

## 5b. Box Plot: CompTime by Conscientiousness Level

```{r}
# Create high/low conscientiousness groups
data$C_Group <- ifelse(data$C_TIPI > median(data$C_TIPI, na.rm = TRUE), "High", "Low")

# Filter data to include only responses with CompTime less than 2500 and remove NAs
ggplot(data %>% filter(!is.na(CompTime) & as.numeric(CompTime) < 2500), aes(x = C_Group, y = as.numeric(CompTime))) +
  geom_boxplot() +
  labs(title = "Completion Time by Conscientiousness Level (Restricted to < 2500 minutes)",
       x = "Conscientiousness Level",
       y = "Completion Time (minutes)") +
  theme_minimal()

```

## 6. MBTI Count Matrix

```{r mbti-matrix}
# Libraries
library(dplyr)
library(ggplot2)

# Exclude NAs and count occurrences of each MBTI type
mbti_counts <- data %>%
  filter(!is.na(MBTI)) %>%  # Exclude NAs
  group_by(MBTI) %>%
  summarize(Count = n()) %>%
  arrange(desc(Count))

# Display the counts
print(mbti_counts)

# Plot the counts using ggplot2
ggplot(mbti_counts, aes(x = reorder(MBTI, -Count), y = Count)) +
  geom_bar(stat = "identity", fill = "#69b3a2") +
  labs(title = "Counts of Each MBTI Type (Excluding NAs)",
       x = "MBTI Type",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```

## 7. TIPI Radar Charts by Pet Preference

```{r tipi-radar-charts, fig.width=15, fig.height=5}
# Libraries
library(dplyr)
library(fmsb)  # For radar charts

# Function to create radar chart with custom color
create_radar_chart <- function(data, title, color) {
  # Prepare radar data with max and min values for scaling
  radar_data <- rbind(
    c(5, 5, 5, 5, 5),  # Max values
    c(1, 1, 1, 1, 1),  # Min values
    data
  )
  colnames(radar_data) <- c("Extraversion", "Agreeableness", "Conscientiousness", "Emotional Stability", "Openness")
  
  # Create radar chart
  radarchart(radar_data, 
             pcol = color,                # Line color
             pfcol = adjustcolor(color, alpha.f = 0.5),  # Filled color
             plwd = 2,                    # Line width
             plty = 1,                    # Line type
             cglcol = "grey",             # Grid color
             cglty = 1,                   # Grid line type
             axislabcol = "grey",         # Axis label color
             caxislabels = seq(1, 5, 1),  # Axis label values
             cglwd = 0.8,                 # Grid line width
             vlcex = 0.8                  # Label size
  )
  title(title)  # Add chart title
}

# Prepare data for each group
dog_people <- data %>% 
  filter(DogCatBoth == "Dogs") %>%
  select(E_TIPI, A_TIPI, C_TIPI, ES_TIPI, O_TIPI) %>%
  summarise(across(everything(), mean, na.rm = TRUE))

cat_people <- data %>% 
  filter(DogCatBoth == "Cats") %>%
  select(E_TIPI, A_TIPI, C_TIPI, ES_TIPI, O_TIPI) %>%
  summarise(across(everything(), mean, na.rm = TRUE))

both_people <- data %>% 
  filter(DogCatBoth == "Both") %>%
  select(E_TIPI, A_TIPI, C_TIPI, ES_TIPI, O_TIPI) %>%
  summarise(across(everything(), mean, na.rm = TRUE))

# Set up plotting area to have 3 plots side by side
par(mfrow = c(1, 3))

# Create radar charts with different colors
create_radar_chart(dog_people, "Dog People", "blue")
create_radar_chart(cat_people, "Cat People", "red")
create_radar_chart(both_people, "Dog & Cat People", "green")


```

## 8. Heatmaps for Seating Preferences and TIPI Traits

```{r seating-heatmaps, fig.width=15, fig.height=15}
# Libraries
library(ggplot2)
library(gridExtra)
library(dplyr)

# Prepare seating data
seating_data <- data %>%
  select(Row, Column, E_TIPI, A_TIPI, C_TIPI, ES_TIPI, O_TIPI) %>%
  na.omit()

# Function to create heatmap with flipped x-axis and smooth coloring for each trait
create_heatmap <- function(data, trait, title, colors) {
  ggplot(data, aes(x = Column, y = Row, fill = .data[[trait]])) +
    geom_tile() +
    scale_x_reverse() +  # Flip the x-axis
    scale_fill_gradientn(colors = colors) +  # Use custom colors for smoother gradient
    labs(title = title, x = "Column", y = "Row") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))
}

# Color palettes for each trait
colors_extraversion <- c("white", "lightblue", "blue")
colors_agreeableness <- c("white", "lightgreen", "green")
colors_conscientiousness <- c("white", "yellow", "orange")
colors_emotional_stability <- c("white", "pink", "red")
colors_openness <- c("white", "purple", "purple4")

# Create heatmaps for each trait with different color scales
plots <- list(
  create_heatmap(seating_data, "E_TIPI", "Extraversion", colors_extraversion),
  create_heatmap(seating_data, "A_TIPI", "Agreeableness", colors_agreeableness),
  create_heatmap(seating_data, "C_TIPI", "Conscientiousness", colors_conscientiousness),
  create_heatmap(seating_data, "ES_TIPI", "Emotional Stability", colors_emotional_stability),
  create_heatmap(seating_data, "O_TIPI", "Openness", colors_openness)
)

# Arrange plots in a grid
grid.arrange(grobs = plots, ncol = 2)

# Scatter plot of seating distribution with flipped x-axis
ggplot(seating_data, aes(x = Column, y = Row)) +
  geom_point(alpha = 0.5) +
  scale_x_reverse() +  # Flip the x-axis
  labs(title = "Distribution of Seating Preferences", x = "Column", y = "Row") +
  theme_minimal()

```

## note: BoxPlot Basics

![](images/boxplot.png){fig-align="center"}

## 9. Compact Boxplots: TIPI Traits

```{r tipi-boxplots, fig.width=10, fig.height=6}
tipi_data <- data %>%
  select(E_TIPI, A_TIPI, C_TIPI, ES_TIPI, O_TIPI) %>%
  pivot_longer(cols = everything(), names_to = "Trait", values_to = "Score") %>%
  mutate(Trait = factor(Trait, levels = c("E_TIPI", "A_TIPI", "C_TIPI", "ES_TIPI", "O_TIPI"),
                        labels = c("Extraversion", "Agreeableness", "Conscientiousness", "Emotional Stability", "Openness")))

ggplot(tipi_data, aes(x = Trait, y = Score, fill = Trait)) +
  geom_boxplot() +
  labs(title = "Distribution of TIPI Traits", x = "", y = "Score") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none")
```

## 10. Violin Plots: EyeContact and Psy4me by MBTI Dimension

```{r mbti-violin-plots, fig.width=12, fig.height=8}

library(ggExtra)
library(gridExtra)
library(ggplot2)  # Explicitly load ggplot2 for clarity

# Filter and mutate data without pipes
mbti_data <- subset(data, !is.na(MBTI))
mbti_data$IE <- substr(mbti_data$MBTI, 1, 1)
mbti_data$SN <- substr(mbti_data$MBTI, 2, 2)
mbti_data$TF <- substr(mbti_data$MBTI, 3, 3)
mbti_data$JP <- substr(mbti_data$MBTI, 4, 4)

# Create the first plot
p1 <- ggplot(mbti_data, aes(x = IE, y = EyeContact, fill = IE)) +
  geom_violin(trim = FALSE) +
  geom_boxplot(width = 0.1, fill = "white") +
  labs(title = "EyeContact by I/E Preference", x = "", y = "EyeContact Score") +
  theme_minimal()

# Create the second plot
p2 <- ggplot(mbti_data, aes(x = SN, y = Psy4me, fill = SN)) +
  geom_violin(trim = FALSE) +
  geom_boxplot(width = 0.1, fill = "white") +
  labs(title = "Psychology Interest by S/N Preference", x = "", y = "Psy4me Score") +
  theme_minimal()

# Arrange the plots side by side
grid.arrange(p1, p2, ncol = 2)
```

## 11. Scatterplots: Exploring Relationships

```{r scatterplots, fig.width=12, fig.height=8}
p3 <- ggplot(data, aes(x = E_TIPI, y = EyeContact, color = DogCatBoth)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Extraversion vs EyeContact", x = "Extraversion Score", y = "EyeContact Score") +
  theme_minimal()

p3
```
